[
  {
    "objectID": "mofa-light.html",
    "href": "mofa-light.html",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "",
    "text": "Setting up environment\nYou will need to install a few packages to fully run this notebook. The main packages needed are MOFA2 and ggplot2.\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"MOFA2\")\n\n# list of packages to be installed\npackages &lt;- c(\"ggplot2\")\n\n# check and install missing packages\nnew_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages, dependencies = TRUE, type = \"binary\")"
  },
  {
    "objectID": "mofa-light.html#building-a-mofa-object",
    "href": "mofa-light.html#building-a-mofa-object",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "Building a MOFA object",
    "text": "Building a MOFA object\nTo create a MOFA object you need to specify three dimensions: samples, features and view(s). Views(s) are the different omics types in our case. MOFA object can be created based on many different data formats such as a list of matrices, a long data.frame, MultiAssayExperiment or even Suerat objects for single-cell genomics data. Here, we will use a list of matrices as our TCGA data is already in this format.\n\n# load library\nlibrary(MOFA2)\n\n# remove the subtype information from training data\ndata_mofa &lt;- breast.TCGA$data.train[-4]\n\n# transpose data because MOFA wants features in rows\ndata_mofa &lt;- lapply(data_mofa, t)\n\n# create MOFA object\nMOFAobject &lt;- create_mofa(data_mofa)\n## Creating MOFA object from a list of matrices (features as rows, sample as columns)...\n\nWe can have a look at the structure of the input data:\n\nplot_data_overview(MOFAobject)\n\n\n\n\n\n\n\n\nThis shows us how many samples we have and how many features per data view is there. If there were missing values, these would be shown as gray lines.\n\nExercise 1 (NA) Can you add a bit of missing values and check how the plot will change?\n\n\n\nShow the code\n# Let's randomly introduce NAs to 20% of samples in one omics e.g. protein data\n# This is an example code. You can probably find an easier way to solve this :) \n\n# make a copy of MOFA data and protein data\ndata_mofa_with_na &lt;- data_mofa\ndata_protein &lt;- data_mofa$protein\n\n# calculate number of data points to replace\nn &lt;- ncol(data_protein) # no. of samples\nn_to_replace &lt;- 20/100 * n # number to replace, 20%\n\n# sample index and replace with NA \ndata_protein[, sample(1:n, n_to_replace)] &lt;- NA\n\n# check that we have NAs, we should have n_to_replace amount\n# sum(is.na(data_protein))\n\n# replace protein data under the MOFA\ndata_mofa_with_na$protein &lt;- data_protein\n\n# create MOFA object\nMOFAobject_with_na &lt;- create_mofa(data_mofa_with_na)\n\n\nCreating MOFA object from a list of matrices (features as rows, sample as columns)...\n\n\nShow the code\n# plot\nplot_data_overview(MOFAobject_with_na)"
  },
  {
    "objectID": "mofa-light.html#defining-data-and-model-options",
    "href": "mofa-light.html#defining-data-and-model-options",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "Defining data and model options",
    "text": "Defining data and model options\nBefore we start modeling, we can specify some data and model options.\nFor data options, we have:\n\nscale_groups: if groups have different ranges/variances, it is good practice to scale each group to unit variance. Default is FALSE\nscale_views: if views have different ranges/variances, it is good practice to scale each view to unit variance. Default is FALSE\n\nand we can confirm that we are using default options by:\n\ndata_opts &lt;- get_default_data_options(MOFAobject)\nhead(data_opts)\n\n$scale_views\n[1] FALSE\n\n$scale_groups\n[1] FALSE\n\n$center_groups\n[1] TRUE\n\n$use_float32\n[1] TRUE\n\n$views\n[1] \"mirna\"   \"mrna\"    \"protein\"\n\n$groups\n[1] \"group1\"\n\n\nFor model options, we have:\n\nnum_factors: number of factors\nlikelihoods: likelihood per view (options are “gaussian”, “poisson”, “bernoulli”). Default is “gaussian”.\nspikeslab_factors: use spike-slab sparsity prior in the factors? Default is FALSE.\nspikeslab_weights: use spike-slab sparsity prior in the weights? Default is TRUE.\nard_factors: use ARD prior in the factors? Default is TRUE if using multiple groups.\nard_weights: use ARD prior in the weights? Default is TRUE if using multiple views.\n\nWe can control the number of factors and we should adjust the likelihoods to match our data. Unless we want to learn more about the underlying mathematical models, we keep other parameters, such as spikeslab and ARD priors set to default.\nLet’s check our omics data distributions to make sure we use correct likelihood values.\n\npar(mfrow=c(2,2))\nhist(data_mofa$mrna)\nhist(data_mofa$protein)\nhist(data_mofa$mirna)\n\n\n\n\n\n\n\n\nAll of our data is seems to be normally distributed so we use normal distribution. In practice MOFA allows us to select ‘gaussian’ for continuous data (e.g proteomics), ‘bernoulli’ for binary data (e.g. methylation) and ‘poisson’ for count data (e.g. RNA-Seq).\nWe can now set the model parameters. We can preview the default parameters already set:\n\nmodel_opts &lt;- get_default_model_options(MOFAobject)\nprint(model_opts)\n\n$likelihoods\n     mirna       mrna    protein \n\"gaussian\" \"gaussian\" \"gaussian\" \n\n$num_factors\n[1] 15\n\n$spikeslab_factors\n[1] FALSE\n\n$spikeslab_weights\n[1] FALSE\n\n$ard_factors\n[1] FALSE\n\n$ard_weights\n[1] TRUE\n\n\nwhere we see that MOFA selected default (gaussian) likelihood for all our data and includes 15 factors (latent variables).\nTo change model parameters, e.g. reduce number of factors from default 15 to 10 to make the computations run faster we type:\n\nmodel_opts$num_factors &lt;- 10"
  },
  {
    "objectID": "mofa-light.html#training-a-mofa-object",
    "href": "mofa-light.html#training-a-mofa-object",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "Training a MOFA object",
    "text": "Training a MOFA object\nOur MOFA object is now set and we can start the training. Similar to model options, there are parameters that define training options. Briefly, these are:\n\nmaxiter: number of iterations\nconvergence_mode\ngpu_mode\nverbose mode\n\nWe can again see the default values:\n\ntrain_opts &lt;- get_default_training_options(MOFAobject)\nhead(train_opts)\n\n$maxiter\n[1] 1000\n\n$convergence_mode\n[1] \"fast\"\n\n$drop_factor_threshold\n[1] -1\n\n$verbose\n[1] FALSE\n\n$startELBO\n[1] 1\n\n$freqELBO\n[1] 5\n\n\nand notice that for instance that the default number of iterations is set to 1000 and the convergence mode is set to “fast”. Similar to model options, these parameters refer to the underlying method. “Fast” convergence mode tends to be good for exploration, but it may be worth considering changing it to “medium” or “slow” for the final model. GPU mode refers to running MOFA on GPU, something that needs cupy installed and a functional GPU.\nTo train a MOFA object:\n\nMOFAobject &lt;- prepare_mofa(MOFAobject,\n                           model_options = model_opts # input model options\n)\n\nChecking data options...\n\n\nNo data options specified, using default...\n\n\nNo training options specified, using default...\n\n\nChecking model options...\n\nMOFAobject &lt;- invisible(run_mofa(MOFAobject))\n\nWarning in run_mofa(MOFAobject): No output filename provided. Using /var/folders/kh/tgq9mmld6_v9z_h220trj0c40000gn/T//Rtmp0Bhe7X/mofa_20231106-092240.hdf5 to store the trained model.\n\n\nConnecting to the mofapy2 python package using reticulate (use_basilisk = FALSE)... \n    Please make sure to manually specify the right python binary when loading R with reticulate::use_python(..., force=TRUE) or the right conda environment with reticulate::use_condaenv(..., force=TRUE)\n    If you prefer to let us automatically install a conda environment with 'mofapy2' installed using the 'basilisk' package, please use the argument 'use_basilisk = TRUE'\n\n\n\n        #########################################################\n        ###           __  __  ____  ______                    ### \n        ###          |  \\/  |/ __ \\|  ____/\\    _             ### \n        ###          | \\  / | |  | | |__ /  \\ _| |_           ### \n        ###          | |\\/| | |  | |  __/ /\\ \\_   _|          ###\n        ###          | |  | | |__| | | / ____ \\|_|            ###\n        ###          |_|  |_|\\____/|_|/_/    \\_\\              ###\n        ###                                                   ### \n        ######################################################### \n       \n \n        \nuse_float32 set to True: replacing float64 arrays by float32 arrays to speed up computations...\n\nSuccessfully loaded view='mirna' group='group1' with N=150 samples and D=184 features...\nSuccessfully loaded view='mrna' group='group1' with N=150 samples and D=200 features...\nSuccessfully loaded view='protein' group='group1' with N=150 samples and D=142 features...\n\n\nModel options:\n- Automatic Relevance Determination prior on the factors: False\n- Automatic Relevance Determination prior on the weights: True\n- Spike-and-slab prior on the factors: False\n- Spike-and-slab prior on the weights: False\nLikelihoods:\n- View 0 (mirna): gaussian\n- View 1 (mrna): gaussian\n- View 2 (protein): gaussian\n\n\n\n\n######################################\n## Training the model with seed 42 ##\n######################################\n\n\nELBO before training: -605349.03 \n\nIteration 1: time=0.01, ELBO=-97986.53, deltaELBO=507362.501 (83.81321728%), Factors=10\nIteration 2: time=0.00, Factors=10\nIteration 3: time=0.00, Factors=10\nIteration 4: time=0.00, Factors=10\nIteration 5: time=0.00, Factors=10\nIteration 6: time=0.01, ELBO=-85812.38, deltaELBO=12174.149 (2.01109572%), Factors=10\nIteration 7: time=0.00, Factors=10\nIteration 8: time=0.00, Factors=10\nIteration 9: time=0.00, Factors=10\nIteration 10: time=0.00, Factors=10\nIteration 11: time=0.01, ELBO=-85546.36, deltaELBO=266.029 (0.04394630%), Factors=10\nIteration 12: time=0.00, Factors=10\nIteration 13: time=0.00, Factors=10\nIteration 14: time=0.00, Factors=10\nIteration 15: time=0.00, Factors=10\nIteration 16: time=0.01, ELBO=-85119.86, deltaELBO=426.494 (0.07045427%), Factors=10\nIteration 17: time=0.00, Factors=10\nIteration 18: time=0.00, Factors=10\nIteration 19: time=0.00, Factors=10\nIteration 20: time=0.00, Factors=10\nIteration 21: time=0.01, ELBO=-84663.08, deltaELBO=456.782 (0.07545757%), Factors=10\nIteration 22: time=0.00, Factors=10\nIteration 23: time=0.00, Factors=10\nIteration 24: time=0.00, Factors=10\nIteration 25: time=0.00, Factors=10\nIteration 26: time=0.01, ELBO=-84469.19, deltaELBO=193.894 (0.03203003%), Factors=10\nIteration 27: time=0.00, Factors=10\nIteration 28: time=0.00, Factors=10\nIteration 29: time=0.00, Factors=10\nIteration 30: time=0.00, Factors=10\nIteration 31: time=0.01, ELBO=-84312.91, deltaELBO=156.276 (0.02581593%), Factors=10\nIteration 32: time=0.00, Factors=10\nIteration 33: time=0.00, Factors=10\nIteration 34: time=0.00, Factors=10\nIteration 35: time=0.00, Factors=10\nIteration 36: time=0.01, ELBO=-84211.62, deltaELBO=101.293 (0.01673295%), Factors=10\nIteration 37: time=0.00, Factors=10\nIteration 38: time=0.00, Factors=10\nIteration 39: time=0.00, Factors=10\nIteration 40: time=0.00, Factors=10\nIteration 41: time=0.01, ELBO=-84149.57, deltaELBO=62.049 (0.01025015%), Factors=10\nIteration 42: time=0.00, Factors=10\nIteration 43: time=0.00, Factors=10\nIteration 44: time=0.01, Factors=10\nIteration 45: time=0.00, Factors=10\nIteration 46: time=0.01, ELBO=-84113.44, deltaELBO=36.127 (0.00596801%), Factors=10\nIteration 47: time=0.00, Factors=10\nIteration 48: time=0.00, Factors=10\nIteration 49: time=0.00, Factors=10\nIteration 50: time=0.00, Factors=10\nIteration 51: time=0.01, ELBO=-84094.13, deltaELBO=19.309 (0.00318972%), Factors=10\nIteration 52: time=0.00, Factors=10\nIteration 53: time=0.00, Factors=10\nIteration 54: time=0.00, Factors=10\nIteration 55: time=0.00, Factors=10\nIteration 56: time=0.01, ELBO=-84083.06, deltaELBO=11.072 (0.00182895%), Factors=10\nIteration 57: time=0.00, Factors=10\nIteration 58: time=0.00, Factors=10\nIteration 59: time=0.00, Factors=10\nIteration 60: time=0.00, Factors=10\nIteration 61: time=0.01, ELBO=-84075.87, deltaELBO=7.195 (0.00118855%), Factors=10\nIteration 62: time=0.00, Factors=10\nIteration 63: time=0.00, Factors=10\nIteration 64: time=0.00, Factors=10\nIteration 65: time=0.00, Factors=10\nIteration 66: time=0.01, ELBO=-84070.67, deltaELBO=5.196 (0.00085839%), Factors=10\nIteration 67: time=0.00, Factors=10\nIteration 68: time=0.00, Factors=10\nIteration 69: time=0.00, Factors=10\nIteration 70: time=0.00, Factors=10\nIteration 71: time=0.01, ELBO=-84066.69, deltaELBO=3.981 (0.00065768%), Factors=10\nIteration 72: time=0.00, Factors=10\nIteration 73: time=0.00, Factors=10\nIteration 74: time=0.00, Factors=10\nIteration 75: time=0.00, Factors=10\nIteration 76: time=0.01, ELBO=-84063.53, deltaELBO=3.155 (0.00052122%), Factors=10\nIteration 77: time=0.00, Factors=10\nIteration 78: time=0.00, Factors=10\nIteration 79: time=0.00, Factors=10\nIteration 80: time=0.00, Factors=10\nIteration 81: time=0.01, ELBO=-84060.96, deltaELBO=2.577 (0.00042565%), Factors=10\nIteration 82: time=0.00, Factors=10\nIteration 83: time=0.00, Factors=10\nIteration 84: time=0.00, Factors=10\nIteration 85: time=0.00, Factors=10\nIteration 86: time=0.01, ELBO=-84058.83, deltaELBO=2.127 (0.00035130%), Factors=10\n\nConverged!\n\n\n\n#######################\n## Training finished ##\n#######################\n\n\nSaving model in /var/folders/kh/tgq9mmld6_v9z_h220trj0c40000gn/T//Rtmp0Bhe7X/mofa_20231106-092240.hdf5...\n\n\nWarning in .quality_control(object, verbose = verbose): Factor(s) 4 are strongly correlated with the total number of expressed features for at least one of your omics. Such factors appear when there are differences in the total 'levels' between your samples, *sometimes* because of poor normalisation in the preprocessing steps.\n\n\nNote that we get a message that a model was automatically save to .hdf5. It is also possible to specify the name and location of the file to save the model to, via outfile option under runMofa() function."
  },
  {
    "objectID": "mofa-light.html#variance-decomposition",
    "href": "mofa-light.html#variance-decomposition",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "Variance decomposition",
    "text": "Variance decomposition\nThe most important insight that MOFA generates is the variance decomposition analysis. This plot shows the percentage of variance explained by each factor across each data modality.\n\nplot_variance_explained(MOFAobject)\n\n\n\n\n\n\n\n\nFrom the results of the plot_variance_explained function in MOFA, we can discern the variance explained by each factor across the three views: mirna, mrna, and protein.\nIn the mirna view, Factor1 leads by explaining approximately 15.96% of the variance. Notably, Factor1 also stands out in both the mrna and protein views, explaining 20.37% and 20.41% respectively, suggesting its consistent importance across all views.\nFor the mrna view, besides Factor1, Factor2 contributes significantly with 11.88%. This contrasts with its contribution in the protein view, where it explains only 1.25% of the variance, and in the mirna view, where it accounts for 6.04%.\nIn the protein view, while Factor1 remains dominant, Factor3 emerges as significant, explaining 12.20% of the variance. This is intriguing as Factor3 has a minimal role in the mrna view (0.12%) but does have a presence in the mirna view with 0.65%.\nFactors such as Factor4 and Factor7 exhibit diverse roles across the views. In the mirna view, Factor4 explains a notable 12.77% but diminishes to 0.16% and 0.02% in the mrna and protein views respectively. Factor7, on the other hand, is more prominent in the mirna view with 7.09% but is almost negligible in the other two views.\nBtw. the variance explained estimates, corresponding to the above plot, are stored in the hdf5 file and loaded in model@cache[[“variance_explained”]]. They can be viewed via:\n\n# variance explained for every factor \nprint(MOFAobject@cache$variance_explained$r2_per_factor)\n\n$group1\n               mirna        mrna     protein\nFactor1  15.95515609 20.36821842 20.41299939\nFactor2   6.04512691 11.88386679  1.25099421\nFactor3   0.64774156  0.12200475 12.20212579\nFactor4  12.77297139  0.16268492  0.02241731\nFactor5   3.50421071  5.50275445  1.06747746\nFactor6   3.08054090  3.41790318  2.97962427\nFactor7   7.09193349  0.05609989  0.02453923\nFactor8   1.25472546  4.96560931  0.72206855\nFactor9   0.01165271  0.11986494  6.47216439\nFactor10  2.66354084  2.11876631  0.31054020\n\n\nWhich factor consistently plays a vital role across all the views?\nIt is also important to see if the model fits the data well. We can do this by looking at how much of total variance is explained by factors across different views. Here, the results will usually vary based on the kind of data, number of samples and number of factors used.\n\nvar_plot &lt;- plot_variance_explained(MOFAobject, plot_total = T)[[2]]\nvar_plot\n\n\n\n\nIn this data set, by using 10 factors the MOFA model explains about 49% of the variation in the miRNA, 48% of the variation in the mRNA data and 45% of the variation in the protein data.\nThe matching variance values can be extracted via:\n\nprint(MOFAobject@cache$variance_explained$r2_total)\n\n$group1\n   mirna     mrna  protein \n49.21639 48.26142 44.95106"
  },
  {
    "objectID": "mofa-light.html#downstream-analysis",
    "href": "mofa-light.html#downstream-analysis",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "Downstream analysis",
    "text": "Downstream analysis\nWe now we have a reasonable model in which Factor1 consistently plays a vital role across all views. In the downstream analysis we can explore more factors, such as Factor1, and features weights. We can for instance aim to characterize Factor1 molecular signal and its association with available sample covariates."
  },
  {
    "objectID": "mofa-light.html#adding-annotations",
    "href": "mofa-light.html#adding-annotations",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "Adding annotations",
    "text": "Adding annotations\nWe can add samples annotations to the model now. We will add only our cancer subtypes, but could also include other covariates that my be relevant to the study, such as age or gender.\n\n# add cancer subtype to the model\nsamples_metadata(MOFAobject) &lt;- data.frame(sample=colnames(data_mofa$mirna),\n                                           subtype=breast.TCGA$data.train$subtype)\n\n\nVisualizing factors\nWe can visualize individual factor or factors combinations. We can also do that in connection to the sample groups. Let’s look at few examples:\n\n# visualize Factors 1, 2, 3\nmodel &lt;- MOFAobject\nplot_factor(model, \n  factor = 1:3,\n  color_by = \"subtype\"\n)\n\n\n\n\nVisualizaiton of individual Factors 1, 2 and 3, with cancer subtype grouping information.\n\n\n\n\n\n# add violin plot\nmodel &lt;- MOFAobject\nplot_factor(model, \n  factor = 1:3,\n  color_by = \"subtype\", \n  add_violin = T, \n  dodge = T\n  \n)\n\n\n\n\nVisualizaiton of individual Factors 1, 2 and 3, with cancer subtype grouping information and with added violin plot.\n\n\n\n\n\n# visualize combination of Factors 1 and 2\nmodel &lt;- MOFAobject\nplot_factors(model, \n  factors = 1:2,\n  color_by = \"subtype\"\n)\n\n\n\n\nVisualizaiton of combinations of Factors 1 and 2, with cancer subtype grouping information.\n\n\n\n\n\n\nAssociation analysis\nTo understand the relation between Factors and sample metadata, we can further perform an association analysis.\n\n# correlate factors with covariates\ncorrelate_factors_with_covariates(MOFAobject, \n  covariates = c(\"subtype\"), \n  plot = \"log_pval\",cluster_cols=F\n)\n\nWarning in correlate_factors_with_covariates(MOFAobject, covariates =\nc(\"subtype\"), : There are non-numeric values in the covariates data.frame,\nconverting to numeric...\n\n\n\n\n\n\n\n\n\nThe results clearly shows a strong association of Factor1 and cancer subtype. The remaining factors do not show a clear association with the cancer subtype. We could have also started from the association analysis to find out the factor that is associated with our grouping or other covariate of interest, and the focus on plotting the factors of interest.\n\n\nVisualizing weights\nWe have a strong trend of subtypes captured by Factor 1. We can now look at the weights for this factor to figure out what are the most important features that contribute to this pattern.\nFeature weights play an important role in understanding the influence of each feature on a given factor. These weights offer a quantifiable score for every feature in relation to its respective factor. Essentially, when a feature doesn’t correlate with a factor, its weight is anticipated to hover around zero. Conversely, features that are robustly associated with the factor will display large absolute weight values. The polarity of the weight whether positive or negative reveals the nature of the relationship: a positive weight suggests that the feature level elevates in instances with positive factor values and the opposite for negative weights.\nLet’s look at the top 10 features in mRNA.\n\nplot_top_weights(MOFAobject,view = \"mrna\",\n factor = 1,\n nfeatures = 10,    \n scale = T          \n)\n\n\n\n\n\n\n\n\nThe plot suggest that STC2 has a strong negative relationship with Factor1. Looking back at the score plot, we see that our Basal subtype has ended up on the right of the plot, Her2 in the middle and LumA on the left. This suggest that the expression of STC2 is higher in LumA vs Her2 and also Her2 vs LumA. Let’s check it:\n\nplot_data_scatter(MOFAobject, \n  view = \"mrna\",\n  factor = 1, features = \"STC2\",color_by = \"subtype\"\n)\n\n\n\n\n\n\n\n\nGreat. But we have so many other features, do we have a subgroup of features in our data:\n\n# plot heatmap\nsample_group &lt;- samples_metadata(MOFAobject)\nrownames(sample_group) &lt;- sample_group[,1]\n\nheatmap_plot &lt;- plot_data_heatmap(MOFAobject, \nview = \"mrna\",\n  factor = 1, features = 50,\n  cluster_rows = TRUE, cluster_cols = FALSE,annotation_samples = sample_group[,\"subtype\",drop=F],\n  show_rownames = TRUE, show_colnames = FALSE,\n  scale = \"row\"\n)\n\n'annotation_samples' provided as a data.frame, please make sure that the rownames match the sample names\n\nheatmap_plot\n\n\n\n\n\n\n\n\nWe can at least see two big groups of mRNAs having contrasting expression pattern. Let’s extract these features and investigate them further.\n\n# cut into features in two groups\nfeature_subgroups &lt;- cutree(heatmap_plot$tree_row, 2)\n\n# plot first group of extracted features\nplot_data_scatter(MOFAobject, \n  view = \"mrna\",\n  factor = 1,  \n  features = names(feature_subgroups[feature_subgroups==1]),\n  color_by = \"subtype\",\n  dot_size = 1.5\n) \n\n\n\n\nScatter plot of features (mRNAs) in the first group of expression pattern agaist Factor 1 values\n\n\n\n\n\n# plot second group of extracted features\nplot_data_scatter(MOFAobject, \n  view = \"mrna\",\n  factor = 1,  \n  features = names(feature_subgroups[feature_subgroups==2]),\n  color_by = \"subtype\", \n  dot_size = 1.5\n) \n\n\n\n\nScatter plot of features (mRNAs) in the second group of expression pattern agaist Factor 1 values\n\n\n\n\nAs it is clear the two subgroups are related to the features that are positively and negatively correlated with the first factor. This is a good indication that we can use the weights to group the features. We could use these groups to do enrichment analysis or similar, but this would be outside MOFA package.\n\nExercise 2 (MOFA) Can you perform MOFA on the test data which is in breast.TCGA$data.test?\n\nDo you see the same pattern as in the training set?\nDo the top 10 most important features overlap between training and testing?\nHow about the grouping of the features?"
  },
  {
    "objectID": "mofa.html",
    "href": "mofa.html",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "",
    "text": "Setting up environment\nYou will need to install a few packages to fully run this notebook. The main packages needed are MOFA2 and ggplot2 but in order to run Bayesian modeling you will need to install rstan. A good place to start with rstan installation is here: https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\nBiocManager::install(\"MOFA2\")\n\n# list of packages to be installed\npackages &lt;- c(\"rstan\",\"ggplot2\")\n\n# check and install missing packages\nnew_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages, dependencies = TRUE, type = \"binary\")"
  },
  {
    "objectID": "mofa.html#pca-in-r",
    "href": "mofa.html#pca-in-r",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "PCA in R",
    "text": "PCA in R\nDoing PCA in R using SVD is straight forward. We center our data and use the svd function.\n\n# center the data\ndata_centered_mrna &lt;- scale(breast.TCGA$data.train$mrna,center = TRUE,scale = FALSE)\n\n# do SVD\nsvd_mrna &lt;- svd(data_centered_mrna)\n\n# calculate the PC scores\ncalculated_scores &lt;- data_centered_mrna%*%svd_mrna$v\n\n# plot the PC scores\nplot(calculated_scores[,1:2],xlab=\"pc1\",ylab=\"pc2\",col=breast.TCGA$data.train$subtype)\n\n\n\n\n\n\n\n\nThis will give us identical results compared to the for example standard prcomp function\n\n# do PCA using prcomp\npca_prcomp &lt;- prcomp(data_centered_mrna)\n\n# plot the PCA\nplot(pca_prcomp$x[,1:2],xlab=\"pc1\",ylab=\"pc2\",col=breast.TCGA$data.train$subtype)\n\n\n\n\n\n\n\n\nIn practice there are more specialized packages that can be used to do PCA. For instance mixOmics provides a very powerful PCA method that provide us not only with standard PCA but also with extra advantages (e.g. missing value handling, plotting, handling repeated measurements etc.). See Data Integration using mixOmics labs for some examples.\nThis observed separation and overlap in the PCA plot is not just a graphical representation but is rooted in the underlying biology of these cancer subtypes. The positioning of the different groups on the PCA plot is influenced by the expression levels of various mRNAs, each contributing differently to the principal components.\nNow, as we go deeper into understanding the PCA plot, it becomes essential to explore the concept of loadings. Loadings help us interpret the contribution of each miRNA to the principal components. They provide insights into which specific miRNAs are driving the separation between different cancer subtypes observed in the PCA plot.\nWe can go ahead and plot the loadings. We start with our most important PC, that is PC1\n\n# loadings for component 1\nloadings &lt;- pca_prcomp$rotation[,1]\n\n# sort the loadings\nsorted_loadings &lt;- loadings[order(abs(loadings),decreasing = T)]\n\n# plot the loadings in a flipped bar plot\npar(mar = c(3, 6, 3, 2))\nbarplot(sorted_loadings, horiz=TRUE, las=1, \n        main=\"PCA Loadings\", xlab=\"Loadings\", \n        border=\"blue\", col=\"skyblue\")\n\n\n\n\n\n\n\n\nIn this bar plot, each bar represents a specific mRNA. The length of the bar corresponds to the value of the loading of that mRNA on PC1, indicating its contribution to this principal component. The mRNAs with the highest absolute contributions are at the bottom, and those with the lowest are at the top, making it easy to identify the most influential mRNAs. Both the length and direction of each bar provide crucial insights into the mRNA’s contribution to the first principal component (PC1). The length of the bar signifies the magnitude of the mRNA’s contribution. Longer bars indicate miRNAs that have a more substantial influence on the variance captured by PC1, highlighting them as key elements in distinguishing different patterns of gene expression within the dataset.\nThe direction of the bar adds another layer of interpretation. Bars extending to the right represent mRNAs that are positively correlated with PC1, indicating that as the values of these mRNAs increase, so does the score of PC1. Conversely, bars extending to the left suggest a negative correlation, meaning as the values of these miRNAs increase, the score of PC1 decreases. This directional information can be important in understanding the expression patterns of mRNAs in different breast cancer subtypes. For instance, mRNAs that are positively correlated with PC1 might be highly expressed in the Basal subtype but low in others, offering insights into the molecular distinctions between these cancer subtypes.\nScore plot together with loading give us powerful tool to investigate pattern in a single dataset."
  },
  {
    "objectID": "mofa.html#probabilistic-principal-component-analysis-ppca",
    "href": "mofa.html#probabilistic-principal-component-analysis-ppca",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "Probabilistic Principal Component Analysis (PPCA)",
    "text": "Probabilistic Principal Component Analysis (PPCA)\nAfter understanding the foundation of PCA, it is time to explore its probabilistic counterpart that is Probabilistic Principal Component Analysis (PPCA). While PCA provides us with a deterministic approach to do data reduction and feature extraction, PPCA introduces a probabilistic framework that models the uncertainties in the data. This transition from a deterministic method to a more flexible, probabilistic one allows for a more systematic understanding of data structures, especially when there is noise or missing values.\nIn PPCA, the relationship between the observed data vector $Y $in \\(\\mathbb{R}^{D}\\) and the latent or principal component coordinates \\(Z\\) in \\(\\mathbb{R}^{d}\\) is expressed by:\n\\[X = ZW^T + \\epsilon\\]\nWhere \\(X\\) is the observed data vector. \\(Z\\) represents the latent variables or the principal component coordinates. \\(W\\) is a matrix of size \\(d \\times D\\) that defines the linear relationship between \\(Z\\) and \\(Y\\). \\(\\epsilon\\) is a Gaussian noise term, which accounts for the variability not captured by the principal components.\nGiven our equation, we start with a prior on the latent variables, \\(Z\\), assuming they are drawn from a standard Gaussian distribution: \\[ p(Z) = N(Z; 0, I) \\] This means that the principal component coordinates have a mean of zero and an identity covariance matrix.\nGiven the linear relationship \\(Y = ZW^T + \\epsilon\\), the conditional distribution of \\(X\\) given \\(Z\\) is \\(p(X|Z) = N(X; ZW^T, \\sigma^2 I)\\) This equation suggests that the observed data \\(X\\) is normally distributed around the value produced by projecting \\(Z\\) onto the data space using \\(W^T\\), with a variance of \\(\\sigma^2\\).\nRemember now that our observed data is \\(X\\) but what we are interested in is \\(Z\\) so we need to calculate \\[ p(Z|X)\\]. Therefore using the Bayes’ rule for Gaussian distributions:\n\\[p(z|x) = \\frac{p(x|z) \\cdot p(z)}{p(x)}\\] We do not need the denominator \\(p(x)\\) explicitly as it is a normalization constant.\n\\[ p(z|x) \\propto p(x|z) \\cdot p(z) \\] In the above equation \\(p(z|x)\\) is normally distributed thus\n\\[\np(z|x)=N(z,\\mu,\\Sigma)\n\\]\nIt can be shown that \\[\n\\mu=\\sigma^{-2}\\Sigma Wx\n\\] and \\[\n\\Sigma^{-1}=I+\\sigma^{-2}CC^T\\] To estimate the model parameters, namely \\(W\\) and \\(\\sigma^2\\), we employ the Expectation-Maximization (EM) algorithm. This iterative algorithm seeks to maximize the likelihood of observing the data \\(x\\) given the model parameters.\nThe EM algorithm consists of two main steps:\n\nE-Step: Here, we estimate the distribution of the latent variable \\(z\\) given the observed data \\(x\\) and the current estimate of the model parameters.\nM-Step: In this step, we update the model parameters \\(W\\) and \\(\\sigma^2\\) to maximize the expected complete-data log-likelihood.\n\nOne challenge in directly maximizing the log-likelihood \\(log p(x)\\) (which quantifies how well our model describes the observed data) is its complexity due to the latent variable \\(z\\). To fix this, we introduce a distribution \\(q(z)\\) over \\(z\\) and derive a lower bound on the log-likelihood.\nOur goal is to express the log marginal likelihood, \\(\\log p(X)\\), in terms of some function involving \\(q(Z)\\). The marginal likelihood is defined as:\n\\[p(X) = \\int p(X, Z) dZ\\] The marginal likelihood represents the probability of the observed data under a particular model, marginalized (or summed/integrated) over all possible values of the latent variables or parameters. So essentially, we are adding up the likelihood of the data \\(X\\) for each possible value or configuration of \\(Z\\).\nUnfortunate, the integral over \\(Z\\) can make the optimization intractable. So we are going to use \\(q(Z)\\) which is chosen from a family of distributions that are tractable and easy to work with. The goal here is to find an approximation to the true but intractable posterior \\(p(Z|X)\\). To introduce \\(q(Z)\\) into our equation, we multiply the inside of the integral by \\(q(Z)\\) and divide by \\(q(Z)\\). Remember, any number divided by itself is 1 (obviously!), so this step is essentially multiplying by 1 (which does not change the value of the integral).\n\\[p(X) = \\int p(X, Z) \\times \\frac{q(Z)}{q(Z)} dZ\\] If we expand the equation we get:\n\\[p(X) = \\int q(Z) \\times \\frac{p(X, Z)}{q(Z)} dZ\\] Now things become much easier. The first thing we want to do is to work on \\(log p\\) instead of probabilities. So we will take the logarithm of both sides:\n\\[\\log p(X) = \\log \\int q(Z) \\frac{p(X, Z)}{q(Z)} dZ\\] However, we are still dealing with a complex log of integrals. What we want to do is to move the log inside the integral so that we can simplify the integrand, making the integration more tractable.\nSince log is concave, we can use Jensen’s inequality, \\(f(E[X]) \\geq E[f(X)]\\). Jensen’s inequality essentially says that for a concave function, the function of an average is always greater than or equal to the average of the function. In the context of our expression, it allows us to move the logarithm inside the integral:\n\\[\\log p(X) \\geq \\int q(Z) \\log \\frac{p(X, Z)}{q(Z)} dZ\\] Expanding the logarithm of the fraction: \\[\\log p(X) \\geq \\int q(Z) [\\log p(X, Z) - \\log q(Z)] dZ\\]\nThis is simply using the logarithmic property: \\[\\log \\frac{a}{b} = \\log a - \\log b\\]\nNow, we separate the integrals: \\[\\log p(X) \\geq \\int q(Z) \\log p(X, Z) dZ - \\int q(Z) \\log q(Z) dZ\\]\nThe two integrals represent two terms: 1. The first integral \\(\\int q(Z) \\log p(X, Z) dZ\\) represents the expected joint log-likelihood of the data and the latent variables, when the expectation is taken with respect to \\(q(Z)\\).\n\nThe second integral \\(\\int q(Z) \\log q(Z) dZ\\) represents the entropy of the variational distribution \\(q(Z)\\).\n\nUsing the definition of expectation, the first integral is: \\[E_q[\\log p(X, Z)]\\] And breaking down the joint probability using the definition of conditional probability: \\[E_q[\\log p(X, Z)] = E_q[\\log p(Z) + \\log p(X|Z)]\\]\nThe second integral, as mentioned, is just the negative entropy: \\[-H(q(Z))\\]\nCombining these terms, we get: \\[\\log p(X) \\geq E_q[\\log p(Z) + \\log p(X|Z)] - H(q(Z))\\]\nWhich we can rearrange to: \\[\\log p(X) \\geq H(q(Z)) + E_q[\\log p(Z) + \\log p(X|Z)]\\] The right-hand side of the inequality is the ELBO (Evidence Lower BOund (ELBO)), which we try to maximize. The entropy term quantifies the uncertainty associated with the distribution and the other term is the expected joint log-likelihood of the data and latent variables under the variational distribution.\nAnyway, given the likelihood function we can now derive our E and M steps of EM.\nWe can simply look at what we derived for \\(p(z|x)\\) which was: \\[\n\\mu=\\sigma^{-2}\\Sigma Wx\n\\] and \\[\n\\Sigma^{-1}=I+\\sigma^{-2}CC^T\\]\nWe can just replace the values to get to expected value of \\(Z\\)\n\\[\nZ=\\sigma^{-2}(I+\\sigma^{-2}CC^T)^{-1} Wx\n\\]\nWe have \\(N\\) data points so we need to sum over them so, we have: \\[\\sum_{n=1}^{N} log p(x_n) \\geq \\sum_{n=1}^{N} \\left( H(q(z_n)) + E_{q(z_n)}[log p(z_n) + log p(x_n|z_n)] \\right)\\]\nExpanding the expectation term for \\(N\\) data points and using the Gaussian distributions, we get: \\[\\sum_{n=1}^{N} E_{q(z_n)}[log p(z_n) + log p(x_n|z_n)] = \\] \\[ - \\frac{1}{2\\sigma^2} \\sum_{n=1}^{N} \\left( \\lVert x_n - W^⊤z_n \\rVert^2 + \\text{Tr}{W^⊤Σ_nW} \\right) - \\frac{1}{2\\sigma^2_{\\text{old}}} \\sum_{n=1}^{N} \\lVert z_n \\rVert^2 - \\frac{N}{2} \\text{Tr}{Σ_n} \\]\nwhere \\(x_n\\) is the observed data point and \\(z_n\\) is the latent representation of \\(x_n\\). The equation might seem complex but it has three main parts:\n\nThe reconstruction error (\\(\\lVert x_n - W^⊤z_n \\rVert^2\\)), which quantifies how well our model can generate the observed data from the latent variables.\nThe variance or spread of the latent variables (\\(\\lVert z_n \\rVert^2\\)).\nThe term (\\(\\text{Tr}{W^⊤Σ_nW}\\)) which captures the total variance explained by the latent variables.\n\nTo determine the model parameters that maximize this bound, we are going to differentiate with respect to \\(W\\) and \\(\\sigma^2\\). Setting these derivatives to zero and we are almost there!\n\\[W = \\left( \\sum_{n=1}^{N} Σ_n + z_n z_n^⊤ \\right)^{-1} \\sum_{n=1}^{N} z_n x_n^⊤\\]\nand \\[\\sigma^2 = \\frac{1}{ND} \\sum_{n=1}^{N} \\left( \\lVert x_n - W^⊤z_n \\rVert^2 + \\text{Tr}{W^⊤Σ_nW} \\right)\\]\nRecall our expanded equation for the lower bound on the log-likelihood: \\[\\sum_{n=1}^{N} log p(x_n) \\geq \\sum_{n=1}^{N} \\left( H(q(z_n)) + E_{q(z_n)}[log p(z_n) + log p(x_n|z_n)] \\right)\\]\nThe entropy \\(H\\) of a Gaussian with covariance matrix \\(Σ\\) is: \\[H = \\frac{1}{2} log |Σ|\\]\nPutting this into our equation, we obtain: \\[\\sum_{n=1}^{N} log p(x_n) \\geq \\sum_{n=1}^{N} \\left( \\frac{1}{2} log |Σ| + E_{q(z_n)}[log p(z_n) + log p(x_n|z_n)] \\right)\\]\nWe can further expand the right-hand side: \\[log p(x) \\geq -ND\\left(1 + log \\sigma^2\\right) - N\\left(Tr\\{Σ\\} - log |Σ|\\right) - \\frac{1}{2\\sigma^2_{\\text{old}}} \\sum_{n=1}^{N} \\lVert z_n \\rVert^2\\]\nHere: \\(N\\) represents the number of data points. \\(D\\) is the dimension of the data vector $ x_n $.\nThis equation represents the EM bound after performing the M-step. The objective in the M-step of the EM algorithm is to maximize this bound with respect to the model parameters \\(W\\) and \\(\\sigma^2\\). By doing so, we iteratively refine our model to better fit the observed data.\n\nPPCA R implementation\nLet’s try to implement the above in R:\n\n# load the breast cancer dataset for training\ninput_data &lt;- breast.TCGA$data.train$mrna\n\n# define the number of data points (samples) and the dimensionality of the data (genes/features)\nN_data &lt;- nrow(input_data)\nD_data &lt;- ncol(input_data)\n\n# define the number of principal components to be extracted\nnPcs &lt;- 2\n\n# define the convergence threshold and maximum number of iterations for the EM algorithm\nthreshold &lt;- 0.0001\nmaxIterations &lt;- 100\n\n# set a seed for reproducibility\nset.seed(123)\n\n# initialization: randomly initialize the W matrix from the data\nW &lt;- t(input_data[sample(N_data, size = nPcs), , drop = FALSE])\nW &lt;- matrix(rnorm(length(W)), nrow(W), ncol(W))\n\n# precompute W'W for efficiency\nWtW &lt;- crossprod(W)\n\n# compute the latent representation Z based on the initial W\nZ &lt;- input_data %*% W %*% solve(WtW)\n\n# calculate the initial reconstruction and its error\nreconstructed &lt;- Z %*% t(W)\nerror_ss &lt;- sum((reconstructed - input_data)^2) / (N_data * D_data)\n\n# initialize the iteration counter and the previous objective value for convergence checking\niteration &lt;- 1\nprevious &lt;- Inf\n\n# start the EM algorithm\nwhile (TRUE) {\n  # E-Step: estimate the covariance of the latent variable Z\n  Z_cov &lt;- solve(diag(nPcs) + WtW/error_ss)\n  \n  # compute the posterior mean of Z\n  Z &lt;- input_data %*% W %*% Z_cov/error_ss\n  ZtZ &lt;- crossprod(Z)\n  \n  # M-Step: update W based on the estimated Z\n  W &lt;- (t(input_data) %*% Z) %*% solve((ZtZ + N_data * Z_cov))\n  WtW &lt;- crossprod(W)\n  \n  # recalculate the reconstruction error based on the updated W\n  error_ss &lt;- (sum((W %*% t(Z) - t(input_data))^2) + \n                 N_data * sum(WtW %*% Z_cov))/(N_data * D_data)\n  \n  # calculate the EM objective (the lower bound of the log-likelihood)\n  obj_val &lt;- N_data * (D_data * log(error_ss) + \n                         sum(diag(Z_cov)) - log(det(Z_cov))) + sum(diag(ZtZ))\n  \n  # check for convergence\n  relative_change &lt;- abs(1 - obj_val/previous)\n  previous &lt;- obj_val\n  \n  iteration &lt;- iteration + 1\n  \n  if (relative_change &lt; threshold | iteration &gt; maxIterations) {\n    break\n  } \n}\n\n# orthogonalize W for stability\nW &lt;- svd(W)$u\n\n# recalculate eigenvalues and eigenvectors after orthogonalization\neig_vals &lt;- eigen(cov(input_data %*% W))$values\neig_vecs &lt;- eigen(cov(input_data %*% W))$vectors\n\n# update W based on the new eigenvectors\nloadings &lt;- W %*% eig_vecs\nscores &lt;- input_data %*% loadings\n\nplot(scores,xlab=\"pc1\",ylab=\"pc2\",col=breast.TCGA$data.train$subtype)"
  },
  {
    "objectID": "mofa.html#bayesian-principal-component-analysis-bpca",
    "href": "mofa.html#bayesian-principal-component-analysis-bpca",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "Bayesian Principal Component Analysis (BPCA)",
    "text": "Bayesian Principal Component Analysis (BPCA)\nIn the traditional Probabilistic Principal Component Analysis (PPCA), we made certain probabilistic assumptions, specifically over the latent variables \\(Z\\) and the noise term \\(\\epsilon\\). The other parameters were treated as non-probabilistic. However, in BPCA, we take a step further by assuming a probabilistic distribution over all the parameters. This provides a more comprehensive probabilistic model.\nGiven the linear relationship: \\[ \\mathbf{x}_n = \\mathbf{W}\\mathbf{z}_n + \\boldsymbol{\\epsilon}_n \\]\nIn BPCA, we make the following probabilistic assumptions:\n\nThe latent variables \\(\\mathbf z\\) follow a standard normal distribution: \\[ \\mathbf z \\sim \\mathcal{N}(0, 1) \\]\nThe weight matrix \\(\\mathbf{W}\\) also follows a standard normal distribution: \\[ \\mathbf{W} \\sim \\mathcal{N}(0, 1) \\] Both the latent variables and the weight matrix are assumed to follow standard normal distributions. This assumption is in line with the traditional PCA where the principal components are orthogonal and typically standardized to have unit variance. The normal distribution assumption implies that the values of the latent variables and weights are most likely to be around their means (which is zero) and decrease in likelihood as they move away from the mean.\nThe precision parameter \\(\\tau\\) follows a gamma distribution with parameters \\(\\alpha_0\\) and \\(\\beta_0\\): \\[ \\tau \\sim \\mathcal{G}(\\alpha_0, \\beta_0) \\] The gamma distribution is a flexible choice for modeling positive continuous variables like precision. The shape and rate parameters \\(\\alpha_0\\) and \\(\\beta_0\\) can be seen as hyperparameters, and their values can be chosen based on prior knowledge or set in a non-informative manner.\nThe noise term \\(\\epsilon_n\\) is normally distributed with mean 0 and precision \\(\\tau^{-1}\\): \\[ \\epsilon_n \\sim \\mathcal{N}(0, \\tau^{-1}) \\] This is a common assumption in many statistical models, implying that the errors (or deviations from the model) are symmetrically distributed around zero. The variance of this noise is controlled by the precision parameter \\(\\tau\\).\nThe observed data \\(\\mathbf{x}_n\\) follows a normal distribution with mean \\(\\mathbf W\\mathbf z_n\\) and covariance \\(\\tau^{-1}\\): \\[ \\mathbf{x}_n \\sim \\mathcal{N}(\\mathbf W\\mathbf z_n, \\tau^{-1} \\mathbf ) \\] The choice of a normal distribution here signifies that the observed data points are most likely to lie close to the subspace spanned by the principal components and deviations from this subspace are captured by the noise term.\n\nOne of the main advantages of BPCA over traditional PCA is that it provides a probabilistic framework, allowing us to quantify the uncertainties associated with the estimated parameters. In addition, BPCA can automatically determine the number of relevant principal components, unlike traditional PCA where the number of components needs to be specified or chosen.\nIn BPCA, our primary parameters of interest are the latent variables \\(\\mathbf{z}\\), the weight matrix \\(\\mathbf{W}\\), the precision parameter \\(\\tau\\), and the noise term \\(\\epsilon_n\\). Bayesian inference provides a good way to estimate these parameters, but exact inference can be computationally difficult, especially in high-dimensional setting This is where we can use algorithms like Variational Inference (VI) or MCMC (Markov Chain Monte Carlo) to simplify the problem. We are going to use Variational Inference here. This is an approximate inference technique that turns the inference problem into an optimization problem. The idea is to approximate the true posterior distribution of the parameters with a simpler, factorized distribution, referred to as the “mean-field” approximation.\nWe are going to assume that the approximate posterior factorizes across all the parameters, i.e., \\[q(\\mathbf{z}, \\mathbf{W}, \\tau, \\epsilon_n) = q(\\mathbf{z})q(\\mathbf{W})q(\\tau)q(\\epsilon_n)\\]\nThe goal here is to minimize the KL divergence between the approximate posterior and the true posterior. This is equivalent to ELBO, which as said before provides a lower bound to the log marginal likelihood of the data. Using an iterative algorithm, we adjust the parameters of our mean-field distribution to maximize the ELBO (we saw in PPCA how ELBO is calculated). Common techniques include coordinate ascent or gradient-based methods can be used. Once the ELBO is maximized, the parameters of the factorized distributions give us the approximate posterior means and variances for \\(\\mathbf{z}\\), \\(\\mathbf{W}\\), \\(\\tau\\), and \\(\\epsilon_n\\). We can then use the estimated parameters to generate new data points, reconstruct the original data with reduced dimensions, or project new observations onto the principal components, thereby facilitating visualization, clustering, etc.\n\nBPCA R implementation\nR provides an excellent interface to Stan through the rstan package, allowing users to build, fit, and interrogate complex Bayesian models with ease. We are going to use rstan here.\nYou can run the script if you have successfully installed rstan package otherwise you can skip running the scripts and continue reading.\n\n# Load necessary libraries\nrequire(rstan)\n\n# Configure rstan options\nrstan_options(auto_write = FALSE)\n# Set the number of cores for parallel processing\noptions(mc.cores = parallel::detectCores())\n\n# Define the BPCA model in Stan language\nbpca &lt;- \"\ndata {\n        int&lt;lower=0&gt; N; // Number of samples\n        int&lt;lower=0&gt; D; // The original dimension\n        int&lt;lower=0&gt; K; // The latent dimension\n        matrix[N, D] X; // The data matrix\n    }\n\n    parameters {\n        matrix[N, K] Z; // The latent matrix\n        matrix[D, K] W; // The weight matrix\n        real&lt;lower=0&gt; tau; // Noise term \n    }\n\n    transformed parameters{\n        real&lt;lower=0&gt; t_tau; // Transformed precision term for noise\n    t_tau = inv(sqrt(tau)); // Compute the inverse of the square root of tau\n    }\n    \n    model {\n        // Prior distributions for the latent matrix and weight matrix\n        to_vector(Z) ~ normal(0,1);\n        to_vector(W)~ normal(0, 1);\n        \n        // Prior distribution for the noise precision term\n        tau ~ gamma(1,1);               \n        \n        // Likelihood for the observed data\n        to_vector(X) ~ normal(to_vector(Z*W'), t_tau);\n\n    }\"\n\n# Compile the Stan model\nbpca_model &lt;- stan_model(model_code = bpca)\n\n# Load and preprocess the breast cancer dataset\nX &lt;- scale(breast.TCGA$data.train$mrna,scale = T,center = T)\n\n# Prepare the data for Stan\ndata_input &lt;- list(N = dim(X)[1], D = dim(X)[2], K = 2, X = X)\n\n# Set random seed for reproducibility\nset.seed(200)\n\n# Fit the BPCA model using Variational Bayes with the meanfield algorithm\nbpca_fit &lt;- vb(bpca_model, data = data_input, algorithm = \"meanfield\", iter = 1000, output_samples = 100)\n\nChain 1: ------------------------------------------------------------\nChain 1: EXPERIMENTAL ALGORITHM:\nChain 1:   This procedure has not been thoroughly tested and may be unstable\nChain 1:   or buggy. The interface is subject to change.\nChain 1: ------------------------------------------------------------\nChain 1: \nChain 1: \nChain 1: \nChain 1: Gradient evaluation took 0.001777 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 17.77 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Begin eta adaptation.\nChain 1: Iteration:   1 / 250 [  0%]  (Adaptation)\nChain 1: Iteration:  50 / 250 [ 20%]  (Adaptation)\nChain 1: Iteration: 100 / 250 [ 40%]  (Adaptation)\nChain 1: Iteration: 150 / 250 [ 60%]  (Adaptation)\nChain 1: Iteration: 200 / 250 [ 80%]  (Adaptation)\nChain 1: Success! Found best value [eta = 1] earlier than expected.\nChain 1: \nChain 1: Begin stochastic gradient ascent.\nChain 1:   iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes \nChain 1:    100       -40058.050             1.000            1.000\nChain 1:    200       -38590.189             0.519            1.000\nChain 1:    300       -38538.645             0.020            0.038\nChain 1:    400       -38530.234             0.001            0.001   MEAN ELBO CONVERGED   MEDIAN ELBO CONVERGED\nChain 1: \nChain 1: Drawing a sample of size 100 from the approximate posterior... \nChain 1: COMPLETED.\n\n# Extract the latent scores from the fit\nscores &lt;- apply(extract(bpca_fit,\"Z\")[[1]], c(2,3), mean)\n\n# Plot the latent scores with colors representing subtypes\nplot(scores,xlab=\"pc1\",ylab=\"pc2\",col=breast.TCGA$data.train$subtype)\n\n\n\n\nThe Stan code here is structured into four main sections, each serving a purpose in the Bayesian modeling framework. The data section declares the observed data and its dimensions, specifying the number of samples (N), the original data dimension (D), the latent dimension (K), and the actual data matrix (X). The parameters section introduces the model’s primary unknowns: the latent matrix (Z) that captures the underlying structure, the weight matrix (W) that maps latent variables to the observed space, and the noise precision parameter (tau). In the transformed parameters section, we computed a derived parameter, t_tau as the inverse of the square root of tau, offering a transformed precision term for the noise. Finally, the model section defines the probabilistic relationships in the model. Here, prior distributions are set for the latent matrix, weight matrix, and noise precision term, while the likelihood of the observed data is defined based on the latent scores, weight matrix, and the transformed noise precision.\nThe rest of the code essentially just compile the Stan code, prepare the data and run the inference. Probably one of the trickiest part of the code is apply(extract(bpca_fit,\"Z\")[[1]], c(2,3), mean). Can you guess what does this do?\nIf you remember, in the context of Bayesian analysis, we don’t deal with point estimate but we have a complete distribution over the parameters of interest. As the consequence we don’t have a single PCA score (PC) for each data point but rather we have a complete distribution. we chose to draw 100 samples (output_samples) from this distribution so we have for each data point 100 scores. I just took the average but we could take any of them or plot the complete distribution.\nWe have now implemented a complete Bayesian PCA in R and Stan. Time to move towards integration using two data views only."
  },
  {
    "objectID": "mofa.html#cca-in-r",
    "href": "mofa.html#cca-in-r",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "CCA in R",
    "text": "CCA in R\nLet’s have a look at how we can derive this in R using miRNA and mRNA data\n\n# center both of the data sets\nX_centered &lt;- scale(breast.TCGA$data.train$mrna, scale = FALSE)\nY_centered &lt;- scale(breast.TCGA$data.train$protein, scale = FALSE)\n\n# calculate cross-covariance matrix\ncross_cov &lt;- t(X_centered)%*%Y_centered\n\n# do a svd (single eigenvector) this is going to give us a signle CCA component\nsvd_result &lt;- svd(cross_cov,1,1)\n\n# extract the vectors\nU &lt;- svd_result$u\nV &lt;- svd_result$v\n\n# calculate the first canonical vectors (the most correlated latent factors)\ncanonical_vars_X &lt;- X_centered %*% U\ncanonical_vars_Y &lt;- Y_centered %*% V\n\n# deflate the original matrices\nX_centered &lt;- X_centered - canonical_vars_X %*% t((t(X_centered)%*%(canonical_vars_X)%*%solve(t(canonical_vars_X)%*%(canonical_vars_X))))\n\nY_centered &lt;- Y_centered - canonical_vars_Y %*% \n  t(t(Y_centered)%*%(canonical_vars_Y)%*%solve(t(canonical_vars_Y)%*%(canonical_vars_Y)))\n\n# redo the svd for the second component\ncross_cov &lt;- t(X_centered)%*%Y_centered\nsvd_result &lt;- svd(cross_cov,1,1)\n\nU &lt;- svd_result$u\nV &lt;- svd_result$v\n\n# calculate the second canonical vectors (the second most correlated latent factors)\ncanonical_vars_X2 &lt;- X_centered %*% U\ncanonical_vars_Y2 &lt;- Y_centered %*% V\n\npar(mfrow=c(2,2))\nplot(canonical_vars_X,canonical_vars_X2,col=breast.TCGA$data.train$subtype,xlab=\"l1\",ylab=\"l2\",main=\"CCA mRNA\")\nplot(canonical_vars_Y,canonical_vars_Y2,col=breast.TCGA$data.train$subtype,xlab=\"l1\",ylab=\"l2\",main=\"CCA protein\")\n\nplot(canonical_vars_X,canonical_vars_Y,col=breast.TCGA$data.train$subtype,xlab=\"mRNA\",ylab=\"protein\",main=\"l1\")\nplot(canonical_vars_X2,canonical_vars_Y2,col=breast.TCGA$data.train$subtype,xlab=\"mRNA\",ylab=\"protein\",main=\"l2\")\n\n\n\n\n\n\n\n\nThe plot above clearly shows that we ended up having a shared pattern in l1 (first CCA component). L1 captures the primary mode of correlation between protein and mRNA expression data. It represents the linear combinations of protein and mRNAs that are most strongly correlated. Since our interest right now is in the suptypes, we can probably ignore the second latent factor but we might as well try to explaining based on some other factors.\nIn the context of CCA, loadings play a role similar to that in PCA, yet they have a distinct interpretation. Similar to PCA, where loadings indicate the contribution of each original variable to the principal components, in CCA, the loadings show the contribution of each variable to the canonical variables. However, the difference lies in their meaning. While PCA loadings represent the contribution to the variance within a single dataset, CCA loadings show the contribution to the correlation between two data sets."
  },
  {
    "objectID": "mofa.html#bayesian-canonical-correlation-analysis-bcca",
    "href": "mofa.html#bayesian-canonical-correlation-analysis-bcca",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "Bayesian Canonical Correlation Analysis (BCCA)",
    "text": "Bayesian Canonical Correlation Analysis (BCCA)\nThe idea of Bayesian Canonical Correlation Analysis (BCCA) is simple. Recall that in BPCA we had \\(X = ZW^T + \\epsilon\\) in CCA however we have two data views (\\(X\\) and \\(Y\\)) that we are after a shared latent factor for them. So we can rewrite our BPCA equations:\n\\[X = ZW_x^T + \\epsilon_x\\] \\[Y = ZW_y^T + \\epsilon_y\\]\nIf you look a the equation we can see we have a shared latent factor (\\(Z\\)) but weights and noise are different across the dataset. So our latent factor \\(Z\\) is going to capture the shared pattern using a global score and in addition the weights can be used to project the original data into data view specific scores.\nIn BCCA also: 1. The latent variables \\(\\mathbf z\\) follow a standard normal distribution: \\[ \\mathbf z \\sim \\mathcal{N}(0, 1) \\] 2. The weight matrix for both data sets \\(\\mathbf{W_{x,y}}\\) also follows a standard normal distribution: \\[ \\mathbf{W_{x,y}} \\sim \\mathcal{N}(0, 1) \\]\n\nThe precision parameter \\(\\tau_{x,y}\\) follows a gamma distribution with parameters \\(\\alpha_0\\) and \\(\\beta_0\\): \\[ \\tau \\sim \\mathcal{G}(\\alpha_0, \\beta_0) \\]\nThe noise term \\(\\epsilon_n\\) is also normally distributed for both dataset with mean 0 and precision \\(\\tau^{-1}\\): \\[ \\epsilon_n \\sim \\mathcal{N}(0, \\tau^{-1}) \\]\nThe observed data \\(\\mathbf{x}_n\\) and \\(\\mathbf{y}_n\\) follows a normal distribution with mean \\(\\mathbf W_x\\mathbf z_n\\) and \\(\\mathbf W_y\\mathbf z_n\\) and covariance \\(\\tau_{x,y}^{-1}\\) so \\[ \\mathbf{x}_n \\sim \\mathcal{N}(\\mathbf W_x\\mathbf z_n, \\tau_x^{-1} \\mathbf ) \\] and \\[ \\mathbf{y}_n \\sim \\mathcal{N}(\\mathbf W_y\\mathbf z_n, \\tau_y^{-1} \\mathbf ) \\].\n\nThe rest of the optimization etc are similar to those of BPCA.\n\nBCCA R implementation\n\n# Load the rstan library for Bayesian analysis using Stan\nrequire(rstan)\n\n# Set rstan options for automatic caching and multi-core processing\nrstan_options(auto_write = FALSE)\noptions(mc.cores = parallel::detectCores())\n\n# Fix the random seed for reproducibility\nset.seed(100)\n\n# Define the Canonical Correlation Analysis (CCA) model in Stan language\ncca2 &lt;- \"\ndata {\n        int&lt;lower=0&gt; N; // Number of samples\n        int&lt;lower=0&gt; D1; // The original dimension\n        int&lt;lower=0&gt; D2; // The original dimension\n        int&lt;lower=0&gt; K; // The latent dimension\n        matrix[N, D1] X1; // The data matrix\n        matrix[N, D2] X2; // The data matrix\n    }\n\n    parameters {\n        matrix[N, K] Z; // The latent matrix\n        matrix[D1, K] W1; // The weight matrix\n        matrix[D2, K] W2; // The weight matrix\n        real&lt;lower=0&gt; tau1; // Noise term \n        real&lt;lower=0&gt; tau2; // Noise term \n    }\n\n    transformed parameters{\n        real&lt;lower=0&gt; t_tau1;\n    t_tau1 = inv(sqrt(tau1));\n    \n        real&lt;lower=0&gt; t_tau2;\n    t_tau2 = inv(sqrt(tau2));\n    }\n    model {\n        tau1 ~ gamma(1,1);\n        tau2 ~ gamma(1,1);\n        to_vector(Z) ~ normal(0,1);\n        to_vector(W1)~ normal(0, 1);\n        to_vector(W2)~ normal(0, 1);\n        to_vector(X1) ~ normal(to_vector(Z*W1'), t_tau1);\n        to_vector(X2) ~ normal(to_vector(Z*W2'), t_tau2);\n\n    }\"\n\n# Compile the Stan model for CCA\ncca_model &lt;- stan_model(model_code = cca2)\n\n# Load and preprocess the breast cancer mRNA and protein datasets\nX1 &lt;- scale(breast.TCGA$data.train$mrna,scale = F,center = T)\nY &lt;- scale(breast.TCGA$data.train$protein,scale = F,center = T)\n\n# Prepare the data for the Stan model\ndata &lt;- list(N = dim(X)[1], D1 = dim(X)[2], K = 2, X1 = X1, X2 = Y, D2 = dim(Y)[2])\n\n# Set another random seed for reproducibility in the inference step\nset.seed(200)\n# Fit the CCA model using Variational Bayes with the meanfield algorithm\ncca_fit &lt;- vb(cca_model, data = data, algorithm = \"meanfield\", iter = 1000, output_samples = 100)\n\nChain 1: ------------------------------------------------------------\nChain 1: EXPERIMENTAL ALGORITHM:\nChain 1:   This procedure has not been thoroughly tested and may be unstable\nChain 1:   or buggy. The interface is subject to change.\nChain 1: ------------------------------------------------------------\nChain 1: \nChain 1: \nChain 1: \nChain 1: Gradient evaluation took 0.0013 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 13 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Begin eta adaptation.\nChain 1: Iteration:   1 / 250 [  0%]  (Adaptation)\nChain 1: Iteration:  50 / 250 [ 20%]  (Adaptation)\nChain 1: Iteration: 100 / 250 [ 40%]  (Adaptation)\nChain 1: Iteration: 150 / 250 [ 60%]  (Adaptation)\nChain 1: Iteration: 200 / 250 [ 80%]  (Adaptation)\nChain 1: Success! Found best value [eta = 1] earlier than expected.\nChain 1: \nChain 1: Begin stochastic gradient ascent.\nChain 1:   iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes \nChain 1:    100       -60643.836             1.000            1.000\nChain 1:    200       -59084.380             0.513            1.000\nChain 1:    300       -58991.764             0.014            0.026\nChain 1:    400       -58978.129             0.001            0.002   MEAN ELBO CONVERGED   MEDIAN ELBO CONVERGED\nChain 1: \nChain 1: Drawing a sample of size 100 from the approximate posterior... \nChain 1: COMPLETED.\n\n\nWarning: Pareto k diagnostic value is 4.98. Resampling is disabled. Decreasing\ntol_rel_obj may help if variational algorithm has terminated prematurely.\nOtherwise consider using sampling instead.\n\n# Extract and compute the average latent scores from the fit for global, mRNA, and protein data\nscores_global &lt;- apply(extract(cca_fit,\"Z\")[[1]], c(2,3), mean)\nscores_x &lt;- X1%*% apply(extract(cca_fit,\"W1\")[[1]], c(2,3), mean)\nscores_y &lt;- Y%*% apply(extract(cca_fit,\"W2\")[[1]], c(2,3), mean)\n\n# Plot the latent scores for mRNA, protein, and global datasets in a 2x2 grid layout\npar(mfrow=c(2,2))\nplot(scores_x, col=breast.TCGA$data.train$subtype, main=\"BCCA mRNA\", xlab=\"L1\", ylab=\"L2\")\nplot(scores_y, col=breast.TCGA$data.train$subtype, main=\"BCCA protein\", xlab=\"L1\", ylab=\"L2\")\nplot(scores_global, col=breast.TCGA$data.train$subtype, main=\"BCCA global\", xlab=\"L1\", ylab=\"L2\")\n\n\n\n\nBased on the scores, can we say if we have captured shared pattern between our data sets? do A PCA on each of the data sets and compare the results with what we got here"
  },
  {
    "objectID": "mofa.html#gfa-mofa-r-implementation",
    "href": "mofa.html#gfa-mofa-r-implementation",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "GFA MOFA R implementation",
    "text": "GFA MOFA R implementation\n\n# Fix the random seed for reproducibility\nset.seed(100)\n\n# Define GFA\ngfa &lt;- \"\ndata {\n    int&lt;lower=1&gt; N;             // Number of data points\n    int&lt;lower=1&gt; K;             // Dimensionality of latent space\n    int&lt;lower=1&gt; M;             // Number of modalities\n    int&lt;lower=1&gt; SumP;          // Total number of features across all modalities\n    int&lt;lower=1&gt; P[M];          // Number of features for each modality\n    matrix[N, SumP] x;          // Concatenated data\n}\n\nparameters {\n    matrix[K, SumP] W;          // Factor loading matrix\n    vector&lt;lower=0&gt;[M] tau;       // Precision for each modality\n    matrix[N, K] z;             // Latent variables\n    matrix&lt;lower=0&gt;[M,K] alpha; // View-specific ARD prior\n}\n\n    transformed parameters{\n        vector&lt;lower=0&gt;[M] t_tau;\n    t_tau = inv(sqrt(tau));\n    \n    }\n\nmodel {\n\n    // fix z first\n    to_vector(z) ~ normal(0,1);\n    tau ~ gamma(1, 1);\n    // Priors\n    int start;\n    start = 0;\n        for (m in 1:M) {\n            for (d in 1:P[m]) {\n                start = start + 1;   \n                W[,start] ~ normal(0,1);\n                x[,start] ~ normal(z*W[,start], t_tau[m]);  \n                \n            }\n        }\n\n    \n}\n\"\n\n\n\ngfa_model &lt;- stan_model(model_code = gfa)\n\n# Load and preprocess the breast cancer mRNA, mirna and protein datasets\nX1 &lt;- scale(breast.TCGA$data.train$mrna,scale = F,center = T)\nX2 &lt;- scale(breast.TCGA$data.train$protein,scale = F,center = T)\nX3 &lt;- scale(breast.TCGA$data.train$mirna,scale = F,center = T)\n\n# prepare the list\nmatrices_list &lt;- list(\n  X1,\n  X2,\n  X3\n)\n\ncombined_data &lt;- do.call(cbind,matrices_list)\n# Prepare the data for the Stan model\nstan_data &lt;- list(\n  N = sapply(matrices_list, nrow)[1],\n  K = 2,\n  P = sapply(matrices_list, ncol),\n  M = length(matrices_list),\n  x = combined_data,\n  SumP = ncol(combined_data)\n  \n)\n\n# Set another random seed for reproducibility in the inference step\nset.seed(2000)\n\n# Fit the GFA model using Variational Bayes with the meanfield algorithm\ngfa_fit &lt;- vb(gfa_model, data = stan_data, algorithm = \"meanfield\", iter = 1000, output_samples = 100)\n\nChain 1: ------------------------------------------------------------\nChain 1: EXPERIMENTAL ALGORITHM:\nChain 1:   This procedure has not been thoroughly tested and may be unstable\nChain 1:   or buggy. The interface is subject to change.\nChain 1: ------------------------------------------------------------\nChain 1: \nChain 1: \nChain 1: \nChain 1: Gradient evaluation took 0.004519 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 45.19 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Begin eta adaptation.\nChain 1: Iteration:   1 / 250 [  0%]  (Adaptation)\nChain 1: Iteration:  50 / 250 [ 20%]  (Adaptation)\nChain 1: Iteration: 100 / 250 [ 40%]  (Adaptation)\nChain 1: Iteration: 150 / 250 [ 60%]  (Adaptation)\nChain 1: Iteration: 200 / 250 [ 80%]  (Adaptation)\nChain 1: Success! Found best value [eta = 1] earlier than expected.\nChain 1: \nChain 1: Begin stochastic gradient ascent.\nChain 1:   iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes \nChain 1:    100       -99211.110             1.000            1.000\nChain 1:    200       -97573.911             0.508            1.000\nChain 1:    300       -97485.188             0.009            0.017   MEAN ELBO CONVERGED\nChain 1: \nChain 1: Drawing a sample of size 100 from the approximate posterior... \nChain 1: COMPLETED.\n\n\nWarning: Pareto k diagnostic value is 26.79. Resampling is disabled. Decreasing\ntol_rel_obj may help if variational algorithm has terminated prematurely.\nOtherwise consider using sampling instead.\n\n# Extract and compute the average latent scores from the fit for global, mRNA, and protein data\nscores_global &lt;- apply(extract(gfa_fit,\"z\")[[1]], c(2,3), mean)\nW_mean &lt;-apply(extract(gfa_fit,\"W\")[[1]], c(2,3), mean)\nW_mean_no_sparse&lt;-W_mean\n# Separate W_mean based on modalities\nW_list &lt;- list()\nstart_col &lt;- 1\nfor(m in 1:stan_data$M) {\n    end_col &lt;- start_col + stan_data$P[m] - 1\n    W_list[[m]] &lt;- W_mean[, start_col:end_col]\n    start_col &lt;- end_col + 1\n}\n\nscores &lt;- mapply(function(x,y){list(x%*%t(y))},x=matrices_list,y=W_list)\n\n\n# Plot the latent scores for mRNA, protein, and global datasets in a 2x2 grid layout\npar(mfrow=c(2,2))\nplot(scores[[1]], col=breast.TCGA$data.train$subtype, main=\"GFA mRNA\", xlab=\"L1\", ylab=\"L2\")\nplot(scores[[2]], col=breast.TCGA$data.train$subtype, main=\"GFA protein\", xlab=\"L1\", ylab=\"L2\")\nplot(scores[[3]], col=breast.TCGA$data.train$subtype, main=\"GFA miRNA\", xlab=\"L1\", ylab=\"L2\")\nplot(scores_global, col=breast.TCGA$data.train$subtype, main=\"GFA global\", xlab=\"L1\", ylab=\"L2\")\n\n\n\n\nIn the code above, since Stan does not support ragged data structures (data structures with different lengths), we had to concatenate the data first, but then instruct Rstan to put different assumptions on different columns of the data depending on which modality it originates from.\nThe results are more or less clear; we have captured the shared pattern across these three data sets.\nOne more thing before talking about MOFA is that we have been omitting sparsity considerations in our model. Sparsity plays a crucial role in high-dimensional data analysis, ensuring that the model remains interpretable and avoids overfitting. Two popular approaches to incorporate sparsity are Automatic Relevance Determination (ARD) and the Spike-and-slab prior.\nARD is a form of Bayesian regularization where each feature is assigned its own regularization coefficient, allowing the model to effectively “turn off” irrelevant features by pushing their coefficients towards zero. This results in a more interpretable model where only the most relevant features contribute to the outcome. Mathematically, this can be represented as: \\[p(\\mathbf{W}|\\boldsymbol{\\alpha}) = \\prod_{k=1}^{K} N(\\mathbf{w}_{:,k};0,\\frac{1}{\\alpha_{k}}I_{D})\\] \\[p(\\boldsymbol{\\alpha}) = \\prod_{k=1}^{K} \\mathcal{G}(\\alpha_k; a_0^\\alpha, b_0^\\alpha)\\] where \\(W\\) is the weight matrix and \\(\\alpha\\) represents the precision of the weights. The Gamma distribution shapes the hyperparameters \\(\\boldsymbol{\\alpha}\\), capturing the uncertainty associated with the inverse variance of the weights.\nOn the other hand, the Spike-and-slab prior combines two distributions: a spike at zero, representing the probability that a coefficient is exactly zero, and a slab, a continuous distribution reflecting the possible non-zero values of the coefficient. This mixture allows for both exact zeros and non-zero coefficients, making it a powerful tool for variable selection in high-dimensional settings. The prior can be represented as: \\[p(w_{d,k} | \\alpha_k,\\theta_k) = (1-\\theta_k) \\delta_0(w_{d,k}) + \\theta_k N(w_{d,k};0, \\alpha_k^{-1})\\] \\[p(\\theta_k) = \\mathcal{B}(\\theta_k; a_0^\\theta,b_0^\\theta)\\] \\[p(\\alpha_k) = \\mathcal{G}(\\alpha_k; a_0^\\alpha, b_0^\\alpha)\\]\nHere, the Beta distribution is utilized for modeling the hyperparameter \\(\\theta_k\\), indicating the probability that a weight is non-zero. As the Beta distribution is defined between [0, 1], it’s a good fit for modeling probabilities.\nLet’s try to incorporate these in the model.\n\n# Fix the random seed for reproducibility\nset.seed(100)\n\ngfa_sparse &lt;- \"\ndata {\n    int&lt;lower=1&gt; N;             // Number of data points\n    int&lt;lower=1&gt; K;             // Dimensionality of latent space\n    int&lt;lower=1&gt; M;             // Number of modalities\n    int&lt;lower=1&gt; SumP;          // Total number of features across all modalities\n    int&lt;lower=1&gt; P[M];          // Number of features for each modality\n    matrix[N, SumP] x;          // Concatenated data\n    real a0_theta;              // Hyperparameter for Beta prior\n    real b0_theta;              // Hyperparameter for Beta prior\n}\n\nparameters {\n    matrix[K, SumP] W;          // Factor loading matrix\n    vector&lt;lower=0&gt;[M] tau;       // Precision for each modality\n    matrix[N, K] z;             // Latent variables\n    matrix&lt;lower=0&gt;[M,K] alpha; // View-specific ARD prior\n    matrix&lt;lower=0, upper=1&gt;[K, SumP] theta; // Spike-and-slab mixing proportion\n}\n\n\n    transformed parameters{\n        vector&lt;lower=0&gt;[M] t_tau;\n    t_tau = inv(sqrt(tau));\n    matrix&lt;lower=0&gt;[M,K] t_alpha; \n    t_alpha = inv(sqrt(alpha));\n    \n    }\n\nmodel {\n\n    // fix z first\n    to_vector(z) ~ normal(0,1);\n    tau ~ gamma(1, 1);\n    to_vector(alpha) ~ gamma(1e-2,1e-2);\n\n    // add aph\n    // Priors\n// Incorporating the ARD and spike-and-slab priors\n    int start;\n    start = 0;\n    for (m in 1:M) {\n        for (d in 1:P[m]) {\n            start = start + 1;   \n            \n            // Spike-and-slab prior\n            for (k in 1:K) {\n                \n                theta[k,start] ~ beta(a0_theta, b0_theta);\n                target += log_mix(theta[k, start],\n                                  normal_lpdf(W[k,start] | 0, t_alpha[m,k]),normal_lpdf(W[k,start] | 0, 1e-14));\n            }\n            \n            // Data likelihood\n            x[,start] ~ normal(z*W[,start], t_tau[m]);  \n        }\n\n    \n    }\n}\n\"\n\n\n\ngfa_model &lt;- stan_model(model_code = gfa_sparse)\n\n# Load and preprocess the breast cancer mRNA and protein datasets\nX1 &lt;- scale(breast.TCGA$data.train$mrna,scale = F,center = T)\nX2 &lt;- scale(breast.TCGA$data.train$protein,scale = F,center = T)\nX3 &lt;- scale(breast.TCGA$data.train$mirna,scale = F,center = T)\n\n# prepare the list\nmatrices_list &lt;- list(\n  X1,\n  X2,\n  X3\n)\n\ncombined_data &lt;- do.call(cbind,matrices_list)\n# Prepare the data for the Stan model\nstan_data &lt;- list(\n  N = sapply(matrices_list, nrow)[1],\n  K = 2,\n  P = sapply(matrices_list, ncol),\n  M = length(matrices_list),\n  x = combined_data,\n  SumP = ncol(combined_data),\n  a0_theta=1,\n  b0_theta=1\n  \n)\n\n# Set another random seed for reproducibility in the inference step\nset.seed(200)\n\n# Fit the GFA model using Variational Bayes with the meanfield algorithm\ngfa_fit &lt;- vb(gfa_model, data = stan_data, algorithm = \"meanfield\", iter = 1000, output_samples = 100)\n\nChain 1: ------------------------------------------------------------\nChain 1: EXPERIMENTAL ALGORITHM:\nChain 1:   This procedure has not been thoroughly tested and may be unstable\nChain 1:   or buggy. The interface is subject to change.\nChain 1: ------------------------------------------------------------\nChain 1: \nChain 1: \nChain 1: \nChain 1: Gradient evaluation took 0.005689 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 56.89 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Begin eta adaptation.\nChain 1: Iteration:   1 / 250 [  0%]  (Adaptation)\nChain 1: Iteration:  50 / 250 [ 20%]  (Adaptation)\nChain 1: Iteration: 100 / 250 [ 40%]  (Adaptation)\nChain 1: Iteration: 150 / 250 [ 60%]  (Adaptation)\nChain 1: Iteration: 200 / 250 [ 80%]  (Adaptation)\nChain 1: Success! Found best value [eta = 1] earlier than expected.\nChain 1: \nChain 1: Begin stochastic gradient ascent.\nChain 1:   iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes \nChain 1:    100      -104607.753             1.000            1.000\nChain 1:    200       -98371.919             0.532            1.000\nChain 1:    300       -98212.222             0.033            0.063\nChain 1:    400       -98172.998             0.001            0.002   MEAN ELBO CONVERGED   MEDIAN ELBO CONVERGED\nChain 1: \nChain 1: Drawing a sample of size 100 from the approximate posterior... \nChain 1: COMPLETED.\n\n\nWarning: Pareto k diagnostic value is 6.26. Resampling is disabled. Decreasing\ntol_rel_obj may help if variational algorithm has terminated prematurely.\nOtherwise consider using sampling instead.\n\n# Extract and compute the average latent scores from the fit for global, mRNA, protein, and miRNA data\nscores_global &lt;- apply(extract(gfa_fit,\"z\")[[1]], c(2,3), mean)\nW_mean &lt;-apply(extract(gfa_fit,\"W\")[[1]], c(2,3), mean)\n\n# Separate W_mean based on modalities\nW_list &lt;- list()\nstart_col &lt;- 1\nfor(m in 1:stan_data$M) {\n  end_col &lt;- start_col + stan_data$P[m] - 1\n  W_list[[m]] &lt;- W_mean[, start_col:end_col]\n  start_col &lt;- end_col + 1\n}\n\n# calculate block scores\nscores &lt;- mapply(function(x,y){list(x%*%t(y))},x=matrices_list,y=W_list)\n\n\n# Plot the latent scores for mRNA, protein, and global datasets in a 2x2 grid layout\npar(mfrow=c(2,2))\nplot(scores[[1]], col=breast.TCGA$data.train$subtype, main=\"GFA mRNA\", xlab=\"L1\", ylab=\"L2\")\nplot(scores[[2]], col=breast.TCGA$data.train$subtype, main=\"GFA protein\", xlab=\"L1\", ylab=\"L2\")\nplot(scores[[3]], col=breast.TCGA$data.train$subtype, main=\"GFA miRNA\", xlab=\"L1\", ylab=\"L2\")\nplot(scores_global, col=breast.TCGA$data.train$subtype, main=\"GFA global\", xlab=\"L1\", ylab=\"L2\")\n\n\n\n\nThe above code computes the log of a mixture of two distributions. In the context of the spike-and-slab prior, this function is used to represent the spike (a point mass at zero) and the slab (a continuous distribution) for the weights. As said, the idea is to combine a “spike” (a point mass at zero, promoting sparsity) with a “slab” (a continuous distribution allowing for non-zero parameter values). By mixing these two distributions, the prior encourages the parameters to be close to zero (due to the spike) while allowing some to take non-zero values (due to the slab).\nIn practice however our parametrization might cause problems in inference.\nMOFA uses re-parametrization of the weights \\(w\\) as a product of a Gaussian random variable \\(\\hat{w}\\) and a Bernoulli random variable \\(s, 12\\), 4] resulting in the following prior:\n\\[\np\\left(\\hat{w}_{d, k}^{m}, s_{d, k}^{m}\\right)=\\mathcal{N}\\left(\\hat{w}_{d, k}^{m} \\mid 0,1 / \\alpha_{k}^{m}\\right) \\operatorname{Ber}\\left(s_{d, k}^{m} \\mid \\theta_{k}^{m}\\right)\n\\] with hyper-parameters \\(a_{0}^{\\theta}, b_{0}^{\\theta}=1\\) and \\(a_{0}^{\\alpha}, b_{0}^{\\alpha}=1 e^{-14}\\) to get uninformative priors. A value of \\(\\theta_{k}^{m}\\) close to 0 implies that most of the weights of factor \\(k\\) in view \\(m\\) are shrinked to 0 , which is the definition of a sparse factor. In contrast, a value of \\(\\theta_{k}^{m}\\) close to 1 implies that most of the weights are non-zero, which is the definition of a non-sparse factor.\nWe cannot directly construct a spike-and-slab prior in Stan since it requires a discrete parameter. We are going to leave it as it is now! So all together with the code above, we have more or less reach the final joint probability of MOFA:\n\\[\n\\begin{aligned}\np(\\mathbf{Y}, \\hat{\\mathbf{W}}, \\mathbf{S}, \\mathbf{Z}, \\boldsymbol{\\Theta}, \\boldsymbol{\\alpha}, \\boldsymbol{\\tau})= & \\prod_{m=1}^{M} \\prod_{n=1}^{N} \\prod_{d=1}^{D_{m}} \\mathcal{N}\\left(y_{n d}^{m} \\mid \\sum_{k=1}^{K} s_{d k}^{m} \\hat{w}_{d k}^{m} z_{n k}, 1 / \\tau_{d}\\right) \\\\\n& \\prod_{m=1}^{M} \\prod_{d=1}^{D_{m}} \\prod_{k=1}^{K} \\mathcal{N}\\left(\\hat{w}_{d k}^{m} \\mid 0,1 / \\alpha_{k}^{m}\\right) \\operatorname{Ber}\\left(s_{d, k}^{m} \\mid \\theta_{k}^{m}\\right) \\\\\n& \\prod_{n=1}^{N} \\prod_{k=1}^{K} \\mathcal{N}\\left(z_{n k} \\mid 0,1\\right) \\\\\n& \\prod_{m=1}^{M} \\prod_{k=1}^{K} \\operatorname{Beta}\\left(\\theta_{k}^{m} \\mid a_{0}^{\\theta}, b_{0}^{\\theta}\\right) \\\\\n& \\prod_{m=1}^{M} \\prod_{k=1}^{K} \\mathcal{G}\\left(\\alpha_{k}^{m} \\mid a_{0}^{\\alpha}, b_{0}^{\\alpha}\\right) \\\\\n& \\prod_{m=1}^{M} \\prod_{d=1}^{D_{m}} \\mathcal{G}\\left(\\tau_{d}^{m} \\mid a_{0}^{\\tau}, b_{0}^{\\tau}\\right) .\n\\end{aligned}\n\\]\nMOFA follows GFA under the hood but it also allows one to use different priors depending on data distribution and also different levels of sparsity but the general idea is what we got to now. After going through the theory it is time to use MOFA to do data integration."
  },
  {
    "objectID": "mofa.html#building-and-training-the-mofa-object",
    "href": "mofa.html#building-and-training-the-mofa-object",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "Building and training the MOFA object",
    "text": "Building and training the MOFA object\nIn order to do data integration using MOFA we first have to construct a list of data matrices:\n\n# load library\nlibrary(MOFA2)\n\n\nAttaching package: 'MOFA2'\n\n\nThe following object is masked from 'package:stats':\n\n    predict\n\n# remove the subtype information from training data\ndata_mofa &lt;- breast.TCGA$data.train[-4]\n\n# transpose data because MOFA wants features in rows\ndata_mofa &lt;- lapply(data_mofa, t)\n\nWe can then use create_mofa to create a MOFA object:\n\nMOFAobject &lt;- create_mofa(data_mofa)\n\nCreating MOFA object from a list of matrices (features as rows, sample as columns)...\n\n\nWe can have a look at the structure of the input data:\n\nplot_data_overview(MOFAobject)\n\n\n\n\n\n\n\n\nThis essentially shows us how many samples we have and how many features per data view is there. If there were missing values, these would be shown as white lines.\n\nExercise 1 (NA) Can you add a bit of missing values and check how the plot will change?\n\n\n\nShow the code\n# Let's randomly introduce NAs to 20% of samples in one omics e.g. protein data\n# This is an example code. You can probably find an easier way to solve this :) \n\n# make a copy of MOFA data and protein data\ndata_mofa_with_na &lt;- data_mofa\ndata_protein &lt;- data_mofa$protein\n\n# calculate number of data points to replace\nn &lt;- ncol(data_protein) # no. of samples\nn_to_replace &lt;- 20/100 * n # number to replace, 20%\n\n# sample index and replace with NA \ndata_protein[, sample(1:n, n_to_replace)] &lt;- NA\n\n# check that we have NAs, we should have n_to_replace amount\nsum(is.na(data_protein))\n\n# replace protein data under the MOFA\ndata_mofa_with_na$protein &lt;- data_protein\n\n# create MOFA object\nMOFAobject_with_na &lt;- create_mofa(data_mofa_with_na)\n\n# plot\nplot_data_overview(MOFAobject_with_na)\n\n\nBefore we start modeling we should have a look at the distribution of the data to decide which one to use.\n\npar(mfrow=c(2,2))\nhist(data_mofa$mrna)\nhist(data_mofa$protein)\nhist(data_mofa$mirna)\n\n\n\n\n\n\n\n\nAll of our data is seems to be normally distributed so we use normal distribution. In practice MOFA allows us to select ‘gaussian’ for continuous data (e.g proteomics), ‘bernoulli’ for binary data (e.g. methylation) and ‘poisson’ for count data (e.g. RNA-Seq).\nWe can now set the parameters and train the model:\n\nmodel_opts &lt;- get_default_model_options(MOFAobject)\nprint(model_opts)\n\n$likelihoods\n     mirna       mrna    protein \n\"gaussian\" \"gaussian\" \"gaussian\" \n\n$num_factors\n[1] 15\n\n$spikeslab_factors\n[1] FALSE\n\n$spikeslab_weights\n[1] FALSE\n\n$ard_factors\n[1] FALSE\n\n$ard_weights\n[1] TRUE\n\n\nWe see that MOFA has select default (gaussian) likelihood for all our data and include 15 factors (latent variables). It does not do Spike and Slap but ARD is switched on.\nLet’s set the number of factors to 10 and continue\n\nmodel_opts$num_factors &lt;- 10\n\nWe can now start the training:\n\nMOFAobject &lt;- prepare_mofa(MOFAobject,\n  model_options = model_opts\n)\n\nChecking data options...\n\n\nNo data options specified, using default...\n\n\nNo training options specified, using default...\n\n\nChecking model options...\n\nMOFAobject &lt;- invisible(run_mofa(MOFAobject))\n\nWarning in run_mofa(MOFAobject): No output filename provided. Using /var/folders/kh/tgq9mmld6_v9z_h220trj0c40000gn/T//RtmpFfp50U/mofa_20231106-090736.hdf5 to store the trained model.\n\n\nConnecting to the mofapy2 python package using reticulate (use_basilisk = FALSE)... \n    Please make sure to manually specify the right python binary when loading R with reticulate::use_python(..., force=TRUE) or the right conda environment with reticulate::use_condaenv(..., force=TRUE)\n    If you prefer to let us automatically install a conda environment with 'mofapy2' installed using the 'basilisk' package, please use the argument 'use_basilisk = TRUE'\n\n\n\n        #########################################################\n        ###           __  __  ____  ______                    ### \n        ###          |  \\/  |/ __ \\|  ____/\\    _             ### \n        ###          | \\  / | |  | | |__ /  \\ _| |_           ### \n        ###          | |\\/| | |  | |  __/ /\\ \\_   _|          ###\n        ###          | |  | | |__| | | / ____ \\|_|            ###\n        ###          |_|  |_|\\____/|_|/_/    \\_\\              ###\n        ###                                                   ### \n        ######################################################### \n       \n \n        \nuse_float32 set to True: replacing float64 arrays by float32 arrays to speed up computations...\n\nSuccessfully loaded view='mirna' group='group1' with N=150 samples and D=184 features...\nSuccessfully loaded view='mrna' group='group1' with N=150 samples and D=200 features...\nSuccessfully loaded view='protein' group='group1' with N=150 samples and D=142 features...\n\n\nModel options:\n- Automatic Relevance Determination prior on the factors: False\n- Automatic Relevance Determination prior on the weights: True\n- Spike-and-slab prior on the factors: False\n- Spike-and-slab prior on the weights: False\nLikelihoods:\n- View 0 (mirna): gaussian\n- View 1 (mrna): gaussian\n- View 2 (protein): gaussian\n\n\n\n\n######################################\n## Training the model with seed 42 ##\n######################################\n\n\nELBO before training: -605349.03 \n\nIteration 1: time=0.01, ELBO=-97986.53, deltaELBO=507362.501 (83.81321728%), Factors=10\nIteration 2: time=0.01, Factors=10\nIteration 3: time=0.01, Factors=10\nIteration 4: time=0.01, Factors=10\nIteration 5: time=0.01, Factors=10\nIteration 6: time=0.01, ELBO=-85812.38, deltaELBO=12174.149 (2.01109572%), Factors=10\nIteration 7: time=0.00, Factors=10\nIteration 8: time=0.00, Factors=10\nIteration 9: time=0.00, Factors=10\nIteration 10: time=0.00, Factors=10\nIteration 11: time=0.01, ELBO=-85546.36, deltaELBO=266.029 (0.04394630%), Factors=10\nIteration 12: time=0.00, Factors=10\nIteration 13: time=0.00, Factors=10\nIteration 14: time=0.00, Factors=10\nIteration 15: time=0.00, Factors=10\nIteration 16: time=0.01, ELBO=-85119.86, deltaELBO=426.494 (0.07045427%), Factors=10\nIteration 17: time=0.01, Factors=10\nIteration 18: time=0.00, Factors=10\nIteration 19: time=0.00, Factors=10\nIteration 20: time=0.00, Factors=10\nIteration 21: time=0.01, ELBO=-84663.08, deltaELBO=456.782 (0.07545757%), Factors=10\nIteration 22: time=0.00, Factors=10\nIteration 23: time=0.01, Factors=10\nIteration 24: time=0.00, Factors=10\nIteration 25: time=0.00, Factors=10\nIteration 26: time=0.01, ELBO=-84469.19, deltaELBO=193.894 (0.03203003%), Factors=10\nIteration 27: time=0.00, Factors=10\nIteration 28: time=0.00, Factors=10\nIteration 29: time=0.00, Factors=10\nIteration 30: time=0.01, Factors=10\nIteration 31: time=0.01, ELBO=-84312.91, deltaELBO=156.276 (0.02581593%), Factors=10\nIteration 32: time=0.00, Factors=10\nIteration 33: time=0.00, Factors=10\nIteration 34: time=0.00, Factors=10\nIteration 35: time=0.00, Factors=10\nIteration 36: time=0.01, ELBO=-84211.62, deltaELBO=101.293 (0.01673295%), Factors=10\nIteration 37: time=0.01, Factors=10\nIteration 38: time=0.00, Factors=10\nIteration 39: time=0.01, Factors=10\nIteration 40: time=0.00, Factors=10\nIteration 41: time=0.01, ELBO=-84149.57, deltaELBO=62.049 (0.01025015%), Factors=10\nIteration 42: time=0.00, Factors=10\nIteration 43: time=0.01, Factors=10\nIteration 44: time=0.00, Factors=10\nIteration 45: time=0.00, Factors=10\nIteration 46: time=0.01, ELBO=-84113.44, deltaELBO=36.127 (0.00596801%), Factors=10\nIteration 47: time=0.00, Factors=10\nIteration 48: time=0.00, Factors=10\nIteration 49: time=0.00, Factors=10\nIteration 50: time=0.01, Factors=10\nIteration 51: time=0.01, ELBO=-84094.13, deltaELBO=19.309 (0.00318972%), Factors=10\nIteration 52: time=0.00, Factors=10\nIteration 53: time=0.00, Factors=10\nIteration 54: time=0.00, Factors=10\nIteration 55: time=0.00, Factors=10\nIteration 56: time=0.01, ELBO=-84083.06, deltaELBO=11.072 (0.00182895%), Factors=10\nIteration 57: time=0.00, Factors=10\nIteration 58: time=0.00, Factors=10\nIteration 59: time=0.00, Factors=10\nIteration 60: time=0.00, Factors=10\nIteration 61: time=0.01, ELBO=-84075.87, deltaELBO=7.195 (0.00118855%), Factors=10\nIteration 62: time=0.00, Factors=10\nIteration 63: time=0.00, Factors=10\nIteration 64: time=0.00, Factors=10\nIteration 65: time=0.01, Factors=10\nIteration 66: time=0.01, ELBO=-84070.67, deltaELBO=5.196 (0.00085839%), Factors=10\nIteration 67: time=0.00, Factors=10\nIteration 68: time=0.00, Factors=10\nIteration 69: time=0.00, Factors=10\nIteration 70: time=0.00, Factors=10\nIteration 71: time=0.01, ELBO=-84066.69, deltaELBO=3.981 (0.00065768%), Factors=10\nIteration 72: time=0.00, Factors=10\nIteration 73: time=0.00, Factors=10\nIteration 74: time=0.00, Factors=10\nIteration 75: time=0.00, Factors=10\nIteration 76: time=0.01, ELBO=-84063.53, deltaELBO=3.155 (0.00052122%), Factors=10\nIteration 77: time=0.00, Factors=10\nIteration 78: time=0.00, Factors=10\nIteration 79: time=0.00, Factors=10\nIteration 80: time=0.00, Factors=10\nIteration 81: time=0.01, ELBO=-84060.96, deltaELBO=2.577 (0.00042565%), Factors=10\nIteration 82: time=0.01, Factors=10\nIteration 83: time=0.00, Factors=10\nIteration 84: time=0.00, Factors=10\nIteration 85: time=0.00, Factors=10\nIteration 86: time=0.01, ELBO=-84058.83, deltaELBO=2.127 (0.00035130%), Factors=10\n\nConverged!\n\n\n\n#######################\n## Training finished ##\n#######################\n\n\nSaving model in /var/folders/kh/tgq9mmld6_v9z_h220trj0c40000gn/T//RtmpFfp50U/mofa_20231106-090736.hdf5...\n\n\nWarning in .quality_control(object, verbose = verbose): Factor(s) 4 are strongly correlated with the total number of expressed features for at least one of your omics. Such factors appear when there are differences in the total 'levels' between your samples, *sometimes* because of poor normalisation in the preprocessing steps."
  },
  {
    "objectID": "mofa.html#variance-decomposition",
    "href": "mofa.html#variance-decomposition",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "Variance decomposition",
    "text": "Variance decomposition\nThe most important insight that MOFA generates is the variance decomposition analysis. This plot shows the percentage of variance explained by each factor across each data modality.\n\nplot_variance_explained(MOFAobject)\n\n\n\n\nFrom the results of the plot_variance_explained function in MOFA, we can discern the variance explained by each factor across the three views: mirna, mrna, and protein.\nIn the mirna view, Factor1 leads by explaining approximately 15.96% of the variance. Notably, Factor1 also stands out in both the mrna and protein views, explaining 20.37% and 20.41% respectively, suggesting its consistent importance across all views.\nFor the mrna view, besides Factor1, Factor2 contributes significantly with 11.88%. This contrasts with its contribution in the protein view, where it explains only 1.25% of the variance, and in the mirna view, where it accounts for 6.04%.\nIn the protein view, while Factor1 remains dominant, Factor3 emerges as significant, explaining 12.20% of the variance. This is intriguing as Factor3 has a minimal role in the mrna view (0.12%) but does have a presence in the mirna view with 0.65%.\nFactors such as Factor4 and Factor7 exhibit diverse roles across the views. In the mirna view, Factor4 explains a notable 12.77% but diminishes to 0.16% and 0.02% in the mrna and protein views respectively. Factor7, on the other hand, is more prominent in the mirna view with 7.09% but is almost negligible in the other two views.\nWhich factor consistently plays a vital role across all views?\nIt is important to see if the model fits the data well. We can do this by looking at how much of total variance is explained by factors across different views. Here, the results will usually vary based on the kind of data, number of samples and number of factors used.\n\nvar_plot &lt;- plot_variance_explained(MOFAobject, plot_total = T)[[2]]\nvar_plot\n\n\n\n\nIn this data set, by using 10 factors the MOFA model explains about 49% of the variation in the miRNA, 48% of the variation in the mRNA data and 45% of the variation in the protein data.\nNow that we know we have a reasonable model, given that Factor1 consistently plays a vital role across all views, we aim to characterize its molecular signal and its association with available sample covariates. Here are the steps we will follow:"
  },
  {
    "objectID": "mofa.html#exploring-factors",
    "href": "mofa.html#exploring-factors",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "Exploring factors",
    "text": "Exploring factors\nTo understand the relation between Factor1 values and sample metadata, we will perform an association analysis.\n\nsamples_metadata(MOFAobject) &lt;- data.frame(sample=colnames(data_mofa$mirna),subtype=breast.TCGA$data.train$subtype)\n\ncorrelate_factors_with_covariates(MOFAobject, \n  covariates = c(\"subtype\"), \n  plot = \"log_pval\",cluster_cols=F\n)\n\nWarning in correlate_factors_with_covariates(MOFAobject, covariates =\nc(\"subtype\"), : There are non-numeric values in the covariates data.frame,\nconverting to numeric...\n\n\n\n\n\n\n\n\n\nThis clearly shows a strong association of Factor1 to cancer subtype. The rest of the factors more or less do not have a clear association with the cancer subtype.\nNow that we have a Factor of interest we can plot the values of it. Factors in MOFA are similar to PCA so they are designed to isolate distinct sources of variance present within the data. From a mathematical standpoint, each factor can be described as a linear amalgamation of the provided features. These factors position samples on a one-dimensional scale centered around zero.\n\n# visualize the factors\nplot_factors(MOFAobject, \n  factors = c(1,2), \n  dot_size = 2.5,shape_by = \"subtype\"\n)\n\n\n\n\n\n\n\n\nWe can see that the pattern in Factor1 has captures the group differences. We can also look at this using violin plot:\n\nplot_factor(MOFAobject, \n  factors = 1, \n  color_by = \"subtype\",\n  add_violin = TRUE,\n  dodge = TRUE\n)\n\n\n\n\n\n\n\n\nAs you see, we have a strong trend of subtypes captured by the first factor. We should have a look at the weights for this factor to figure out what are the most important features that contribute to this pattern.\nFeature weights play an important role in understanding the influence of each feature on a given factor. These weights offer a quantifiable score for every feature in relation to its respective factor. Essentially, when a feature doesn’t correlate with a factor, its weight is anticipated to hover around zero. Conversely, features that are robustly associated with the factor will display large absolute weight values. The polarity of the weight whether positive or negative reveals the nature of the relationship: a positive weight suggests that the feature level elevates in instances with positive factor values and the opposite for negative weights.\nLet’s look athe top 10 features in mRNA.\n\nplot_top_weights(MOFAobject,view = \"mrna\",\n factor = 1,\n nfeatures = 10,    \n scale = T          \n)\n\n\n\n\n\n\n\n\nThe plot suggest that STC2 has a strong negative relationship with Factor1. Looking back at the score plot, we see that our Basal subtype has ended up on the right of the plot, Her2 in the middle and LumA on the left. This suggest that the expression of STC2 is higher in LumA vs Her2 and also Her2 vs LumA. Let’s check it:\n\nplot_data_scatter(MOFAobject, \n  view = \"mrna\",\n  factor = 1, features = \"STC2\",color_by = \"subtype\"\n)\n\n\n\n\n\n\n\n\nGreat. But we have so many other features, do we have a subgroup of features in our data:\n\n# plot heatmap\nsample_group &lt;- samples_metadata(MOFAobject)\nrownames(sample_group) &lt;- sample_group[,1]\n\nheatmap_plot &lt;- plot_data_heatmap(MOFAobject, \nview = \"mrna\",\n  factor = 1, features = 50,\n  cluster_rows = TRUE, cluster_cols = FALSE,annotation_samples = sample_group[,\"subtype\",drop=F],\n  show_rownames = TRUE, show_colnames = FALSE,\n  scale = \"row\"\n)\n\n'annotation_samples' provided as a data.frame, please make sure that the rownames match the sample names\n\nheatmap_plot\n\n\n\n\n\n\n\n\nWe can at least see two big groups of genes having contrasting expression pattern. Let’s extract these features and investigate them more.\n\nfeature_subgroups&lt;-cutree(heatmap_plot$tree_row,2)\n\nplot_data_scatter(MOFAobject, \n  view = \"mrna\",\n  factor = 1,  \n  features = names(feature_subgroups[feature_subgroups==1]),\n  color_by = \"subtype\"\n)\n\n\n\n\n\n\n\nplot_data_scatter(MOFAobject, \n  view = \"mrna\",\n  factor = 1,  \n  features = names(feature_subgroups[feature_subgroups==2]),\n  color_by = \"subtype\"\n) \n\n\n\n\n\n\n\n\nAs it is clear the two subgroups are related to the features that are positively and negatively correlated with the first factor. This is a good indication that we can use the weights to group the features. We can use these groups to do enrichment analysis or similar. We are going to move on now to the last section.\n\nExercise 2 (MOFA) Can you perform MOFA on the test data which is in breast.TCGA$data.test?\n\nDo you see the same pattern as in the training set?\nDo the top 10 most important features overlap between training and testing?\nHow about the grouping of the features?"
  },
  {
    "objectID": "mofa.html#r-implemenation",
    "href": "mofa.html#r-implemenation",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "R implemenation",
    "text": "R implemenation\nSo let’s try to implement this! First we will have to change our initial data a bit to have a smooth trend that emulates real-world scenarios. A common pattern observed in biological systems is the circadian rhythm, a natural, internal process that regulates the sleep-wake cycle and repeats roughly every 24 hours. This rhythmic oscillation is present in various biological processes and can significantly influence the expression patterns of certain genes and proteins. To do that we will introduce an artificial circadian-like oscillation to specific features of our dataset. This sinusoidal pattern will mimic the natural rhythmic changes one might observe in gene or protein expression data due to the influence of the circadian clock.\n\n# Create an artificial time covariate with a sinusoidal pattern\nset.seed(123)  # For reproducibility\ntime_covariate &lt;- sample(seq(1, 150, by=1),replace = F)\nsinusoidal_trend &lt;- sin(2 * pi * time_covariate / 24)  # A 24-unit cycle for the circadian rhythm\n\n# Modify data with a sinusoidal time trend for some features\n# remove the subtype information\ndata_mofa &lt;- breast.TCGA$data.train[-4]\n# we do transpose because mofa wants features in rows\ndata_mofa &lt;- lapply(data_mofa,t)\n\n\n# For miRNA data\nmiRNA_data &lt;- data_mofa$mirna\nmiRNA_data[1:20, ] &lt;- data_mofa$mirna[1:20, ] + 0.7* matrix(rep(sinusoidal_trend, 20), nrow=20, byrow=TRUE)  # Add trend to the first 20 features\n\n# For mRNA data\nmRNA_data &lt;- data_mofa$mrna\nmRNA_data[1:25, ] &lt;- data_mofa$mrna[1:25, ] +  matrix(rep(sinusoidal_trend, 25), nrow=25, byrow=TRUE)  # Add trend to the first 25 features\n\n# For protein data\nprotein_data &lt;- data_mofa$protein\nprotein_data[1:15, ] &lt;- protein_data[1:15, ] +  0.2* matrix(rep(sinusoidal_trend, 15), nrow=15, byrow=TRUE)  # Add trend to the first 15 features\n\nscatter.smooth(sinusoidal_trend~time_covariate,span = 0.1,evaluation = 500)\n\n\n\n\nThe plot shows the pattern that we have added to some of features in each modalities. We are now going to change the prior of our GFA:\n\n# Fix the random seed for reproducibility\nset.seed(100)\n\n# Define GFA\ngfa_smooth &lt;- \"\ndata {\n    int&lt;lower=1&gt; N;             // Number of data points\n    int&lt;lower=1&gt; K;             // Dimensionality of latent space\n    int&lt;lower=1&gt; M;             // Number of modalities\n    int&lt;lower=1&gt; SumP;          // Total number of features across all modalities\n    int&lt;lower=1&gt; P[M];          // Number of features for each modality\n    matrix[N, SumP] x;          // Concatenated data\n    real a0_theta;              // Hyperparameter for Beta prior\n    real b0_theta;              // Hyperparameter for Beta prior\n    real cov_vector[N]; \n}\n\nparameters {\n    matrix[K, SumP] W;          // Factor loading matrix\n    vector&lt;lower=0&gt;[M] tau;       // Precision for each modality\n    matrix[N, K] z;             // Latent variables\n    matrix&lt;lower=0&gt;[M,K] alpha; // View-specific ARD prior\n    matrix&lt;lower=0, upper=1&gt;[K, SumP] theta; // Spike-and-slab mixing proportion\n   vector&lt;lower=0&gt;[K] le;\n   vector&lt;lower=0&gt;[K] sigma_f;\n}\n\n\n    transformed parameters{\n        vector&lt;lower=0&gt;[M] t_tau;\n    t_tau = inv(sqrt(tau));\n    matrix&lt;lower=0&gt;[M,K] t_alpha; \n    t_alpha = inv(sqrt(alpha));\n    matrix[N, N] K_f[K];\n    matrix[N, N] L_K[K];\n    for (k in 1:K) {\n        K_f[k] = gp_exp_quad_cov(cov_vector, sigma_f[k], le[k]);\n\n        L_K[k] = cholesky_decompose(add_diag(K_f[k], rep_vector(1e-9, N)));\n    }\n    \n    }\n\nmodel {\n    // GP parameters\n    le ~ normal(0, 1);\n    sigma_f ~ normal(0, 1);\n  \n  // priors\n    for (k in 1:K) {\n        z[, k] ~ multi_normal_cholesky(rep_vector(0, N), L_K[k]);\n    }\n     \n    tau ~ gamma(1, 1);\n    to_vector(alpha) ~ gamma(1e-2,1e-2);\n\n    // add aph\n    // Priors\n// Incorporating the ARD and spike-and-slab priors\n    int start;\n    start = 0;\n    for (m in 1:M) {\n        for (d in 1:P[m]) {\n            start = start + 1;   \n            \n            // Spike-and-slab prior\n            for (k in 1:K) {\n                \n                theta[k,start] ~ beta(a0_theta, b0_theta);\n                target += log_mix(theta[k, start],\n                                  normal_lpdf(W[k,start] | 0, t_alpha[m,k]),normal_lpdf(W[k,start] | 0, 1e-14));\n            }\n            \n            // Data likelihood\n            x[,start] ~ normal(z*W[,start], t_tau[m]);  \n        }\n\n    \n    }\n}\n\n\n\"\n\nlibrary(rstan)\n\ngfa_model_old &lt;- stan_model(model_code = gfa_sparse)\ngfa_model_smooth &lt;- stan_model(model_code = gfa_smooth)\n# Load and preprocess the breast cancer mRNA, mirna and protein datasets\nX1 &lt;- t(miRNA_data)\nX2 &lt;- t(mRNA_data)\nX3 &lt;- t(protein_data)\n\n# prepare the list\nmatrices_list &lt;- list(\n  X1,\n  X2,\n  X3\n)\n#matrices_list&lt;-lapply(matrices_list,function(x){scale(x,scale = F)})\n\ncombined_data &lt;- do.call(cbind,matrices_list)\n# Prepare the data for the Stan model\nstan_data_old &lt;- list(\n  N = sapply(matrices_list, nrow)[1],\n  K = 4,\n  P = sapply(matrices_list, ncol),\n  M = length(matrices_list),\n  x = combined_data,\n  SumP = ncol(combined_data),\n  a0_theta=1,\n  b0_theta=1\n  \n)\n\nstan_data &lt;- list(\n  N = sapply(matrices_list, nrow)[1],\n  K = 4,\n  P = sapply(matrices_list, ncol),\n  M = length(matrices_list),\n  x = combined_data,\n  SumP = ncol(combined_data),\n  cov_vector=time_covariate,\n  a0_theta=1,\n  b0_theta=1\n  \n)\n\n\n# Set another random seed for reproducibility in the inference step\nset.seed(1000)\n\ngfa_fit_old &lt;- vb(gfa_model_old, data = stan_data_old, algorithm = \"meanfield\", iter = 2000, output_samples = 100)\n\nChain 1: ------------------------------------------------------------\nChain 1: EXPERIMENTAL ALGORITHM:\nChain 1:   This procedure has not been thoroughly tested and may be unstable\nChain 1:   or buggy. The interface is subject to change.\nChain 1: ------------------------------------------------------------\nChain 1: \nChain 1: \nChain 1: \nChain 1: Gradient evaluation took 0.008195 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 81.95 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Begin eta adaptation.\nChain 1: Iteration:   1 / 250 [  0%]  (Adaptation)\nChain 1: Iteration:  50 / 250 [ 20%]  (Adaptation)\nChain 1: Iteration: 100 / 250 [ 40%]  (Adaptation)\nChain 1: Iteration: 150 / 250 [ 60%]  (Adaptation)\nChain 1: Iteration: 200 / 250 [ 80%]  (Adaptation)\nChain 1: Success! Found best value [eta = 1] earlier than expected.\nChain 1: \nChain 1: Begin stochastic gradient ascent.\nChain 1:   iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes \nChain 1:    100      -116040.090             1.000            1.000\nChain 1:    200      -107023.648             0.542            1.000\nChain 1:    300      -105188.701             0.051            0.084\nChain 1:    400      -103915.494             0.015            0.017\nChain 1:    500      -103297.899             0.009            0.012   MEAN ELBO CONVERGED\nChain 1: \nChain 1: Drawing a sample of size 100 from the approximate posterior... \nChain 1: COMPLETED.\n\n\nWarning: Pareto k diagnostic value is 24.05. Resampling is disabled. Decreasing\ntol_rel_obj may help if variational algorithm has terminated prematurely.\nOtherwise consider using sampling instead.\n\n# Fit the GFA model using Variational Bayes with the meanfield algorithm\n\nset.seed(1000)\ngfa_fit_smooth &lt;- vb(gfa_model_smooth, data = stan_data, algorithm = \"meanfield\", iter = 2000, output_samples = 100)\n\nChain 1: ------------------------------------------------------------\nChain 1: EXPERIMENTAL ALGORITHM:\nChain 1:   This procedure has not been thoroughly tested and may be unstable\nChain 1:   or buggy. The interface is subject to change.\nChain 1: ------------------------------------------------------------\nChain 1: \nChain 1: \nChain 1: \nChain 1: Gradient evaluation took 0.01751 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 175.1 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Begin eta adaptation.\nChain 1: Iteration:   1 / 250 [  0%]  (Adaptation)\nChain 1: Iteration:  50 / 250 [ 20%]  (Adaptation)\nChain 1: Iteration: 100 / 250 [ 40%]  (Adaptation)\nChain 1: Iteration: 150 / 250 [ 60%]  (Adaptation)\nChain 1: Iteration: 200 / 250 [ 80%]  (Adaptation)\nChain 1: Success! Found best value [eta = 1] earlier than expected.\nChain 1: \nChain 1: Begin stochastic gradient ascent.\nChain 1:   iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes \nChain 1:    100      -262112.939             1.000            1.000\nChain 1:    200   -257883021.448             0.999            1.000\nChain 1:    300      -119529.996          1078.737         2156.475\nChain 1:    400      -108765.162          1078.287         2156.475\nChain 1:    500      -105680.106             0.064            0.099\nChain 1:    600      -104278.242             0.021            0.029\nChain 1:    700      -103670.861             0.010            0.013   MEAN ELBO CONVERGED\nChain 1: \nChain 1: Drawing a sample of size 100 from the approximate posterior... \nChain 1: COMPLETED.\n\n\nWarning: Pareto k diagnostic value is 20.13. Resampling is disabled. Decreasing\ntol_rel_obj may help if variational algorithm has terminated prematurely.\nOtherwise consider using sampling instead.\n\n# Extract and compute the average latent scores from the fit for global, mRNA, and protein data\nscores_global_old &lt;- apply(extract(gfa_fit_old,\"z\")[[1]], c(2,3), mean)\nscores_global &lt;- apply(extract(gfa_fit_smooth,\"z\")[[1]], c(2,3), mean)\n\npar(mfrow=c(4,2))\nscatter.smooth(time_covariate,scores_global_old[,1],span = 0.1,main=\"GFA l1\")\nscatter.smooth(time_covariate,scores_global[,1],span = 0.1,,main=\"smooth GFA l1\")\nscatter.smooth(time_covariate,scores_global_old[,2],span = 0.1,main=\"GFA l2\")\nscatter.smooth(time_covariate,scores_global[,2],span = 0.1,,main=\"smooth GFA l2\")\nscatter.smooth(time_covariate,scores_global_old[,3],span = 0.1,main=\"GFA l3\")\nscatter.smooth(time_covariate,scores_global[,3],span = 0.1,,main=\"smooth GFA l3\")\nscatter.smooth(time_covariate,scores_global_old[,4],span = 0.1,main=\"GFA l4\")\nscatter.smooth(time_covariate,scores_global[,4],span = 0.1,,main=\"smooth GFA l4\")\n\n\n\n\nHere I have done GFA both with and without smoothing. Just by looking at the factors we can see the smooth GFA has captured the time trend in our data where as in the GFA without smooth, the trend is not as clear. Please note that i have just used normal prior for \\(l\\) which might not make that much sense. In reality one can use a much suited prior."
  },
  {
    "objectID": "mofa.html#time-series-in-mofa",
    "href": "mofa.html#time-series-in-mofa",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "Time series in MOFA",
    "text": "Time series in MOFA\nMOFA also provides us with tools to do time series analysis using GP but with much more robust model. Let’s see how can do the same analysis using MOFA or more specifically MEFISTO.\n\n# Combine into a list\ndata_mofa &lt;- list(mirna=miRNA_data, mrna=mRNA_data, protein=protein_data)\n\n# Convert the time_covariate to a data frame and set its row and column names\ntime &lt;- as.data.frame(t(time_covariate))\nrownames(time) &lt;- \"time\"\ncolnames(time) &lt;- colnames(miRNA_data)\n\n# Create a MOFA object using the data\nsm &lt;- create_mofa(data = data_mofa)\n\nCreating MOFA object from a list of matrices (features as rows, sample as columns)...\n\n# Set the time covariate to the MOFA object\nsm &lt;- set_covariates(sm, covariates = as.matrix(time))\n\n# Define default options for the data\ndata_opts &lt;- get_default_data_options(sm)\n\n# Define default model options and set the number of factors to 4\nmodel_opts &lt;- get_default_model_options(sm)\nmodel_opts$num_factors &lt;- 4\n\n# Define default training options and set the maximum iterations to 100\ntrain_opts &lt;- get_default_training_options(sm)\ntrain_opts$maxiter &lt;- 100\n\n# Define default MEFISTO options \nmefisto_opts &lt;- get_default_mefisto_options(sm)\n\n# Prepare the MOFA object with the specified options\nsm &lt;- prepare_mofa(sm, model_options = model_opts,\n                   mefisto_options = mefisto_opts,\n                   training_options = train_opts,\n                   data_options = data_opts)\n\nChecking data options...\n\n\nChecking training options...\n\n\nWarning in prepare_mofa(sm, model_options = model_opts, mefisto_options = mefisto_opts, : Maximum number of iterations is very small\n\n\nChecking model options...\n\n\nChecking inference options for mefisto covariates...\n\n# Run the MOFA analysis\nsm &lt;- run_mofa(sm)\n\nWarning in run_mofa(sm): No output filename provided. Using /var/folders/kh/tgq9mmld6_v9z_h220trj0c40000gn/T//RtmpFfp50U/mofa_20231106-091122.hdf5 to store the trained model.\n\n\nConnecting to the mofapy2 python package using reticulate (use_basilisk = FALSE)... \n    Please make sure to manually specify the right python binary when loading R with reticulate::use_python(..., force=TRUE) or the right conda environment with reticulate::use_condaenv(..., force=TRUE)\n    If you prefer to let us automatically install a conda environment with 'mofapy2' installed using the 'basilisk' package, please use the argument 'use_basilisk = TRUE'\n\n\n\n        #########################################################\n        ###           __  __  ____  ______                    ### \n        ###          |  \\/  |/ __ \\|  ____/\\    _             ### \n        ###          | \\  / | |  | | |__ /  \\ _| |_           ### \n        ###          | |\\/| | |  | |  __/ /\\ \\_   _|          ###\n        ###          | |  | | |__| | | / ____ \\|_|            ###\n        ###          |_|  |_|\\____/|_|/_/    \\_\\              ###\n        ###                                                   ### \n        ######################################################### \n       \n \n        \nuse_float32 set to True: replacing float64 arrays by float32 arrays to speed up computations...\n\nSuccessfully loaded view='mirna' group='group1' with N=150 samples and D=184 features...\nSuccessfully loaded view='mrna' group='group1' with N=150 samples and D=200 features...\nSuccessfully loaded view='protein' group='group1' with N=150 samples and D=142 features...\n\n\nLoaded 1 covariate(s) for each sample...\n\n\nModel options:\n- Automatic Relevance Determination prior on the factors: False\n- Automatic Relevance Determination prior on the weights: True\n- Spike-and-slab prior on the factors: False\n- Spike-and-slab prior on the weights: False\nLikelihoods:\n- View 0 (mirna): gaussian\n- View 1 (mrna): gaussian\n- View 2 (protein): gaussian\n\n\n\n\n######################################\n## Training the model with seed 42 ##\n######################################\n\n\nELBO before training: -334295.48 \n\nIteration 1: time=0.01, ELBO=-98851.09, deltaELBO=235444.388 (70.43002487%), Factors=4\nIteration 2: time=0.01, Factors=4\nIteration 3: time=0.01, Factors=4\nIteration 4: time=0.01, Factors=4\nIteration 5: time=0.01, Factors=4\nIteration 6: time=0.01, ELBO=-91819.21, deltaELBO=7031.884 (2.10349349%), Factors=4\nIteration 7: time=0.01, Factors=4\nIteration 8: time=0.01, Factors=4\nIteration 9: time=0.01, Factors=4\nIteration 10: time=0.01, Factors=4\nIteration 11: time=0.01, ELBO=-91743.47, deltaELBO=75.736 (0.02265530%), Factors=4\nIteration 12: time=0.01, Factors=4\nIteration 13: time=0.01, Factors=4\nIteration 14: time=0.01, Factors=4\nIteration 15: time=0.01, Factors=4\nIteration 16: time=0.01, ELBO=-91701.50, deltaELBO=41.974 (0.01255601%), Factors=4\nIteration 17: time=0.00, Factors=4\nIteration 18: time=0.01, Factors=4\nIteration 19: time=0.01, Factors=4\nOptimising sigma node...\nIteration 20: time=0.72, Factors=4\nIteration 21: time=0.01, ELBO=-91619.80, deltaELBO=81.691 (0.02443686%), Factors=4\nIteration 22: time=0.01, Factors=4\nIteration 23: time=0.01, Factors=4\nIteration 24: time=0.00, Factors=4\nIteration 25: time=0.01, Factors=4\nIteration 26: time=0.01, ELBO=-91601.08, deltaELBO=18.721 (0.00560018%), Factors=4\nIteration 27: time=0.01, Factors=4\nIteration 28: time=0.00, Factors=4\nIteration 29: time=0.01, Factors=4\nOptimising sigma node...\nIteration 30: time=0.66, Factors=4\nIteration 31: time=0.01, ELBO=-91587.99, deltaELBO=13.095 (0.00391731%), Factors=4\nIteration 32: time=0.01, Factors=4\nIteration 33: time=0.01, Factors=4\nIteration 34: time=0.01, Factors=4\nIteration 35: time=0.01, Factors=4\nIteration 36: time=0.01, ELBO=-91578.67, deltaELBO=9.317 (0.00278709%), Factors=4\nIteration 37: time=0.01, Factors=4\nIteration 38: time=0.01, Factors=4\nIteration 39: time=0.01, Factors=4\nOptimising sigma node...\nIteration 40: time=0.71, Factors=4\nIteration 41: time=0.01, ELBO=-91571.49, deltaELBO=7.182 (0.00214829%), Factors=4\nIteration 42: time=0.01, Factors=4\nIteration 43: time=0.01, Factors=4\nIteration 44: time=0.01, Factors=4\nIteration 45: time=0.00, Factors=4\nIteration 46: time=0.01, ELBO=-91565.91, deltaELBO=5.583 (0.00167015%), Factors=4\nIteration 47: time=0.01, Factors=4\nIteration 48: time=0.00, Factors=4\nIteration 49: time=0.00, Factors=4\nOptimising sigma node...\nIteration 50: time=0.68, Factors=4\nIteration 51: time=0.01, ELBO=-91561.42, deltaELBO=4.484 (0.00134132%), Factors=4\nIteration 52: time=0.01, Factors=4\nIteration 53: time=0.01, Factors=4\nIteration 54: time=0.01, Factors=4\nIteration 55: time=0.01, Factors=4\nIteration 56: time=0.01, ELBO=-91557.78, deltaELBO=3.644 (0.00109016%), Factors=4\nIteration 57: time=0.01, Factors=4\nIteration 58: time=0.01, Factors=4\nIteration 59: time=0.01, Factors=4\nOptimising sigma node...\nIteration 60: time=0.74, Factors=4\nIteration 61: time=0.02, ELBO=-91554.72, deltaELBO=3.057 (0.00091448%), Factors=4\nIteration 62: time=0.01, Factors=4\nIteration 63: time=0.01, Factors=4\nIteration 64: time=0.02, Factors=4\nIteration 65: time=0.01, Factors=4\nIteration 66: time=0.01, ELBO=-91552.11, deltaELBO=2.612 (0.00078128%), Factors=4\nIteration 67: time=0.01, Factors=4\nIteration 68: time=0.01, Factors=4\nIteration 69: time=0.01, Factors=4\nOptimising sigma node...\nIteration 70: time=0.73, Factors=4\nIteration 71: time=0.01, ELBO=-91549.81, deltaELBO=2.300 (0.00068805%), Factors=4\nIteration 72: time=0.01, Factors=4\nIteration 73: time=0.01, Factors=4\nIteration 74: time=0.01, Factors=4\nIteration 75: time=0.01, Factors=4\nIteration 76: time=0.01, ELBO=-91547.74, deltaELBO=2.065 (0.00061777%), Factors=4\nIteration 77: time=0.01, Factors=4\nIteration 78: time=0.01, Factors=4\nIteration 79: time=0.01, Factors=4\nOptimising sigma node...\nIteration 80: time=0.74, Factors=4\nIteration 81: time=0.01, ELBO=-91545.80, deltaELBO=1.947 (0.00058242%), Factors=4\nIteration 82: time=0.01, Factors=4\nIteration 83: time=0.01, Factors=4\nIteration 84: time=0.01, Factors=4\nIteration 85: time=0.01, Factors=4\nIteration 86: time=0.01, ELBO=-91543.95, deltaELBO=1.850 (0.00055353%), Factors=4\nIteration 87: time=0.01, Factors=4\nIteration 88: time=0.01, Factors=4\nIteration 89: time=0.01, Factors=4\nOptimising sigma node...\nIteration 90: time=0.83, Factors=4\nIteration 91: time=0.01, ELBO=-91542.09, deltaELBO=1.858 (0.00055576%), Factors=4\nIteration 92: time=0.01, Factors=4\nIteration 93: time=0.01, Factors=4\nIteration 94: time=0.01, Factors=4\nIteration 95: time=0.01, Factors=4\nIteration 96: time=0.01, ELBO=-91540.21, deltaELBO=1.882 (0.00056308%), Factors=4\nIteration 97: time=0.01, Factors=4\nIteration 98: time=0.01, Factors=4\nIteration 99: time=0.01, Factors=4\n\n\n#######################\n## Training finished ##\n#######################\n\n\nSaving model in /var/folders/kh/tgq9mmld6_v9z_h220trj0c40000gn/T//RtmpFfp50U/mofa_20231106-091122.hdf5...\n\n\nWarning in .quality_control(object, verbose = verbose): Factor(s) 2 are strongly correlated with the total number of expressed features for at least one of your omics. Such factors appear when there are differences in the total 'levels' between your samples, *sometimes* because of poor normalisation in the preprocessing steps.\n\n\nIn the above code, we begin by generating an artificial time-based covariate that follows a sinusoidal pattern. We then introduce this time-based trend into certain features of three modalities. Specifically, the first few features of each data type have this sinusoidal pattern superimposed on them. With our modified data in hand, we prepare it for a MOFA (Multi-Omics Factor Analysis) analysis.\nWe also convert our time-based covariate into a data frame format, ensuring its naming aligns with the miRNA data’s column names. With all data elements ready, we create a MOFA object and associate the time covariate with it. We then set default options for the data, model, training, and MEFISTO (an extension of MOFA). With everything set up, we finalize the MOFA object’s configuration with our specified options and initiate the MOFA analysis.\nWe can now have a look the expained variance across different data views:\n\n# Plot the variance explained by the factors\nplot_variance_explained(sm)\n\n\n\n\nMore or less all factors have some activities but the first factor has much more activity compared to rest of them. What we are going to do now is to figure out which factors are related to cancer subtypes and which ones related to the time trend. We need to set the covariates and plot the factors:\n\n# Add metadata (sample subtype and time covariate) to the MOFA object\nsamples_metadata(sm) &lt;- data.frame(sample=colnames(data_mofa$mirna), \n                                   subtype=breast.TCGA$data.train$subtype, \n                                   time=time_covariate,trend=sinusoidal_trend)\n\n# Plot the factors against the time covariate and color them by the time, while shaping them by the subtype\nplot_factors_vs_cov(sm, color_by = \"time\", shape_by = \"subtype\")\n\n\n\n\nWhat we can see is that the first factor is associated with subtype whereas the third one is more related to the trend. We can see this better if we start interpolating factors.\n\nsm &lt;- interpolate_factors(sm, new_values = seq(1, 150, by=0.1))\nplot_interpolation_vs_covariate(sm, covariate = \"time\")\n\n\n\n\nIf the we compare the graphs above with the actual trend we can immediately see that the third factor is much smoother representation of the actual trend:\n\nscatter.smooth(sinusoidal_trend~time_covariate,span = 0.1,evaluation = 500)\n\n\n\n\nGiven this, the main question is if MOFA has been successful in capturing the variables reflecting the trend? We know exactly what variables we changed so we can compare it to what MOFA has found.\n\n# Plot the weights of the top 20 features for the third factor\n\ncolor_mirna=data.frame(feature=rownames(sm@data$mirna$group1),\n                       sinusoidal=rep(c(\"Yes\",\"No\"),c(20,nrow(sm@data$mirna$group1)-20)),\n                       view=\"mirna\")\ncolor_mrna=data.frame(feature=rownames(sm@data$mrna$group1),\n                      sinusoidal=rep(c(\"Yes\",\"No\"),c(25,nrow(sm@data$mrna$group1)-25)),\n                      view=\"mrna\")\ncolor_protein=data.frame(feature=rownames(sm@data$protein$group1),sinusoidal=rep(c(\"Yes\",\"No\"),c(15,nrow(sm@data$protein$group1)-15)),view=\"protein\")\n\nfeatures_metadata(sm)&lt;-rbind(color_mirna,color_mrna,color_protein)\nlibrary(ggplot2)\nplot_weights(sm, factors = 3, view = \"mirna\", nfeatures = 20,color_by = \"sinusoidal\")+ggtitle(\"miRNA\")\nplot_weights(sm, factors = 3, view = \"mrna\", nfeatures = 25,color_by = \"sinusoidal\")+ggtitle(\"mRNA\")\nplot_weights(sm, factors = 3, view = \"protein\", nfeatures = 15,color_by = \"sinusoidal\")+ggtitle(\"protein\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plot above shows the weights of each feature on the x-axis and its ranking on the y-axis. What this plot shows us is that all top features selected by MOFA are in fact the ones we added the trend to. So MOFA has been almost perfectly successful in extracting the underlining pattern and important features. After getting the important features the rest of the analysis is more or less similar to classical data integration. In addition, similar kind of analysis can be performed on spatial data. For more information visit https://biofam.github.io/MOFA2/MEFISTO.html\nWe reached end of this lab now. There are very good tutorials on https://biofam.github.io/MOFA2/tutorials.html"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nData Integration using MOFA lab comes in two flavors, light and detailed version. See a short description on what to except from each and feel free to choose the one you want to work with during the workshop.\n\nLight version\n\nShows how we use MOFA2 package for factor analysis model, integrating multi-omics data in unsupervised way.\nGives minimalist explanation of the method, skipping mathematical foundations. It may be good as first read or as a quick guide how to use MOFA2 when already knowing the foundations behind the methods.\n\nTake me to the light version\n\n\nDetailed version\n\nIntroduces MOFA starting from mathematical foundations of PCA. Goes over PCA, PPCA, BPCA, CCA, BCCA to reach GA MOFA.\nIt includes mathematical foundations and R code from scratch. Yay!\nIt shows how to run things with MOFA2, also introducing time series analysis (R code and via MEFISTO).\nMay take some time to go through and be slightly challenging. Recommended when wanting to understand the methods.\n\nTake me to the detailed version"
  },
  {
    "objectID": "mofa-prev.html",
    "href": "mofa-prev.html",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "",
    "text": "Setting up environment\nYou will need to install a few packages to fully run this notebook. The main packages needed are MOFA2 and ggplot2 but in order to run bayesian modeling you will need to install rstan. A good place to start with rstan installation is here: https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\nBiocManager::install(\"MOFA2\")\n\n# list of packages to be installed\npackages &lt;- c(\"rstan\",\"ggplot2\")\n\n# check and install missing packages\nnew_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages, dependencies = TRUE, type = \"binary\")"
  },
  {
    "objectID": "mofa-prev.html#pca-in-r",
    "href": "mofa-prev.html#pca-in-r",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "PCA in R",
    "text": "PCA in R\nDoing PCA in R using SVD is straight forward. Wecenter our data and use the svd function.\n\n# center the data\ndata_centered_mrna &lt;- scale(breast.TCGA$data.train$mrna,center = TRUE,scale = FALSE)\n\n# do SVD\nsvd_mrna &lt;- svd(data_centered_mrna)\n\n# calculate the PC scores\ncalculated_scores &lt;- data_centered_mrna%*%svd_mrna$v\n\n# plot the PC scores\nplot(calculated_scores[,1:2],xlab=\"pc1\",ylab=\"pc2\",col=breast.TCGA$data.train$subtype)\n\n\n\n\n\n\n\n\nThis will give us identical results comapred to the for example standard prcomp functio\n\n# do PCA using prcomp\npca_prcomp &lt;- prcomp(data_centered_mrna)\n\n# plot the PCA\nplot(pca_prcomp$x[,1:2],xlab=\"pc1\",ylab=\"pc2\",col=breast.TCGA$data.train$subtype)\n\n\n\n\n\n\n\n\nIn practice there are more specialized packages that can be used to do PCA. For instance mixOmics provides a very powerful PCA method that provide us not only with standard PCA but also with extra advantages (e.g. missing value handling, plotting, handling repeated measurements etc.). See DataIntegration using mixOmics labs for some examples.\nThis observed separation and overlap in the PCA plot is not just a graphical representation but is rooted in the underlying biology of these cancer subtypes. The positioning of the different groups on the PCA plot is influenced by the expression levels of various mRNAs, each contributing differently to the principal components.\nNow, as we go deeper into understanding the PCA plot, it becomes essential to explore the concept of loadings. Loadings help us interpret the contribution of each miRNA to the principal components. They provide insights into which specific miRNAs are driving the separation between different cancer subtypes observed in the PCA plot.\nWe can go ahead and plot the loadings. We start with our most important PC, that is PC1\n\n# loadings for component 1\nloadings &lt;- pca_prcomp$rotation[,1]\n\n# sort the loadings\nsorted_loadings &lt;- loadings[order(abs(loadings),decreasing = T)]\n\n# plot the loadings in a flipped bar plot\npar(mar = c(3, 6, 3, 2))\nbarplot(sorted_loadings, horiz=TRUE, las=1, \n        main=\"PCA Loadings\", xlab=\"Loadings\", \n        border=\"blue\", col=\"skyblue\")\n\n\n\n\n\n\n\n\nIn this bar plot, each bar represents a specific mRNA. The length of the bar corresponds to the value of the loading of that mRNA on PC1, indicating its contribution to this principal component. The mRNAs with the highest absolute contributions are at the bottom, and those with the lowest are at the top, making it easy to identify the most influential mRNAs. Both the length and direction of each bar provide crucial insights into the mRNA’s contribution to the first principal component (PC1). The length of the bar signifies the magnitude of the mRNA’s contribution. Longer bars indicate miRNAs that have a more substantial influence on the variance captured by PC1, highlighting them as key elements in distinguishing different patterns of gene expression within the dataset.\nThe direction of the bar adds another layer of interpretation. Bars extending to the right represent mRNAs that are positively correlated with PC1, indicating that as the values of these mRNAs increase, so does the score of PC1. Conversely, bars extending to the left suggest a negative correlation, meaning as the values of these miRNAs increase, the score of PC1 decreases. This directional information can be important in understanding the expression patterns of mRNAs in different breast cancer subtypes. For instance, mRNAs that are positively correlated with PC1 might be highly expressed in the Basal subtype but low in others, offering insights into the molecular distinctions between these cancer subtypes.\nScore plot together with loading give us powerful tool to investigate pattern in a single dataset."
  },
  {
    "objectID": "mofa-prev.html#probabilistic-principal-component-analysis-ppca",
    "href": "mofa-prev.html#probabilistic-principal-component-analysis-ppca",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "Probabilistic Principal Component Analysis (PPCA)",
    "text": "Probabilistic Principal Component Analysis (PPCA)\nAfter understanding the foundation of PCA, it is time to explore its probabilistic counterpart that is Probabilistic Principal Component Analysis (PPCA). While PCA provides us with a deterministic approach to do data reduction and feature extraction, PPCA introduces a probabilistic framework that models the uncertainties in the data. This transition from a deterministic method to a more flexible, probabilistic one allows for a more systematic understanding of data structures, especially when there is noise or missing values.\nIn PPCA, the relationship between the observed data vector $Y $in \\(\\mathbb{R}^{D}\\) and the latent or principal component coordinates \\(Z\\) in \\(\\mathbb{R}^{d}\\) is expressed by:\n\\[X = ZW^T + \\epsilon\\]\nWhere \\(X\\) is the observed data vector. \\(Z\\) represents the latent variables or the principal component coordinates. \\(W\\) is a matrix of size \\(d \\times D\\) that defines the linear relationship between \\(Z\\) and \\(Y\\). \\(\\epsilon\\) is a Gaussian noise term, which accounts for the variability not captured by the principal components.\nGiven our equation, we start with a prior on the latent variables, \\(Z\\), assuming they are drawn from a standard Gaussian distribution: \\[ p(Z) = N(Z; 0, I) \\] This means that the principal component coordinates have a mean of zero and an identity covariance matrix.\nGiven the linear relationship \\(Y = ZW^T + \\epsilon\\), the conditional distribution of \\(X\\) given \\(Z\\) is \\(p(X|Z) = N(X; ZW^T, \\sigma^2 I)\\) This equation suggests that the observed data \\(X\\) is normally distributed around the value produced by projecting \\(Z\\) onto the data space using \\(W^T\\), with a variance of \\(\\sigma^2\\).\nRemember now that our observed data is \\(X\\) but what we are interested in is \\(Z\\) so we need to calculate \\[ p(Z|X)\\]. Therefore using the Bayes’ rule for Gaussian distributions:\n\\[p(z|x) = \\frac{p(x|z) \\cdot p(z)}{p(x)}\\] We do not need the denominator \\(p(x)\\) explicitly as it is a normalization constant.\n\\[ p(z|x) \\propto p(x|z) \\cdot p(z) \\] In the above equation \\(p(z|x)\\) is normally distributed thus\n\\[\np(z|x)=N(z,\\mu,\\Sigma)\n\\]\nIt can be shown that \\[\n\\mu=\\sigma^{-2}\\Sigma Wx\n\\] and \\[\n\\Sigma^{-1}=I+\\sigma^{-2}CC^T\\] To estimate the model parameters, namely \\(W\\) and \\(\\sigma^2\\), we employ the Expectation-Maximization (EM) algorithm. This iterative algorithm seeks to maximize the likelihood of observing the data \\(x\\) given the model parameters.\nThe EM algorithm consists of two main steps:\n\nE-Step: Here, we estimate the distribution of the latent variable \\(z\\) given the observed data \\(x\\) and the current estimate of the model parameters.\nM-Step: In this step, we update the model parameters \\(W\\) and \\(\\sigma^2\\) to maximize the expected complete-data log-likelihood.\n\nOne challenge in directly maximizing the log-likelihood \\(log p(x)\\) (which quantifies how well our model describes the observed data) is its complexity due to the latent variable \\(z\\). To fix this, we introduce a distribution \\(q(z)\\) over \\(z\\) and derive a lower bound on the log-likelihood.\nOur goal is to express the log marginal likelihood, \\(\\log p(X)\\), in terms of some function involving \\(q(Z)\\). The marginal likelihood is defined as:\n\\[p(X) = \\int p(X, Z) dZ\\] The marginal likelihood represents the probability of the observed data under a particular model, marginalized (or summed/integrated) over all possible values of the latent variables or parameters. So essentially, we are adding up the likelihood of the data \\(X\\) for each possible value or configuration of \\(Z\\).\nUnfortunate, the integral over \\(Z\\) can make the optimization intractable. So we are going to use \\(q(Z)\\) which is chosen from a family of distributions that are tractable and easy to work with. The goal here is to find an approximation to the true but intractable posterior \\(p(Z|X)\\). To introduce \\(q(Z)\\) into our equation, we multiply the inside of the integral by \\(q(Z)\\) and divide by \\(q(Z)\\). Remember, any number divided by itself is 1 (obviously!), so this step is essentially multiplying by 1 (which does not change the value of the integral).\n\\[p(X) = \\int p(X, Z) \\times \\frac{q(Z)}{q(Z)} dZ\\] If we expand the equation we get:\n\\[p(X) = \\int q(Z) \\times \\frac{p(X, Z)}{q(Z)} dZ\\] Now things become much easier. The first thing we want to do is to work on \\(log p\\) instead of probabilities. So we will take the logarithm of both sides:\n\\[\\log p(X) = \\log \\int q(Z) \\frac{p(X, Z)}{q(Z)} dZ\\] However, we are still dealing with a complex log of integrals. What we want to do is to move the log inside the integral so that we can simplify the integrand, making the integration more tractable.\nSince log is concave, we can use Jensen’s inequality, \\(f(E[X]) \\geq E[f(X)]\\). Jensen’s inequality essentially says that for a concave function, the function of an average is always greater than or equal to the average of the function. In the context of our expression, it allows us to move the logarithm inside the integral:\n\\[\\log p(X) \\geq \\int q(Z) \\log \\frac{p(X, Z)}{q(Z)} dZ\\] Expanding the logarithm of the fraction: \\[\\log p(X) \\geq \\int q(Z) [\\log p(X, Z) - \\log q(Z)] dZ\\]\nThis is simply using the logarithmic property: \\[\\log \\frac{a}{b} = \\log a - \\log b\\]\nNow, we separate the integrals: \\[\\log p(X) \\geq \\int q(Z) \\log p(X, Z) dZ - \\int q(Z) \\log q(Z) dZ\\]\nThe two integrals represent two terms: 1. The first integral \\(\\int q(Z) \\log p(X, Z) dZ\\) represents the expected joint log-likelihood of the data and the latent variables, when the expectation is taken with respect to \\(q(Z)\\).\n\nThe second integral \\(\\int q(Z) \\log q(Z) dZ\\) represents the entropy of the variational distribution \\(q(Z)\\).\n\nUsing the definition of expectation, the first integral is: \\[E_q[\\log p(X, Z)]\\] And breaking down the joint probability using the definition of conditional probability: \\[E_q[\\log p(X, Z)] = E_q[\\log p(Z) + \\log p(X|Z)]\\]\nThe second integral, as mentioned, is just the negative entropy: \\[-H(q(Z))\\]\nCombining these terms, we get: \\[\\log p(X) \\geq E_q[\\log p(Z) + \\log p(X|Z)] - H(q(Z))\\]\nWhich we can rearrange to: \\[\\log p(X) \\geq H(q(Z)) + E_q[\\log p(Z) + \\log p(X|Z)]\\] The right-hand side of the inequality is the ELBO (Evidence Lower BOund (ELBO)), which we try to maximize. The entropy term quantifies the uncertainty associated with the distribution and the other term is the expected joint log-likelihood of the data and latent variables under the variational distribution.\nAnyway, given the likelihood function we can now derive our E and M steps of EM.\nWe can simply look at what we derived for \\(p(z|x)\\) which was: \\[\n\\mu=\\sigma^{-2}\\Sigma Wx\n\\] and \\[\n\\Sigma^{-1}=I+\\sigma^{-2}CC^T\\]\nWe can just replace the values to get to expected value of \\(Z\\)\n\\[\nZ=\\sigma^{-2}(I+\\sigma^{-2}CC^T)^{-1} Wx\n\\]\nWe have \\(N\\) data points so we need to sum over them so, we have: \\[\\sum_{n=1}^{N} log p(x_n) \\geq \\sum_{n=1}^{N} \\left( H(q(z_n)) + E_{q(z_n)}[log p(z_n) + log p(x_n|z_n)] \\right)\\]\nExpanding the expectation term for \\(N\\) data points and using the Gaussian distributions, we get: \\[\\sum_{n=1}^{N} E_{q(z_n)}[log p(z_n) + log p(x_n|z_n)] = \\] \\[ - \\frac{1}{2\\sigma^2} \\sum_{n=1}^{N} \\left( \\lVert x_n - W^⊤z_n \\rVert^2 + \\text{Tr}{W^⊤Σ_nW} \\right) - \\frac{1}{2\\sigma^2_{\\text{old}}} \\sum_{n=1}^{N} \\lVert z_n \\rVert^2 - \\frac{N}{2} \\text{Tr}{Σ_n} \\]\nwhere \\(x_n\\) is the observed data point and \\(z_n\\) is the latent representation of \\(x_n\\). The equation might seem complex but it has three main parts:\n\nThe reconstruction error (\\(\\lVert x_n - W^⊤z_n \\rVert^2\\)), which quantifies how well our model can generate the observed data from the latent variables.\nThe variance or spread of the latent variables (\\(\\lVert z_n \\rVert^2\\)).\nThe term (\\(\\text{Tr}{W^⊤Σ_nW}\\)) which captures the total variance explained by the latent variables.\n\nTo determine the model parameters that maximize this bound, we are going to differentiate with respect to \\(W\\) and \\(\\sigma^2\\). Setting these derivatives to zero and we are almost there!\n\\[W = \\left( \\sum_{n=1}^{N} Σ_n + z_n z_n^⊤ \\right)^{-1} \\sum_{n=1}^{N} z_n x_n^⊤\\]\nand \\[\\sigma^2 = \\frac{1}{ND} \\sum_{n=1}^{N} \\left( \\lVert x_n - W^⊤z_n \\rVert^2 + \\text{Tr}{W^⊤Σ_nW} \\right)\\]\nRecall our expanded equation for the lower bound on the log-likelihood: \\[\\sum_{n=1}^{N} log p(x_n) \\geq \\sum_{n=1}^{N} \\left( H(q(z_n)) + E_{q(z_n)}[log p(z_n) + log p(x_n|z_n)] \\right)\\]\nThe entropy \\(H\\) of a Gaussian with covariance matrix \\(Σ\\) is: \\[H = \\frac{1}{2} log |Σ|\\]\nPutting this into our equation, we obtain: \\[\\sum_{n=1}^{N} log p(x_n) \\geq \\sum_{n=1}^{N} \\left( \\frac{1}{2} log |Σ| + E_{q(z_n)}[log p(z_n) + log p(x_n|z_n)] \\right)\\]\nWe can further expand the right-hand side: \\[log p(x) \\geq -ND\\left(1 + log \\sigma^2\\right) - N\\left(Tr\\{Σ\\} - log |Σ|\\right) - \\frac{1}{2\\sigma^2_{\\text{old}}} \\sum_{n=1}^{N} \\lVert z_n \\rVert^2\\]\nHere: \\(N\\) represents the number of data points. \\(D\\) is the dimension of the data vector $ x_n $.\nThis equation represents the EM bound after performing the M-step. The objective in the M-step of the EM algorithm is to maximize this bound with respect to the model parameters \\(W\\) and \\(\\sigma^2\\). By doing so, we iteratively refine our model to better fit the observed data.\n\nPPCA R implementation\nLet’s try to implement the above in R:\n\n# load the breast cancer dataset for training\ninput_data &lt;- breast.TCGA$data.train$mrna\n\n# define the number of data points (samples) and the dimensionality of the data (genes/features)\nN_data &lt;- nrow(input_data)\nD_data &lt;- ncol(input_data)\n\n# define the number of principal components to be extracted\nnPcs &lt;- 2\n\n# define the convergence threshold and maximum number of iterations for the EM algorithm\nthreshold &lt;- 0.0001\nmaxIterations &lt;- 100\n\n# set a seed for reproducibility\nset.seed(123)\n\n# initialization: randomly initialize the W matrix from the data\nW &lt;- t(input_data[sample(N_data, size = nPcs), , drop = FALSE])\nW &lt;- matrix(rnorm(length(W)), nrow(W), ncol(W))\n\n# precompute W'W for efficiency\nWtW &lt;- crossprod(W)\n\n# compute the latent representation Z based on the initial W\nZ &lt;- input_data %*% W %*% solve(WtW)\n\n# calculate the initial reconstruction and its error\nreconstructed &lt;- Z %*% t(W)\nerror_ss &lt;- sum((reconstructed - input_data)^2) / (N_data * D_data)\n\n# initialize the iteration counter and the previous objective value for convergence checking\niteration &lt;- 1\nprevious &lt;- Inf\n\n# start the EM algorithm\nwhile (TRUE) {\n  # E-Step: estimate the covariance of the latent variable Z\n  Z_cov &lt;- solve(diag(nPcs) + WtW/error_ss)\n  \n  # compute the posterior mean of Z\n  Z &lt;- input_data %*% W %*% Z_cov/error_ss\n  ZtZ &lt;- crossprod(Z)\n  \n  # M-Step: update W based on the estimated Z\n  W &lt;- (t(input_data) %*% Z) %*% solve((ZtZ + N_data * Z_cov))\n  WtW &lt;- crossprod(W)\n  \n  # recalculate the reconstruction error based on the updated W\n  error_ss &lt;- (sum((W %*% t(Z) - t(input_data))^2) + \n                 N_data * sum(WtW %*% Z_cov))/(N_data * D_data)\n  \n  # calculate the EM objective (the lower bound of the log-likelihood)\n  obj_val &lt;- N_data * (D_data * log(error_ss) + \n                         sum(diag(Z_cov)) - log(det(Z_cov))) + sum(diag(ZtZ))\n  \n  # check for convergence\n  relative_change &lt;- abs(1 - obj_val/previous)\n  previous &lt;- obj_val\n  \n  iteration &lt;- iteration + 1\n  \n  if (relative_change &lt; threshold | iteration &gt; maxIterations) {\n    break\n  } \n}\n\n# orthogonalize W for stability\nW &lt;- svd(W)$u\n\n# recalculate eigenvalues and eigenvectors after orthogonalization\neig_vals &lt;- eigen(cov(input_data %*% W))$values\neig_vecs &lt;- eigen(cov(input_data %*% W))$vectors\n\n# update W based on the new eigenvectors\nloadings &lt;- W %*% eig_vecs\nscores &lt;- input_data %*% loadings\n\nplot(scores,xlab=\"pc1\",ylab=\"pc2\",col=breast.TCGA$data.train$subtype)"
  },
  {
    "objectID": "mofa-prev.html#bayesian-principal-component-analysis-bpca",
    "href": "mofa-prev.html#bayesian-principal-component-analysis-bpca",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "Bayesian Principal Component Analysis (BPCA)",
    "text": "Bayesian Principal Component Analysis (BPCA)\nIn the traditional Probabilistic Principal Component Analysis (PPCA), we made certain probabilistic assumptions, specifically over the latent variables \\(Z\\) and the noise term \\(\\epsilon\\). The other parameters were treated as non-probabilistic. However, in BPCA, we take a step further by assuming a probabilistic distribution over all the parameters. This provides a more comprehensive probabilistic model.\nGiven the linear relationship: \\[ \\mathbf{x}_n = \\mathbf{W}\\mathbf{z}_n + \\boldsymbol{\\epsilon}_n \\]\nIn BPCA, we make the following probabilistic assumptions:\n\nThe latent variables \\(\\mathbf z\\) follow a standard normal distribution: \\[ \\mathbf z \\sim \\mathcal{N}(0, 1) \\]\nThe weight matrix \\(\\mathbf{W}\\) also follows a standard normal distribution: \\[ \\mathbf{W} \\sim \\mathcal{N}(0, 1) \\] Both the latent variables and the weight matrix are assumed to follow standard normal distributions. This assumption is in line with the traditional PCA where the principal components are orthogonal and typically standardized to have unit variance. The normal distribution assumption implies that the values of the latent variables and weights are most likely to be around their means (which is zero) and decrease in likelihood as they move away from the mean.\nThe precision parameter \\(\\tau\\) follows a gamma distribution with parameters \\(\\alpha_0\\) and \\(\\beta_0\\): \\[ \\tau \\sim \\mathcal{G}(\\alpha_0, \\beta_0) \\] The gamma distribution is a flexible choice for modeling positive continuous variables like precision. The shape and rate parameters \\(\\alpha_0\\) and \\(\\beta_0\\) can be seen as hyperparameters, and their values can be chosen based on prior knowledge or set in a non-informative manner.\nThe noise term \\(\\epsilon_n\\) is normally distributed with mean 0 and precision \\(\\tau^{-1}\\): \\[ \\epsilon_n \\sim \\mathcal{N}(0, \\tau^{-1}) \\] This is a common assumption in many statistical models, implying that the errors (or deviations from the model) are symmetrically distributed around zero. The variance of this noise is controlled by the precision parameter \\(\\tau\\).\nThe observed data \\(\\mathbf{x}_n\\) follows a normal distribution with mean \\(\\mathbf W\\mathbf z_n\\) and covariance \\(\\tau^{-1}\\): \\[ \\mathbf{x}_n \\sim \\mathcal{N}(\\mathbf W\\mathbf z_n, \\tau^{-1} \\mathbf ) \\] The choice of a normal distribution here signifies that the observed data points are most likely to lie close to the subspace spanned by the principal components and deviations from this subspace are captured by the noise term.\n\nOne of the main advantages of BPCA over traditional PCA is that it provides a probabilistic framework, allowing us to quantify the uncertainties associated with the estimated parameters. In addition, BPCA can automatically determine the number of relevant principal components, unlike traditional PCA where the number of components needs to be specified or chosen.\nIn BPCA, our primary parameters of interest are the latent variables \\(\\mathbf{z}\\), the weight matrix \\(\\mathbf{W}\\), the precision parameter \\(\\tau\\), and the noise term \\(\\epsilon_n\\). Bayesian inference provides a good way to estimate these parameters, but exact inference can be computationally difficult, especially in high-dimensional setting This is where we can use algorithms like Variational Inference (VI) or MCMC (Markov Chain Monte Carlo) to simplify the problem. We are going to use Variational Inference here. This is an approximate inference technique that turns the inference problem into an optimization problem. The idea is to approximate the true posterior distribution of the parameters with a simpler, factorized distribution, referred to as the “mean-field” approximation.\nWe are going to assume that the approximate posterior factorizes across all the parameters, i.e., \\[q(\\mathbf{z}, \\mathbf{W}, \\tau, \\epsilon_n) = q(\\mathbf{z})q(\\mathbf{W})q(\\tau)q(\\epsilon_n)\\]\nThe goal here is to minimize the KL divergence between the approximate posterior and the true posterior. This is equivalent to ELBO, which as said before provides a lower bound to the log marginal likelihood of the data. Using an iterative algorithm, we adjust the parameters of our mean-field distribution to maximize the ELBO (we saw in PPCA how ELBO is calculated). Common techniques include coordinate ascent or gradient-based methods can be used. Once the ELBO is maximized, the parameters of the factorized distributions give us the approximate posterior means and variances for \\(\\mathbf{z}\\), \\(\\mathbf{W}\\), \\(\\tau\\), and \\(\\epsilon_n\\). We can then use the estimated parameters to generate new data points, reconstruct the original data with reduced dimensions, or project new observations onto the principal components, thereby facilitating visualization, clustering, etc.\n\nBPCA R implementation\nR provides an excellent interface to Stan through the rstan package, allowing users to build, fit, and interrogate complex Bayesian models with ease. We are going to use rstan here.\nYou can run the script if you have successfully installed rstan package otherwise you can skip running the scripts and continue reading.\n\n# Load necessary libraries\nrequire(rstan)\n\n# Configure rstan options\nrstan_options(auto_write = FALSE)\n# Set the number of cores for parallel processing\noptions(mc.cores = parallel::detectCores())\n\n# Define the BPCA model in Stan language\nbpca &lt;- \"\ndata {\n        int&lt;lower=0&gt; N; // Number of samples\n        int&lt;lower=0&gt; D; // The original dimension\n        int&lt;lower=0&gt; K; // The latent dimension\n        matrix[N, D] X; // The data matrix\n    }\n\n    parameters {\n        matrix[N, K] Z; // The latent matrix\n        matrix[D, K] W; // The weight matrix\n        real&lt;lower=0&gt; tau; // Noise term \n    }\n\n    transformed parameters{\n        real&lt;lower=0&gt; t_tau; // Transformed precision term for noise\n    t_tau = inv(sqrt(tau)); // Compute the inverse of the square root of tau\n    }\n    \n    model {\n        // Prior distributions for the latent matrix and weight matrix\n        to_vector(Z) ~ normal(0,1);\n        to_vector(W)~ normal(0, 1);\n        \n        // Prior distribution for the noise precision term\n        tau ~ gamma(1,1);               \n        \n        // Likelihood for the observed data\n        to_vector(X) ~ normal(to_vector(Z*W'), t_tau);\n\n    }\"\n\n# Compile the Stan model\nbpca_model &lt;- stan_model(model_code = bpca)\n\n# Load and preprocess the breast cancer dataset\nX &lt;- scale(breast.TCGA$data.train$mrna,scale = T,center = T)\n\n# Prepare the data for Stan\ndata_input &lt;- list(N = dim(X)[1], D = dim(X)[2], K = 2, X = X)\n\n# Set random seed for reproducibility\nset.seed(200)\n\n# Fit the BPCA model using Variational Bayes with the meanfield algorithm\nbpca_fit &lt;- vb(bpca_model, data = data_input, algorithm = \"meanfield\", iter = 1000, output_samples = 100)\n\nChain 1: ------------------------------------------------------------\nChain 1: EXPERIMENTAL ALGORITHM:\nChain 1:   This procedure has not been thoroughly tested and may be unstable\nChain 1:   or buggy. The interface is subject to change.\nChain 1: ------------------------------------------------------------\nChain 1: \nChain 1: \nChain 1: \nChain 1: Gradient evaluation took 0.001322 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 13.22 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Begin eta adaptation.\nChain 1: Iteration:   1 / 250 [  0%]  (Adaptation)\nChain 1: Iteration:  50 / 250 [ 20%]  (Adaptation)\nChain 1: Iteration: 100 / 250 [ 40%]  (Adaptation)\nChain 1: Iteration: 150 / 250 [ 60%]  (Adaptation)\nChain 1: Iteration: 200 / 250 [ 80%]  (Adaptation)\nChain 1: Success! Found best value [eta = 1] earlier than expected.\nChain 1: \nChain 1: Begin stochastic gradient ascent.\nChain 1:   iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes \nChain 1:    100       -40058.050             1.000            1.000\nChain 1:    200       -38590.189             0.519            1.000\nChain 1:    300       -38538.645             0.020            0.038\nChain 1:    400       -38530.234             0.001            0.001   MEAN ELBO CONVERGED   MEDIAN ELBO CONVERGED\nChain 1: \nChain 1: Drawing a sample of size 100 from the approximate posterior... \nChain 1: COMPLETED.\n\n# Extract the latent scores from the fit\nscores &lt;- apply(extract(bpca_fit,\"Z\")[[1]], c(2,3), mean)\n\n# Plot the latent scores with colors representing subtypes\nplot(scores,xlab=\"pc1\",ylab=\"pc2\",col=breast.TCGA$data.train$subtype)\n\n\n\n\nThe Stan code here is structured into four main sections, each serving a purpose in the Bayesian modeling framework. The data section declares the observed data and its dimensions, specifying the number of samples (N), the original data dimension (D), the latent dimension (K), and the actual data matrix (X). The parameters section introduces the model’s primary unknowns: the latent matrix (Z) that captures the underlying structure, the weight matrix (W) that maps latent variables to the observed space, and the noise precision parameter (tau). In the transformed parameters section, we computed a derived parameter, t_tau as the inverse of the square root of tau, offering a transformed precision term for the noise. Finally, the model section defines the probabilistic relationships in the model. Here, prior distributions are set for the latent matrix, weight matrix, and noise precision term, while the likelihood of the observed data is defined based on the latent scores, weight matrix, and the transformed noise precision.\nThe rest of the code essentially just compile the Stan code, prepare the data and run the inference. Probably one of the trickiest part of the code is apply(extract(bpca_fit,\"Z\")[[1]], c(2,3), mean). Can you guess what does this do?\nIf you remember, in the context of Bayesian analysis, we don’t deal with point estimate but we have a complete distribution over the parameters of interest. As the consequence we don’t have a single PCA score (PC) for each data point but rather we have a complete distribution. we chose to draw 100 samples (output_samples) from this distribution so we have for each data point 100 scores. I just took the average but we could take any of them or plot the complete distribution.\nWe have now implemented a complete Bayesian PCA in R and Stan. Time to move towards integration using two data views only."
  },
  {
    "objectID": "mofa-prev.html#cca-in-r",
    "href": "mofa-prev.html#cca-in-r",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "CCA in R",
    "text": "CCA in R\nLet’s have a look at how we can derive this in R using miRNA and mRNA data\n\n# center both of the data sets\nX_centered &lt;- scale(breast.TCGA$data.train$mrna, scale = FALSE)\nY_centered &lt;- scale(breast.TCGA$data.train$protein, scale = FALSE)\n\n# calculate cross-covariance matrix\ncross_cov &lt;- t(X_centered)%*%Y_centered\n\n# do a svd (single eigenvector) this is going to give us a signle CCA component\nsvd_result &lt;- svd(cross_cov,1,1)\n\n# extract the vectors\nU &lt;- svd_result$u\nV &lt;- svd_result$v\n\n# calculate the first canonical vectors (the most correlated latent factors)\ncanonical_vars_X &lt;- X_centered %*% U\ncanonical_vars_Y &lt;- Y_centered %*% V\n\n# deflate the original matrices\nX_centered &lt;- X_centered - canonical_vars_X %*% t((t(X_centered)%*%(canonical_vars_X)%*%solve(t(canonical_vars_X)%*%(canonical_vars_X))))\n\nY_centered &lt;- Y_centered - canonical_vars_Y %*% \n  t(t(Y_centered)%*%(canonical_vars_Y)%*%solve(t(canonical_vars_Y)%*%(canonical_vars_Y)))\n\n# redo the svd for the second component\ncross_cov &lt;- t(X_centered)%*%Y_centered\nsvd_result &lt;- svd(cross_cov,1,1)\n\nU &lt;- svd_result$u\nV &lt;- svd_result$v\n\n# calculate the second canonical vectors (the second most correlated latent factors)\ncanonical_vars_X2 &lt;- X_centered %*% U\ncanonical_vars_Y2 &lt;- Y_centered %*% V\n\npar(mfrow=c(2,2))\nplot(canonical_vars_X,canonical_vars_X2,col=breast.TCGA$data.train$subtype,xlab=\"l1\",ylab=\"l2\",main=\"CCA protein\")\nplot(canonical_vars_Y,canonical_vars_Y2,col=breast.TCGA$data.train$subtype,xlab=\"l1\",ylab=\"l2\",main=\"CCA protein\")\n\nplot(canonical_vars_X,canonical_vars_Y,col=breast.TCGA$data.train$subtype,xlab=\"mRNA\",ylab=\"protein\",main=\"l1\")\nplot(canonical_vars_X2,canonical_vars_Y2,col=breast.TCGA$data.train$subtype,xlab=\"mRNA\",ylab=\"protein\",main=\"l2\")\n\n\n\n\n\n\n\n\nThe plot above clearly shows that we ended up having a shared pattern in l1 (first CCA component). L1 captures the primary mode of correlation between protein and mRNA expression data. It represents the linear combinations of protein and mRNAs that are most strongly correlated. Since our interest right now is in the suptypes, we can probably ignore the second latent factor but we might as well try to explaining based on some other factors.\nIn the context of CCA, loadings play a role similar to that in PCA, yet they have a distinct interpretation. Similar to PCA, where loadings indicate the contribution of each original variable to the principal components, in CCA, the loadings show the contribution of each variable to the canonical variables. However, the difference lies in their meaning. While PCA loadings represent the contribution to the variance within a single dataset, CCA loadings show the contribution to the correlation between two datasets."
  },
  {
    "objectID": "mofa-prev.html#bayesian-canonical-correlation-analysis-bcca",
    "href": "mofa-prev.html#bayesian-canonical-correlation-analysis-bcca",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "Bayesian Canonical Correlation Analysis (BCCA)",
    "text": "Bayesian Canonical Correlation Analysis (BCCA)\nThe idea of Bayesian Canonical Correlation Analysis (BCCA) is simple. Recall that in BPCA we had \\(X = ZW^T + \\epsilon\\) in CCA however we have two data views (\\(X\\) and \\(Y\\)) that we are after a shared latent factor for them. So we can rewrite our BPCA equations:\n\\[X = ZW_x^T + \\epsilon_x\\] \\[Y = ZW_y^T + \\epsilon_y\\]\nIf you look a the equation we can see we have a shared latent factor (\\(Z\\)) but weights and noise are different across the dataset. So our latent factor \\(Z\\) is going to capture the shared pattern using a global score and in addition the weights can be used to project the original data into data view specific scores.\nIn BCCA also: 1. The latent variables \\(\\mathbf z\\) follow a standard normal distribution: \\[ \\mathbf z \\sim \\mathcal{N}(0, 1) \\] 2. The weight matrix for both datasets \\(\\mathbf{W_{x,y}}\\) also follows a standard normal distribution: \\[ \\mathbf{W_{x,y}} \\sim \\mathcal{N}(0, 1) \\]\n\nThe precision parameter \\(\\tau_{x,y}\\) follows a gamma distribution with parameters \\(\\alpha_0\\) and \\(\\beta_0\\): \\[ \\tau \\sim \\mathcal{G}(\\alpha_0, \\beta_0) \\]\nThe noise term \\(\\epsilon_n\\) is also normally distributed for both dataset with mean 0 and precision \\(\\tau^{-1}\\): \\[ \\epsilon_n \\sim \\mathcal{N}(0, \\tau^{-1}) \\]\nThe observed data \\(\\mathbf{x}_n\\) and \\(\\mathbf{y}_n\\) follows a normal distribution with mean \\(\\mathbf W_x\\mathbf z_n\\) and \\(\\mathbf W_y\\mathbf z_n\\) and covariance \\(\\tau_{x,y}^{-1}\\) so \\[ \\mathbf{x}_n \\sim \\mathcal{N}(\\mathbf W_x\\mathbf z_n, \\tau_x^{-1} \\mathbf ) \\] and \\[ \\mathbf{y}_n \\sim \\mathcal{N}(\\mathbf W_y\\mathbf z_n, \\tau_y^{-1} \\mathbf ) \\].\n\nThe rest of the optimization etc are similar to those of BPCA.\n\nBCCA R implementation\n\n# Load the rstan library for Bayesian analysis using Stan\nrequire(rstan)\n\n# Set rstan options for automatic caching and multi-core processing\nrstan_options(auto_write = FALSE)\noptions(mc.cores = parallel::detectCores())\n\n# Fix the random seed for reproducibility\nset.seed(100)\n\n# Define the Canonical Correlation Analysis (CCA) model in Stan language\ncca2 &lt;- \"\ndata {\n        int&lt;lower=0&gt; N; // Number of samples\n        int&lt;lower=0&gt; D1; // The original dimension\n        int&lt;lower=0&gt; D2; // The original dimension\n        int&lt;lower=0&gt; K; // The latent dimension\n        matrix[N, D1] X1; // The data matrix\n        matrix[N, D2] X2; // The data matrix\n    }\n\n    parameters {\n        matrix[N, K] Z; // The latent matrix\n        matrix[D1, K] W1; // The weight matrix\n        matrix[D2, K] W2; // The weight matrix\n        real&lt;lower=0&gt; tau1; // Noise term \n        real&lt;lower=0&gt; tau2; // Noise term \n    }\n\n    transformed parameters{\n        real&lt;lower=0&gt; t_tau1;\n    t_tau1 = inv(sqrt(tau1));\n    \n        real&lt;lower=0&gt; t_tau2;\n    t_tau2 = inv(sqrt(tau2));\n    }\n    model {\n        tau1 ~ gamma(1,1);\n        tau2 ~ gamma(1,1);\n        to_vector(Z) ~ normal(0,1);\n        to_vector(W1)~ normal(0, 1);\n        to_vector(W2)~ normal(0, 1);\n        to_vector(X1) ~ normal(to_vector(Z*W1'), t_tau1);\n        to_vector(X2) ~ normal(to_vector(Z*W2'), t_tau2);\n\n    }\"\n\n# Compile the Stan model for CCA\ncca_model &lt;- stan_model(model_code = cca2)\n\n# Load and preprocess the breast cancer mRNA and protein datasets\nX1 &lt;- scale(breast.TCGA$data.train$mrna,scale = F,center = T)\nY &lt;- scale(breast.TCGA$data.train$protein,scale = F,center = T)\n\n# Prepare the data for the Stan model\ndata &lt;- list(N = dim(X)[1], D1 = dim(X)[2], K = 2, X1 = X1, X2 = Y, D2 = dim(Y)[2])\n\n# Set another random seed for reproducibility in the inference step\nset.seed(200)\n# Fit the CCA model using Variational Bayes with the meanfield algorithm\ncca_fit &lt;- vb(cca_model, data = data, algorithm = \"meanfield\", iter = 1000, output_samples = 100)\n\nChain 1: ------------------------------------------------------------\nChain 1: EXPERIMENTAL ALGORITHM:\nChain 1:   This procedure has not been thoroughly tested and may be unstable\nChain 1:   or buggy. The interface is subject to change.\nChain 1: ------------------------------------------------------------\nChain 1: \nChain 1: \nChain 1: \nChain 1: Gradient evaluation took 0.002368 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 23.68 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Begin eta adaptation.\nChain 1: Iteration:   1 / 250 [  0%]  (Adaptation)\nChain 1: Iteration:  50 / 250 [ 20%]  (Adaptation)\nChain 1: Iteration: 100 / 250 [ 40%]  (Adaptation)\nChain 1: Iteration: 150 / 250 [ 60%]  (Adaptation)\nChain 1: Iteration: 200 / 250 [ 80%]  (Adaptation)\nChain 1: Success! Found best value [eta = 1] earlier than expected.\nChain 1: \nChain 1: Begin stochastic gradient ascent.\nChain 1:   iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes \nChain 1:    100       -60643.836             1.000            1.000\nChain 1:    200       -59084.380             0.513            1.000\nChain 1:    300       -58991.764             0.014            0.026\nChain 1:    400       -58978.129             0.001            0.002   MEAN ELBO CONVERGED   MEDIAN ELBO CONVERGED\nChain 1: \nChain 1: Drawing a sample of size 100 from the approximate posterior... \nChain 1: COMPLETED.\n\n\nWarning: Pareto k diagnostic value is 4.98. Resampling is disabled. Decreasing\ntol_rel_obj may help if variational algorithm has terminated prematurely.\nOtherwise consider using sampling instead.\n\n# Extract and compute the average latent scores from the fit for global, mRNA, and protein data\nscores_global &lt;- apply(extract(cca_fit,\"Z\")[[1]], c(2,3), mean)\nscores_x &lt;- X1%*% apply(extract(cca_fit,\"W1\")[[1]], c(2,3), mean)\nscores_y &lt;- Y%*% apply(extract(cca_fit,\"W2\")[[1]], c(2,3), mean)\n\n# Plot the latent scores for mRNA, protein, and global datasets in a 2x2 grid layout\npar(mfrow=c(2,2))\nplot(scores_x, col=breast.TCGA$data.train$subtype, main=\"BCCA mRNA\", xlab=\"L1\", ylab=\"L2\")\nplot(scores_y, col=breast.TCGA$data.train$subtype, main=\"BCCA protein\", xlab=\"L1\", ylab=\"L2\")\nplot(scores_global, col=breast.TCGA$data.train$subtype, main=\"BCCA global\", xlab=\"L1\", ylab=\"L2\")\n\n\n\n\nBased on the scores, can we say if we have captured shared pattern between our data sets? do A PCA on each of the data sets and compare the results with what we got here"
  },
  {
    "objectID": "mofa-prev.html#gfa-mofa-r-implementation",
    "href": "mofa-prev.html#gfa-mofa-r-implementation",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "GFA MOFA R implementation",
    "text": "GFA MOFA R implementation\n\n# Fix the random seed for reproducibility\nset.seed(100)\n\n# Define GFA\ngfa &lt;- \"\ndata {\n    int&lt;lower=1&gt; N;             // Number of data points\n    int&lt;lower=1&gt; K;             // Dimensionality of latent space\n    int&lt;lower=1&gt; M;             // Number of modalities\n    int&lt;lower=1&gt; SumP;          // Total number of features across all modalities\n    int&lt;lower=1&gt; P[M];          // Number of features for each modality\n    matrix[N, SumP] x;          // Concatenated data\n}\n\nparameters {\n    matrix[K, SumP] W;          // Factor loading matrix\n    vector&lt;lower=0&gt;[M] tau;       // Precision for each modality\n    matrix[N, K] z;             // Latent variables\n    matrix&lt;lower=0&gt;[M,K] alpha; // View-specific ARD prior\n}\n\n    transformed parameters{\n        vector&lt;lower=0&gt;[M] t_tau;\n    t_tau = inv(sqrt(tau));\n    \n    }\n\nmodel {\n\n    // fix z first\n    to_vector(z) ~ normal(0,1);\n    tau ~ gamma(1, 1);\n    // Priors\n    int start;\n    start = 0;\n        for (m in 1:M) {\n            for (d in 1:P[m]) {\n                start = start + 1;   \n                W[,start] ~ normal(0,1);\n                x[,start] ~ normal(z*W[,start], t_tau[m]);  \n                \n            }\n        }\n\n    \n}\n\"\n\n\n\ngfa_model &lt;- stan_model(model_code = gfa)\n\n# Load and preprocess the breast cancer mRNA, mirna and protein datasets\nX1 &lt;- scale(breast.TCGA$data.train$mrna,scale = F,center = T)\nX2 &lt;- scale(breast.TCGA$data.train$protein,scale = F,center = T)\nX3 &lt;- scale(breast.TCGA$data.train$mirna,scale = F,center = T)\n\n# prepare the list\nmatrices_list &lt;- list(\n  X1,\n  X2,\n  X3\n)\n\ncombined_data &lt;- do.call(cbind,matrices_list)\n# Prepare the data for the Stan model\nstan_data &lt;- list(\n  N = sapply(matrices_list, nrow)[1],\n  K = 2,\n  P = sapply(matrices_list, ncol),\n  M = length(matrices_list),\n  x = combined_data,\n  SumP = ncol(combined_data)\n  \n)\n\n# Set another random seed for reproducibility in the inference step\nset.seed(2000)\n\n# Fit the GFA model using Variational Bayes with the meanfield algorithm\ngfa_fit &lt;- vb(gfa_model, data = stan_data, algorithm = \"meanfield\", iter = 1000, output_samples = 100)\n\nChain 1: ------------------------------------------------------------\nChain 1: EXPERIMENTAL ALGORITHM:\nChain 1:   This procedure has not been thoroughly tested and may be unstable\nChain 1:   or buggy. The interface is subject to change.\nChain 1: ------------------------------------------------------------\nChain 1: \nChain 1: \nChain 1: \nChain 1: Gradient evaluation took 0.003909 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 39.09 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Begin eta adaptation.\nChain 1: Iteration:   1 / 250 [  0%]  (Adaptation)\nChain 1: Iteration:  50 / 250 [ 20%]  (Adaptation)\nChain 1: Iteration: 100 / 250 [ 40%]  (Adaptation)\nChain 1: Iteration: 150 / 250 [ 60%]  (Adaptation)\nChain 1: Iteration: 200 / 250 [ 80%]  (Adaptation)\nChain 1: Success! Found best value [eta = 1] earlier than expected.\nChain 1: \nChain 1: Begin stochastic gradient ascent.\nChain 1:   iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes \nChain 1:    100       -99211.110             1.000            1.000\nChain 1:    200       -97573.911             0.508            1.000\nChain 1:    300       -97485.188             0.009            0.017   MEAN ELBO CONVERGED\nChain 1: \nChain 1: Drawing a sample of size 100 from the approximate posterior... \nChain 1: COMPLETED.\n\n\nWarning: Pareto k diagnostic value is 26.79. Resampling is disabled. Decreasing\ntol_rel_obj may help if variational algorithm has terminated prematurely.\nOtherwise consider using sampling instead.\n\n# Extract and compute the average latent scores from the fit for global, mRNA, and protein data\nscores_global &lt;- apply(extract(gfa_fit,\"z\")[[1]], c(2,3), mean)\nW_mean &lt;-apply(extract(gfa_fit,\"W\")[[1]], c(2,3), mean)\nW_mean_no_sparse&lt;-W_mean\n# Separate W_mean based on modalities\nW_list &lt;- list()\nstart_col &lt;- 1\nfor(m in 1:stan_data$M) {\n    end_col &lt;- start_col + stan_data$P[m] - 1\n    W_list[[m]] &lt;- W_mean[, start_col:end_col]\n    start_col &lt;- end_col + 1\n}\n\nscores &lt;- mapply(function(x,y){list(x%*%t(y))},x=matrices_list,y=W_list)\n\n\n# Plot the latent scores for mRNA, protein, and global datasets in a 2x2 grid layout\npar(mfrow=c(2,2))\nplot(scores[[1]], col=breast.TCGA$data.train$subtype, main=\"GFA mRNA\", xlab=\"L1\", ylab=\"L2\")\nplot(scores[[2]], col=breast.TCGA$data.train$subtype, main=\"GFA protein\", xlab=\"L1\", ylab=\"L2\")\nplot(scores[[3]], col=breast.TCGA$data.train$subtype, main=\"GFA miRNA\", xlab=\"L1\", ylab=\"L2\")\nplot(scores_global, col=breast.TCGA$data.train$subtype, main=\"GFA global\", xlab=\"L1\", ylab=\"L2\")\n\n\n\n\nIn the code above, since Stan does not support ragged data structures (data structures with different lengths), we had to concatenate the data first, but then instruct Rstan to put different assumptions on different columns of the data depending on which modality it originates from.\nThe results are more or less clear; we have captured the shared pattern across these three datasets.\nOne more thing before talking about MOFA is that we have been omitting sparsity considerations in our model. Sparsity plays a crucial role in high-dimensional data analysis, ensuring that the model remains interpretable and avoids overfitting. Two popular approaches to incorporate sparsity are Automatic Relevance Determination (ARD) and the Spike-and-slab prior.\nARD is a form of Bayesian regularization where each feature is assigned its own regularization coefficient, allowing the model to effectively “turn off” irrelevant features by pushing their coefficients towards zero. This results in a more interpretable model where only the most relevant features contribute to the outcome. Mathematically, this can be represented as: \\[p(\\mathbf{W}|\\boldsymbol{\\alpha}) = \\prod_{k=1}^{K} N(\\mathbf{w}_{:,k};0,\\frac{1}{\\alpha_{k}}I_{D})\\] \\[p(\\boldsymbol{\\alpha}) = \\prod_{k=1}^{K} \\mathcal{G}(\\alpha_k; a_0^\\alpha, b_0^\\alpha)\\] where \\(W\\) is the weight matrix and \\(\\alpha\\) represents the precision of the weights. The Gamma distribution shapes the hyperparameters \\(\\boldsymbol{\\alpha}\\), capturing the uncertainty associated with the inverse variance of the weights.\nOn the other hand, the Spike-and-slab prior combines two distributions: a spike at zero, representing the probability that a coefficient is exactly zero, and a slab, a continuous distribution reflecting the possible non-zero values of the coefficient. This mixture allows for both exact zeros and non-zero coefficients, making it a powerful tool for variable selection in high-dimensional settings. The prior can be represented as: \\[p(w_{d,k} | \\alpha_k,\\theta_k) = (1-\\theta_k) \\delta_0(w_{d,k}) + \\theta_k N(w_{d,k};0, \\alpha_k^{-1})\\] \\[p(\\theta_k) = \\mathcal{B}(\\theta_k; a_0^\\theta,b_0^\\theta)\\] \\[p(\\alpha_k) = \\mathcal{G}(\\alpha_k; a_0^\\alpha, b_0^\\alpha)\\]\nHere, the Beta distribution is utilized for modeling the hyperparameter \\(\\theta_k\\), indicating the probability that a weight is non-zero. As the Beta distribution is defined between [0, 1], it’s a good fit for modeling probabilities.\nLet’s try to incorporate these in the model.\n\n# Fix the random seed for reproducibility\nset.seed(100)\n\ngfa_sparse &lt;- \"\ndata {\n    int&lt;lower=1&gt; N;             // Number of data points\n    int&lt;lower=1&gt; K;             // Dimensionality of latent space\n    int&lt;lower=1&gt; M;             // Number of modalities\n    int&lt;lower=1&gt; SumP;          // Total number of features across all modalities\n    int&lt;lower=1&gt; P[M];          // Number of features for each modality\n    matrix[N, SumP] x;          // Concatenated data\n    real a0_theta;              // Hyperparameter for Beta prior\n    real b0_theta;              // Hyperparameter for Beta prior\n}\n\nparameters {\n    matrix[K, SumP] W;          // Factor loading matrix\n    vector&lt;lower=0&gt;[M] tau;       // Precision for each modality\n    matrix[N, K] z;             // Latent variables\n    matrix&lt;lower=0&gt;[M,K] alpha; // View-specific ARD prior\n    matrix&lt;lower=0, upper=1&gt;[K, SumP] theta; // Spike-and-slab mixing proportion\n}\n\n\n    transformed parameters{\n        vector&lt;lower=0&gt;[M] t_tau;\n    t_tau = inv(sqrt(tau));\n    matrix&lt;lower=0&gt;[M,K] t_alpha; \n    t_alpha = inv(sqrt(alpha));\n    \n    }\n\nmodel {\n\n    // fix z first\n    to_vector(z) ~ normal(0,1);\n    tau ~ gamma(1, 1);\n    to_vector(alpha) ~ gamma(1e-2,1e-2);\n\n    // add aph\n    // Priors\n// Incorporating the ARD and spike-and-slab priors\n    int start;\n    start = 0;\n    for (m in 1:M) {\n        for (d in 1:P[m]) {\n            start = start + 1;   \n            \n            // Spike-and-slab prior\n            for (k in 1:K) {\n                \n                theta[k,start] ~ beta(a0_theta, b0_theta);\n                target += log_mix(theta[k, start],\n                                  normal_lpdf(W[k,start] | 0, t_alpha[m,k]),normal_lpdf(W[k,start] | 0, 1e-14));\n            }\n            \n            // Data likelihood\n            x[,start] ~ normal(z*W[,start], t_tau[m]);  \n        }\n\n    \n    }\n}\n\"\n\n\n\ngfa_model &lt;- stan_model(model_code = gfa_sparse)\n\n# Load and preprocess the breast cancer mRNA and protein datasets\nX1 &lt;- scale(breast.TCGA$data.train$mrna,scale = F,center = T)\nX2 &lt;- scale(breast.TCGA$data.train$protein,scale = F,center = T)\nX3 &lt;- scale(breast.TCGA$data.train$mirna,scale = F,center = T)\n\n# prepare the list\nmatrices_list &lt;- list(\n  X1,\n  X2,\n  X3\n)\n\ncombined_data &lt;- do.call(cbind,matrices_list)\n# Prepare the data for the Stan model\nstan_data &lt;- list(\n  N = sapply(matrices_list, nrow)[1],\n  K = 2,\n  P = sapply(matrices_list, ncol),\n  M = length(matrices_list),\n  x = combined_data,\n  SumP = ncol(combined_data),\n  a0_theta=1,\n  b0_theta=1\n  \n)\n\n# Set another random seed for reproducibility in the inference step\nset.seed(200)\n\n# Fit the GFA model using Variational Bayes with the meanfield algorithm\ngfa_fit &lt;- vb(gfa_model, data = stan_data, algorithm = \"meanfield\", iter = 1000, output_samples = 100)\n\nChain 1: ------------------------------------------------------------\nChain 1: EXPERIMENTAL ALGORITHM:\nChain 1:   This procedure has not been thoroughly tested and may be unstable\nChain 1:   or buggy. The interface is subject to change.\nChain 1: ------------------------------------------------------------\nChain 1: \nChain 1: \nChain 1: \nChain 1: Gradient evaluation took 0.006706 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 67.06 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Begin eta adaptation.\nChain 1: Iteration:   1 / 250 [  0%]  (Adaptation)\nChain 1: Iteration:  50 / 250 [ 20%]  (Adaptation)\nChain 1: Iteration: 100 / 250 [ 40%]  (Adaptation)\nChain 1: Iteration: 150 / 250 [ 60%]  (Adaptation)\nChain 1: Iteration: 200 / 250 [ 80%]  (Adaptation)\nChain 1: Success! Found best value [eta = 1] earlier than expected.\nChain 1: \nChain 1: Begin stochastic gradient ascent.\nChain 1:   iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes \nChain 1:    100      -104607.753             1.000            1.000\nChain 1:    200       -98371.919             0.532            1.000\nChain 1:    300       -98212.222             0.033            0.063\nChain 1:    400       -98172.998             0.001            0.002   MEAN ELBO CONVERGED   MEDIAN ELBO CONVERGED\nChain 1: \nChain 1: Drawing a sample of size 100 from the approximate posterior... \nChain 1: COMPLETED.\n\n\nWarning: Pareto k diagnostic value is 6.26. Resampling is disabled. Decreasing\ntol_rel_obj may help if variational algorithm has terminated prematurely.\nOtherwise consider using sampling instead.\n\n# Extract and compute the average latent scores from the fit for global, mRNA, protein, and miRNA data\nscores_global &lt;- apply(extract(gfa_fit,\"z\")[[1]], c(2,3), mean)\nW_mean &lt;-apply(extract(gfa_fit,\"W\")[[1]], c(2,3), mean)\n\n# Separate W_mean based on modalities\nW_list &lt;- list()\nstart_col &lt;- 1\nfor(m in 1:stan_data$M) {\n  end_col &lt;- start_col + stan_data$P[m] - 1\n  W_list[[m]] &lt;- W_mean[, start_col:end_col]\n  start_col &lt;- end_col + 1\n}\n\n# calculate block scores\nscores &lt;- mapply(function(x,y){list(x%*%t(y))},x=matrices_list,y=W_list)\n\n\n# Plot the latent scores for mRNA, protein, and global datasets in a 2x2 grid layout\npar(mfrow=c(2,2))\nplot(scores[[1]], col=breast.TCGA$data.train$subtype, main=\"GFA mRNA\", xlab=\"L1\", ylab=\"L2\")\nplot(scores[[2]], col=breast.TCGA$data.train$subtype, main=\"GFA protein\", xlab=\"L1\", ylab=\"L2\")\nplot(scores[[3]], col=breast.TCGA$data.train$subtype, main=\"GFA miRNA\", xlab=\"L1\", ylab=\"L2\")\nplot(scores_global, col=breast.TCGA$data.train$subtype, main=\"GFA global\", xlab=\"L1\", ylab=\"L2\")\n\n\n\n\nThe above code computes the log of a mixture of two distributions. In the context of the spike-and-slab prior, this function is used to represent the spike (a point mass at zero) and the slab (a continuous distribution) for the weights. As said, the idea is to combine a “spike” (a point mass at zero, promoting sparsity) with a “slab” (a continuous distribution allowing for non-zero parameter values). By mixing these two distributions, the prior encourages the parameters to be close to zero (due to the spike) while allowing some to take non-zero values (due to the slab).\nIn practice however our parametrization might cause problems in inference.\nMOFA uses re-parametrization of the weights \\(w\\) as a product of a Gaussian random variable \\(\\hat{w}\\) and a Bernoulli random variable \\(s, 12\\), 4] resulting in the following prior:\n\\[\np\\left(\\hat{w}_{d, k}^{m}, s_{d, k}^{m}\\right)=\\mathcal{N}\\left(\\hat{w}_{d, k}^{m} \\mid 0,1 / \\alpha_{k}^{m}\\right) \\operatorname{Ber}\\left(s_{d, k}^{m} \\mid \\theta_{k}^{m}\\right)\n\\] with hyper-parameters \\(a_{0}^{\\theta}, b_{0}^{\\theta}=1\\) and \\(a_{0}^{\\alpha}, b_{0}^{\\alpha}=1 e^{-14}\\) to get uninformative priors. A value of \\(\\theta_{k}^{m}\\) close to 0 implies that most of the weights of factor \\(k\\) in view \\(m\\) are shrinked to 0 , which is the definition of a sparse factor. In contrast, a value of \\(\\theta_{k}^{m}\\) close to 1 implies that most of the weights are non-zero, which is the definition of a non-sparse factor.\nWe cannot directly construct a spike-and-slab prior in Stan since it requires a discrete parameter. We are going to leave it as it is now! So all together with the code above, we have more or less reach the final joint probability of MOFA:\n\\[\n\\begin{aligned}\np(\\mathbf{Y}, \\hat{\\mathbf{W}}, \\mathbf{S}, \\mathbf{Z}, \\boldsymbol{\\Theta}, \\boldsymbol{\\alpha}, \\boldsymbol{\\tau})= & \\prod_{m=1}^{M} \\prod_{n=1}^{N} \\prod_{d=1}^{D_{m}} \\mathcal{N}\\left(y_{n d}^{m} \\mid \\sum_{k=1}^{K} s_{d k}^{m} \\hat{w}_{d k}^{m} z_{n k}, 1 / \\tau_{d}\\right) \\\\\n& \\prod_{m=1}^{M} \\prod_{d=1}^{D_{m}} \\prod_{k=1}^{K} \\mathcal{N}\\left(\\hat{w}_{d k}^{m} \\mid 0,1 / \\alpha_{k}^{m}\\right) \\operatorname{Ber}\\left(s_{d, k}^{m} \\mid \\theta_{k}^{m}\\right) \\\\\n& \\prod_{n=1}^{N} \\prod_{k=1}^{K} \\mathcal{N}\\left(z_{n k} \\mid 0,1\\right) \\\\\n& \\prod_{m=1}^{M} \\prod_{k=1}^{K} \\operatorname{Beta}\\left(\\theta_{k}^{m} \\mid a_{0}^{\\theta}, b_{0}^{\\theta}\\right) \\\\\n& \\prod_{m=1}^{M} \\prod_{k=1}^{K} \\mathcal{G}\\left(\\alpha_{k}^{m} \\mid a_{0}^{\\alpha}, b_{0}^{\\alpha}\\right) \\\\\n& \\prod_{m=1}^{M} \\prod_{d=1}^{D_{m}} \\mathcal{G}\\left(\\tau_{d}^{m} \\mid a_{0}^{\\tau}, b_{0}^{\\tau}\\right) .\n\\end{aligned}\n\\]\nMOFA follows GFA under the hood but it also allows one to use different priors depending on data distribution and also different levels of sparsity but the general idea is what we got to now. After going through the theory it is time to use MOFA to do data integration."
  },
  {
    "objectID": "mofa-prev.html#time-series-analysis",
    "href": "mofa-prev.html#time-series-analysis",
    "title": "Multi-Omics Factor Analysis (MOFA)",
    "section": "Time Series analysis",
    "text": "Time Series analysis\nIf you remember from the original definition of GFA, our prior assumption on \\(Z\\) is \\(\\mathbf z \\sim \\mathcal{N}(0, 1)\\). This means that we draw observations from a normal distribution with a mean of 0 and an identity covariance matrix, each observation is independent of the others. The identity covariance matrix means that there’s no correlation between any pair of observations.\nHowever, in time series analysis what we are interested in are the factor values that gradually change along a covariate of interest. To achieve this, we need to start introducing some small but important changes in our assumption. Instead of assuming that the latent factors are independent, we model them with a structured covariance matrix derived from a Gaussian Process (GP). This matrix, often denoted as \\(K\\), captures the correlation between the factors based on their position in the covariate space. By using a GP as our prior on \\(Z\\), we are essentially saying that nearby points in our covariate (e.g., time or space) will have similar factor values, introducing a smoothness constraint. The specific nature of this smoothness - whether it’s a general trend, periodicity, or some other pattern - is determined by the kernel function we choose for the GP. This approach allows us to model complex, structured behaviors in our factors, making our GFA model more flexible and capable of capturing the intricate patterns in multi-view data that vary along a specific covariate.\nMore specifically, we are going to use Exponentiated Quadratic (EQ) kernel, also commonly referred to as Gaussian kernel, is one of the most widely used kernels in Gaussian Processes. Its formula is given by:\n\\[k(x, x') = \\sigma^2 \\exp\\left(-\\frac{(x - x')^2}{2l^2}\\right)\\]\nWhere: \\(x\\) and \\(x'\\) are two points in the input space. \\(\\sigma^2\\) is the variance parameter, which controls the overall scale of the function. And \\(l\\) is the lengthscale parameter, which determines how quickly the correlation between two points decays with distance.\nThe EQ kernel has a few key properties and implications:\n\nLocality: The lengthscale \\(l\\) determines how “local” the kernel is. A small \\(l\\) means that the function values can change rapidly, leading to a wiggly function. A large \\(l\\) means that the function changes slowly, leading to smoother functions. This parameter is essential for capturing the underlying smoothness in the data.\nVariance: The variance parameter \\(\\sigma^2\\) determines the vertical scale of the functions. A larger \\(\\sigma^2\\) will lead to functions that vary more in amplitude.\n\nLet’s see an example to understand the concept better. Suppose we’re tracking the gene expression levels of a particular gene in a cell line over time. Our measurements are taken at three time points following a stimulus: \\(t_1 = 1\\) (1 hour post-stimulus), \\(t_2 = 2\\) (2 hours post-stimulus), and \\(t_3 = 4\\) (4 hours post-stimulus). We want to calculate the covariance between these time points using the EQ kernel to understand how the expression levels of this gene are correlated over time.\nLet’s assume \\(\\sigma^2 = 1\\) and \\(l = 2\\) (meaning we expect the gene expression to have a consistent change over a 2-hour period).\n\nCovariance between 1 hour and 2 hours post-stimulus: \\[k(t_1, t_2) = \\exp\\left(-\\frac{(1 - 2)^2}{2(2^2)}\\right) = \\exp(-0.125) \\approx 0.883\\]\n\nThis indicates that the gene expression levels at 1 hour and 2 hours post-stimulus are closely correlated.\n\nCovariance between 1 hour and 4 hours post-stimulus: \\[k(t_1, t_3) = \\exp\\left(-\\frac{(1 - 4)^2}{2(2^2)}\\right) = \\exp(-1.125) \\approx 0.325\\]\n\nThis lower covariance suggests that the gene expression levels at 1 hour and 4 hours post-stimulus are less correlated compared to consecutive hours.\n\nCovariance between 2 hours and 4 hours post-stimulus: \\[k(t_2, t_3) = \\exp\\left(-\\frac{(2 - 4)^2}{2(2^2)}\\right) = \\exp(-0.5) \\approx 0.607\\]\n\nThe covariance matrix \\(K\\) for our three time points based on gene expression would be:\n\\[K = \\begin{bmatrix}\n1 & 0.883 & 0.325 \\\\\n0.883 & 1 & 0.607 \\\\\n0.325 & 0.607 & 1 \\\\\n\\end{bmatrix}\\]\nGene expression levels closer in time (e.g., consecutive hours) are more correlated than those further apart. This is because the cellular response, post-stimulus, might involve a cascade of events, and the gene’s role in this cascade might change as time progresses. The EQ kernel captures this decay in correlation, reflecting the idea that as time progresses, the cellular context changes, and thus the gene’s expression level might become less predictable based on earlier measurements.\nNow we are going to change the previous proir on \\(Z\\) to be \\(\\mathbf z \\sim \\mathcal{N}(0, K)\\). Here, we are drawing observations from a multivariate normal distribution with a mean of 0 but with a covariance matrix \\(K\\) derived from a Gaussian Process. as said before this covariance matrix \\(K\\) introduces dependencies between the observations and the nature and structure of these dependencies are determined by the kernel function used to generate ( K ).\nSo let’s try to implement this! First we will have to change our initial data a bit to have a smooth trend that emulates real-world scenarios. A common pattern observed in biological systems is the circadian rhythm, a natural, internal process that regulates the sleep-wake cycle and repeats roughly every 24 hours. This rhythmic oscillation is present in various biological processes and can significantly influence the expression patterns of certain genes and proteins. To do that we will introduce an artificial circadian-like oscillation to specific features of our dataset. This sinusoidal pattern will mimic the natural rhythmic changes one might observe in gene or protein expression data due to the influence of the circadian clock.\n\n# Create an artificial time covariate with a sinusoidal pattern\nset.seed(123)  # For reproducibility\ntime_covariate &lt;- sample(seq(1, 150, by=1),replace = F)\nsinusoidal_trend &lt;- sin(2 * pi * time_covariate / 24)  # A 24-unit cycle for the circadian rhythm\n\n# Modify data with a sinusoidal time trend for some features\n# remove the subtype information\ndata_mofa &lt;- breast.TCGA$data.train[-4]\n# we do transpose because mofa wants features in rows\ndata_mofa &lt;- lapply(data_mofa,t)\n\n\n# For miRNA data\nmiRNA_data &lt;- data_mofa$mirna\nmiRNA_data[1:20, ] &lt;- data_mofa$mirna[1:20, ] + 0.7* matrix(rep(sinusoidal_trend, 20), nrow=20, byrow=TRUE)  # Add trend to the first 20 features\n\n# For mRNA data\nmRNA_data &lt;- data_mofa$mrna\nmRNA_data[1:25, ] &lt;- data_mofa$mrna[1:25, ] +  matrix(rep(sinusoidal_trend, 25), nrow=25, byrow=TRUE)  # Add trend to the first 25 features\n\n# For protein data\nprotein_data &lt;- data_mofa$protein\nprotein_data[1:15, ] &lt;- protein_data[1:15, ] +  0.2* matrix(rep(sinusoidal_trend, 15), nrow=15, byrow=TRUE)  # Add trend to the first 15 features\n\nscatter.smooth(sinusoidal_trend~time_covariate,span = 0.1,evaluation = 500)\n\n\n\n\nThe plot shows the pattern that we have added to some of features in each modalities. We are now going to change the prior of our GFA:\n\n# Fix the random seed for reproducibility\nset.seed(100)\n\n# Define GFA\ngfa_smooth &lt;- \"\ndata {\n    int&lt;lower=1&gt; N;             // Number of data points\n    int&lt;lower=1&gt; K;             // Dimensionality of latent space\n    int&lt;lower=1&gt; M;             // Number of modalities\n    int&lt;lower=1&gt; SumP;          // Total number of features across all modalities\n    int&lt;lower=1&gt; P[M];          // Number of features for each modality\n    matrix[N, SumP] x;          // Concatenated data\n    real a0_theta;              // Hyperparameter for Beta prior\n    real b0_theta;              // Hyperparameter for Beta prior\n    real cov_vector[N]; \n}\n\nparameters {\n    matrix[K, SumP] W;          // Factor loading matrix\n    vector&lt;lower=0&gt;[M] tau;       // Precision for each modality\n    matrix[N, K] z;             // Latent variables\n    matrix&lt;lower=0&gt;[M,K] alpha; // View-specific ARD prior\n    matrix&lt;lower=0, upper=1&gt;[K, SumP] theta; // Spike-and-slab mixing proportion\n   vector&lt;lower=0&gt;[K] le;\n   vector&lt;lower=0&gt;[K] sigma_f;\n}\n\n\n    transformed parameters{\n        vector&lt;lower=0&gt;[M] t_tau;\n    t_tau = inv(sqrt(tau));\n    matrix&lt;lower=0&gt;[M,K] t_alpha; \n    t_alpha = inv(sqrt(alpha));\n    matrix[N, N] K_f[K];\n    matrix[N, N] L_K[K];\n    for (k in 1:K) {\n        K_f[k] = gp_exp_quad_cov(cov_vector, sigma_f[k], le[k]);\n\n        L_K[k] = cholesky_decompose(add_diag(K_f[k], rep_vector(1e-9, N)));\n    }\n    \n    }\n\nmodel {\n    // GP parameters\n    le ~ normal(0, 1);\n    sigma_f ~ normal(0, 1);\n  \n  // priors\n    for (k in 1:K) {\n        z[, k] ~ multi_normal_cholesky(rep_vector(0, N), L_K[k]);\n    }\n     \n    tau ~ gamma(1, 1);\n    to_vector(alpha) ~ gamma(1e-2,1e-2);\n\n    // add aph\n    // Priors\n// Incorporating the ARD and spike-and-slab priors\n    int start;\n    start = 0;\n    for (m in 1:M) {\n        for (d in 1:P[m]) {\n            start = start + 1;   \n            \n            // Spike-and-slab prior\n            for (k in 1:K) {\n                \n                theta[k,start] ~ beta(a0_theta, b0_theta);\n                target += log_mix(theta[k, start],\n                                  normal_lpdf(W[k,start] | 0, t_alpha[m,k]),normal_lpdf(W[k,start] | 0, 1e-14));\n            }\n            \n            // Data likelihood\n            x[,start] ~ normal(z*W[,start], t_tau[m]);  \n        }\n\n    \n    }\n}\n\n\n\"\n\nlibrary(rstan)\n\ngfa_model_old &lt;- stan_model(model_code = gfa_sparse)\ngfa_model_smooth &lt;- stan_model(model_code = gfa_smooth)\n# Load and preprocess the breast cancer mRNA, mirna and protein datasets\nX1 &lt;- t(miRNA_data)\nX2 &lt;- t(mRNA_data)\nX3 &lt;- t(protein_data)\n\n# prepare the list\nmatrices_list &lt;- list(\n  X1,\n  X2,\n  X3\n)\n#matrices_list&lt;-lapply(matrices_list,function(x){scale(x,scale = F)})\n\ncombined_data &lt;- do.call(cbind,matrices_list)\n# Prepare the data for the Stan model\nstan_data_old &lt;- list(\n  N = sapply(matrices_list, nrow)[1],\n  K = 4,\n  P = sapply(matrices_list, ncol),\n  M = length(matrices_list),\n  x = combined_data,\n  SumP = ncol(combined_data),\n  a0_theta=1,\n  b0_theta=1\n  \n)\n\nstan_data &lt;- list(\n  N = sapply(matrices_list, nrow)[1],\n  K = 4,\n  P = sapply(matrices_list, ncol),\n  M = length(matrices_list),\n  x = combined_data,\n  SumP = ncol(combined_data),\n  cov_vector=time_covariate,\n  a0_theta=1,\n  b0_theta=1\n  \n)\n\n\n# Set another random seed for reproducibility in the inference step\nset.seed(1000)\n\ngfa_fit_old &lt;- vb(gfa_model_old, data = stan_data_old, algorithm = \"meanfield\", iter = 2000, output_samples = 100)\n\nChain 1: ------------------------------------------------------------\nChain 1: EXPERIMENTAL ALGORITHM:\nChain 1:   This procedure has not been thoroughly tested and may be unstable\nChain 1:   or buggy. The interface is subject to change.\nChain 1: ------------------------------------------------------------\nChain 1: \nChain 1: \nChain 1: \nChain 1: Gradient evaluation took 0.005563 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 55.63 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Begin eta adaptation.\nChain 1: Iteration:   1 / 250 [  0%]  (Adaptation)\nChain 1: Iteration:  50 / 250 [ 20%]  (Adaptation)\nChain 1: Iteration: 100 / 250 [ 40%]  (Adaptation)\nChain 1: Iteration: 150 / 250 [ 60%]  (Adaptation)\nChain 1: Iteration: 200 / 250 [ 80%]  (Adaptation)\nChain 1: Success! Found best value [eta = 1] earlier than expected.\nChain 1: \nChain 1: Begin stochastic gradient ascent.\nChain 1:   iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes \nChain 1:    100      -116040.090             1.000            1.000\nChain 1:    200      -107023.648             0.542            1.000\nChain 1:    300      -105188.701             0.051            0.084\nChain 1:    400      -103915.494             0.015            0.017\nChain 1:    500      -103297.899             0.009            0.012   MEAN ELBO CONVERGED\nChain 1: \nChain 1: Drawing a sample of size 100 from the approximate posterior... \nChain 1: COMPLETED.\n\n\nWarning: Pareto k diagnostic value is 24.05. Resampling is disabled. Decreasing\ntol_rel_obj may help if variational algorithm has terminated prematurely.\nOtherwise consider using sampling instead.\n\n# Fit the GFA model using Variational Bayes with the meanfield algorithm\n\nset.seed(1000)\ngfa_fit_smooth &lt;- vb(gfa_model_smooth, data = stan_data, algorithm = \"meanfield\", iter = 2000, output_samples = 100)\n\nChain 1: ------------------------------------------------------------\nChain 1: EXPERIMENTAL ALGORITHM:\nChain 1:   This procedure has not been thoroughly tested and may be unstable\nChain 1:   or buggy. The interface is subject to change.\nChain 1: ------------------------------------------------------------\nChain 1: \nChain 1: \nChain 1: \nChain 1: Gradient evaluation took 0.019267 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 192.67 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Begin eta adaptation.\nChain 1: Iteration:   1 / 250 [  0%]  (Adaptation)\nChain 1: Iteration:  50 / 250 [ 20%]  (Adaptation)\nChain 1: Iteration: 100 / 250 [ 40%]  (Adaptation)\nChain 1: Iteration: 150 / 250 [ 60%]  (Adaptation)\nChain 1: Iteration: 200 / 250 [ 80%]  (Adaptation)\nChain 1: Success! Found best value [eta = 1] earlier than expected.\nChain 1: \nChain 1: Begin stochastic gradient ascent.\nChain 1:   iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes \nChain 1:    100      -262112.939             1.000            1.000\nChain 1:    200   -257883021.448             0.999            1.000\nChain 1:    300      -119529.996          1078.737         2156.475\nChain 1:    400      -108765.162          1078.287         2156.475\nChain 1:    500      -105680.106             0.064            0.099\nChain 1:    600      -104278.242             0.021            0.029\nChain 1:    700      -103670.861             0.010            0.013   MEAN ELBO CONVERGED\nChain 1: \nChain 1: Drawing a sample of size 100 from the approximate posterior... \nChain 1: COMPLETED.\n\n\nWarning: Pareto k diagnostic value is 20.13. Resampling is disabled. Decreasing\ntol_rel_obj may help if variational algorithm has terminated prematurely.\nOtherwise consider using sampling instead.\n\n# Extract and compute the average latent scores from the fit for global, mRNA, and protein data\nscores_global_old &lt;- apply(extract(gfa_fit_old,\"z\")[[1]], c(2,3), mean)\nscores_global &lt;- apply(extract(gfa_fit_smooth,\"z\")[[1]], c(2,3), mean)\n\npar(mfrow=c(4,2))\nscatter.smooth(time_covariate,scores_global_old[,1],span = 0.1,main=\"GFA l1\")\nscatter.smooth(time_covariate,scores_global[,1],span = 0.1,,main=\"smooth GFA l1\")\nscatter.smooth(time_covariate,scores_global_old[,2],span = 0.1,main=\"GFA l2\")\nscatter.smooth(time_covariate,scores_global[,2],span = 0.1,,main=\"smooth GFA l2\")\nscatter.smooth(time_covariate,scores_global_old[,3],span = 0.1,main=\"GFA l3\")\nscatter.smooth(time_covariate,scores_global[,3],span = 0.1,,main=\"smooth GFA l3\")\nscatter.smooth(time_covariate,scores_global_old[,4],span = 0.1,main=\"GFA l4\")\nscatter.smooth(time_covariate,scores_global[,4],span = 0.1,,main=\"smooth GFA l4\")\n\n\n\n\nHere I have done GFA both with and without smoothing. Just by looking at the factors we can see the smooth GFA has captured the time trend in our data where as in the GFA without smooth, the trend is not as clear. Please note that i have just used normal prior for \\(l\\) which might not make that much sense. In reality one can use a much suited prior.\n\nTime series in MOFA\nMOFA also provides us with tools to do time series analysis using GP but with much more robust model. Let’s see how can do the same analysis using MOFA or more specifically MEFISTO.\n\n# Combine into a list\ndata_mofa &lt;- list(mirna=miRNA_data, mrna=mRNA_data, protein=protein_data)\n\n# Convert the time_covariate to a data frame and set its row and column names\ntime &lt;- as.data.frame(t(time_covariate))\nrownames(time) &lt;- \"time\"\ncolnames(time) &lt;- colnames(miRNA_data)\n\n# Create a MOFA object using the data\nsm &lt;- create_mofa(data = data_mofa)\n\nCreating MOFA object from a list of matrices (features as rows, sample as columns)...\n\n# Set the time covariate to the MOFA object\nsm &lt;- set_covariates(sm, covariates = as.matrix(time))\n\n# Define default options for the data\ndata_opts &lt;- get_default_data_options(sm)\n\n# Define default model options and set the number of factors to 4\nmodel_opts &lt;- get_default_model_options(sm)\nmodel_opts$num_factors &lt;- 4\n\n# Define default training options and set the maximum iterations to 100\ntrain_opts &lt;- get_default_training_options(sm)\ntrain_opts$maxiter &lt;- 100\n\n# Define default MEFISTO options \nmefisto_opts &lt;- get_default_mefisto_options(sm)\n\n# Prepare the MOFA object with the specified options\nsm &lt;- prepare_mofa(sm, model_options = model_opts,\n                   mefisto_options = mefisto_opts,\n                   training_options = train_opts,\n                   data_options = data_opts)\n\nChecking data options...\n\n\nChecking training options...\n\n\nWarning in prepare_mofa(sm, model_options = model_opts, mefisto_options = mefisto_opts, : Maximum number of iterations is very small\n\n\nChecking model options...\n\n\nChecking inference options for mefisto covariates...\n\n# Run the MOFA analysis\nsm &lt;- run_mofa(sm)\n\nWarning in run_mofa(sm): No output filename provided. Using /var/folders/kh/tgq9mmld6_v9z_h220trj0c40000gn/T//RtmpDasfQ7/mofa_20231106-092223.hdf5 to store the trained model.\n\n\nConnecting to the mofapy2 python package using reticulate (use_basilisk = FALSE)... \n    Please make sure to manually specify the right python binary when loading R with reticulate::use_python(..., force=TRUE) or the right conda environment with reticulate::use_condaenv(..., force=TRUE)\n    If you prefer to let us automatically install a conda environment with 'mofapy2' installed using the 'basilisk' package, please use the argument 'use_basilisk = TRUE'\n\n\n\n        #########################################################\n        ###           __  __  ____  ______                    ### \n        ###          |  \\/  |/ __ \\|  ____/\\    _             ### \n        ###          | \\  / | |  | | |__ /  \\ _| |_           ### \n        ###          | |\\/| | |  | |  __/ /\\ \\_   _|          ###\n        ###          | |  | | |__| | | / ____ \\|_|            ###\n        ###          |_|  |_|\\____/|_|/_/    \\_\\              ###\n        ###                                                   ### \n        ######################################################### \n       \n \n        \nuse_float32 set to True: replacing float64 arrays by float32 arrays to speed up computations...\n\nSuccessfully loaded view='mirna' group='group1' with N=150 samples and D=184 features...\nSuccessfully loaded view='mrna' group='group1' with N=150 samples and D=200 features...\nSuccessfully loaded view='protein' group='group1' with N=150 samples and D=142 features...\n\n\nLoaded 1 covariate(s) for each sample...\n\n\nModel options:\n- Automatic Relevance Determination prior on the factors: False\n- Automatic Relevance Determination prior on the weights: True\n- Spike-and-slab prior on the factors: False\n- Spike-and-slab prior on the weights: False\nLikelihoods:\n- View 0 (mirna): gaussian\n- View 1 (mrna): gaussian\n- View 2 (protein): gaussian\n\n\n\n\n######################################\n## Training the model with seed 42 ##\n######################################\n\n\nELBO before training: -334295.48 \n\nIteration 1: time=0.01, ELBO=-98851.09, deltaELBO=235444.388 (70.43002487%), Factors=4\nIteration 2: time=0.00, Factors=4\nIteration 3: time=0.00, Factors=4\nIteration 4: time=0.00, Factors=4\nIteration 5: time=0.00, Factors=4\nIteration 6: time=0.01, ELBO=-91819.21, deltaELBO=7031.884 (2.10349349%), Factors=4\nIteration 7: time=0.00, Factors=4\nIteration 8: time=0.00, Factors=4\nIteration 9: time=0.00, Factors=4\nIteration 10: time=0.00, Factors=4\nIteration 11: time=0.01, ELBO=-91743.47, deltaELBO=75.736 (0.02265530%), Factors=4\nIteration 12: time=0.00, Factors=4\nIteration 13: time=0.00, Factors=4\nIteration 14: time=0.00, Factors=4\nIteration 15: time=0.00, Factors=4\nIteration 16: time=0.01, ELBO=-91701.50, deltaELBO=41.974 (0.01255601%), Factors=4\nIteration 17: time=0.01, Factors=4\nIteration 18: time=0.00, Factors=4\nIteration 19: time=0.00, Factors=4\nOptimising sigma node...\nIteration 20: time=0.69, Factors=4\nIteration 21: time=0.01, ELBO=-91619.80, deltaELBO=81.691 (0.02443686%), Factors=4\nIteration 22: time=0.01, Factors=4\nIteration 23: time=0.01, Factors=4\nIteration 24: time=0.01, Factors=4\nIteration 25: time=0.01, Factors=4\nIteration 26: time=0.01, ELBO=-91601.09, deltaELBO=18.716 (0.00559871%), Factors=4\nIteration 27: time=0.00, Factors=4\nIteration 28: time=0.00, Factors=4\nIteration 29: time=0.00, Factors=4\nOptimising sigma node...\nIteration 30: time=0.63, Factors=4\nIteration 31: time=0.01, ELBO=-91587.99, deltaELBO=13.100 (0.00391879%), Factors=4\nIteration 32: time=0.00, Factors=4\nIteration 33: time=0.00, Factors=4\nIteration 34: time=0.00, Factors=4\nIteration 35: time=0.00, Factors=4\nIteration 36: time=0.01, ELBO=-91578.67, deltaELBO=9.317 (0.00278715%), Factors=4\nIteration 37: time=0.00, Factors=4\nIteration 38: time=0.00, Factors=4\nIteration 39: time=0.00, Factors=4\nOptimising sigma node...\nIteration 40: time=0.66, Factors=4\nIteration 41: time=0.01, ELBO=-91571.49, deltaELBO=7.181 (0.00214798%), Factors=4\nIteration 42: time=0.01, Factors=4\nIteration 43: time=0.01, Factors=4\nIteration 44: time=0.01, Factors=4\nIteration 45: time=0.00, Factors=4\nIteration 46: time=0.01, ELBO=-91565.91, deltaELBO=5.583 (0.00167019%), Factors=4\nIteration 47: time=0.01, Factors=4\nIteration 48: time=0.00, Factors=4\nIteration 49: time=0.01, Factors=4\nOptimising sigma node...\nIteration 50: time=0.61, Factors=4\nIteration 51: time=0.01, ELBO=-91561.42, deltaELBO=4.490 (0.00134304%), Factors=4\nIteration 52: time=0.00, Factors=4\nIteration 53: time=0.00, Factors=4\nIteration 54: time=0.00, Factors=4\nIteration 55: time=0.00, Factors=4\nIteration 56: time=0.01, ELBO=-91557.78, deltaELBO=3.640 (0.00108897%), Factors=4\nIteration 57: time=0.00, Factors=4\nIteration 58: time=0.00, Factors=4\nIteration 59: time=0.00, Factors=4\nOptimising sigma node...\nIteration 60: time=0.62, Factors=4\nIteration 61: time=0.01, ELBO=-91554.73, deltaELBO=3.052 (0.00091290%), Factors=4\nIteration 62: time=0.00, Factors=4\nIteration 63: time=0.00, Factors=4\nIteration 64: time=0.00, Factors=4\nIteration 65: time=0.00, Factors=4\nIteration 66: time=0.01, ELBO=-91552.11, deltaELBO=2.616 (0.00078267%), Factors=4\nIteration 67: time=0.01, Factors=4\nIteration 68: time=0.00, Factors=4\nIteration 69: time=0.01, Factors=4\nOptimising sigma node...\nIteration 70: time=0.62, Factors=4\nIteration 71: time=0.01, ELBO=-91549.82, deltaELBO=2.293 (0.00068587%), Factors=4\nIteration 72: time=0.00, Factors=4\nIteration 73: time=0.00, Factors=4\nIteration 74: time=0.00, Factors=4\nIteration 75: time=0.00, Factors=4\nIteration 76: time=0.01, ELBO=-91547.74, deltaELBO=2.072 (0.00061988%), Factors=4\nIteration 77: time=0.00, Factors=4\nIteration 78: time=0.00, Factors=4\nIteration 79: time=0.00, Factors=4\nOptimising sigma node...\nIteration 80: time=0.64, Factors=4\nIteration 81: time=0.01, ELBO=-91545.80, deltaELBO=1.947 (0.00058231%), Factors=4\nIteration 82: time=0.01, Factors=4\nIteration 83: time=0.01, Factors=4\nIteration 84: time=0.01, Factors=4\nIteration 85: time=0.01, Factors=4\nIteration 86: time=0.01, ELBO=-91543.94, deltaELBO=1.854 (0.00055459%), Factors=4\nIteration 87: time=0.01, Factors=4\nIteration 88: time=0.01, Factors=4\nIteration 89: time=0.01, Factors=4\nOptimising sigma node...\nIteration 90: time=0.65, Factors=4\nIteration 91: time=0.01, ELBO=-91542.09, deltaELBO=1.856 (0.00055511%), Factors=4\nIteration 92: time=0.00, Factors=4\nIteration 93: time=0.00, Factors=4\nIteration 94: time=0.01, Factors=4\nIteration 95: time=0.00, Factors=4\nIteration 96: time=0.01, ELBO=-91540.21, deltaELBO=1.881 (0.00056268%), Factors=4\nIteration 97: time=0.00, Factors=4\nIteration 98: time=0.00, Factors=4\nIteration 99: time=0.00, Factors=4\n\n\n#######################\n## Training finished ##\n#######################\n\n\nSaving model in /var/folders/kh/tgq9mmld6_v9z_h220trj0c40000gn/T//RtmpDasfQ7/mofa_20231106-092223.hdf5...\n\n\nWarning in .quality_control(object, verbose = verbose): Factor(s) 2 are strongly correlated with the total number of expressed features for at least one of your omics. Such factors appear when there are differences in the total 'levels' between your samples, *sometimes* because of poor normalisation in the preprocessing steps.\n\n\nIn the above code, we begin by generating an artificial time-based covariate that follows a sinusoidal pattern. We then introduce this time-based trend into certain features of three modalities. Specifically, the first few features of each data type have this sinusoidal pattern superimposed on them. With our modified data in hand, we prepare it for a MOFA (Multi-Omics Factor Analysis) analysis.\nWe also convert our time-based covariate into a data frame format, ensuring its naming aligns with the miRNA data’s column names. With all data elements ready, we create a MOFA object and associate the time covariate with it. We then set default options for the data, model, training, and MEFISTO (an extension of MOFA). With everything set up, we finalize the MOFA object’s configuration with our specified options and initiate the MOFA analysis.\nWe can now have a look the expained variance across different data views:\n\n# Plot the variance explained by the factors\nplot_variance_explained(sm)\n\n\n\n\nMore or less all factors have some activities but the first factor has much more activity compared to rest of them. What we are going to do now is to figure out which factors are related to cancer subtypes and which ones related to the time trend. We need to set the covariates and plot the factors:\n\n# Add metadata (sample subtype and time covariate) to the MOFA object\nsamples_metadata(sm) &lt;- data.frame(sample=colnames(data_mofa$mirna), \n                                   subtype=breast.TCGA$data.train$subtype, \n                                   time=time_covariate,trend=sinusoidal_trend)\n\n# Plot the factors against the time covariate and color them by the time, while shaping them by the subtype\nplot_factors_vs_cov(sm, color_by = \"time\", shape_by = \"subtype\")\n\n\n\n\nWhat we can see is that the first factor is associated with subtype whereas the third one is more related to the trend. We can see this better if we start interpolating factors.\n\nsm &lt;- interpolate_factors(sm, new_values = seq(1, 150, by=0.1))\nplot_interpolation_vs_covariate(sm, covariate = \"time\")\n\n\n\n\nIf the we compare the graphs above with the actual trend we can immediately see that the third factor is much smoother representation of the actual trend:\n\nscatter.smooth(sinusoidal_trend~time_covariate,span = 0.1,evaluation = 500)\n\n\n\n\nGiven this, the main question is if MOFA has been successful in capturing the variables reflecting the trend? We know exactly what variables we changed so we can compare it to what MOFA has found.\n\n# Plot the weights of the top 20 features for the third factor\n\ncolor_mirna=data.frame(feature=rownames(MOFAobject@data$mirna$group1),\n                       sinusoidal=rep(c(\"Yes\",\"No\"),c(20,nrow(MOFAobject@data$mirna$group1)-20)),\n                       view=\"mirna\")\ncolor_mrna=data.frame(feature=rownames(MOFAobject@data$mrna$group1),\n                      sinusoidal=rep(c(\"Yes\",\"No\"),c(25,nrow(MOFAobject@data$mrna$group1)-25)),\n                      view=\"mrna\")\ncolor_protein=data.frame(feature=rownames(MOFAobject@data$protein$group1),sinusoidal=rep(c(\"Yes\",\"No\"),c(15,nrow(MOFAobject@data$protein$group1)-15)),view=\"protein\")\n\nfeatures_metadata(sm)&lt;-rbind(color_mirna,color_mrna,color_protein)\nlibrary(ggplot2)\nplot_weights(sm, factors = 1, view = \"mirna\", nfeatures = 20,color_by = \"sinusoidal\")+ggtitle(\"miRNA\")\nplot_weights(sm, factors = 1, view = \"mrna\", nfeatures = 25,color_by = \"sinusoidal\")+ggtitle(\"mRNA\")\nplot_weights(sm, factors = 1, view = \"protein\", nfeatures = 15,color_by = \"sinusoidal\")+ggtitle(\"protein\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plot above shows the weights of each feature on the x-axis and its ranking on the y-axis. What this plot shows us is that all top features selected by MOFA are in fact the ones we added the trend to. So MOFA has been almost perfectly successful in extracting the underlining pattern and important features. After getting the important features the rest of the analysis is more or less similar to classical data integration. In addition, similar kind of analysis can be performed on spatial data. For more information visit https://biofam.github.io/MOFA2/MEFISTO.html\nWe reached end of this lab now. There are very good tutorials on https://biofam.github.io/MOFA2/tutorials.html"
  }
]